From 1f587db2b51fc267e4b9a371a8b1c6a5f84403b5 Mon Sep 17 00:00:00 2001
From: Marcin Wojtas <mw@semihalf.com>
Date: Thu, 2 Jun 2016 13:15:55 +0200
Subject: [PATCH 0515/2241] Revert "mm, page_alloc: only enforce watermarks for
 order-0 allocations"

This is 1/3 of commits, whose purpose is to restore MIGRATE_RESERVE
and mm state before v4.2 kernel. Its impact, according to Mel Gorman,
was to give by coincidence larger window for kswapd to do its work
and prepare resources for intensive order 0 allocation. Originally
MIGRATE_RESERVE was not designed for this purpose, but no issue
was observed prior to its removal during the tests.

This patch series disables improvements around page allocation
merged into v4.4 kernel, which after tens of patches get efficient
enough in v4.7-rc1 kernel, so that page alloc failures seem to
eventually disappear.

This reverts commit 97a16fc82a7c ("mm, page_alloc: only enforce
watermarks for order-0 allocations")

Fix 1/4 of SYSTEMSW-2443: "Armada 3700 page alloc failures"

Change-Id: I3c6e8903340d1e835649c1085316cb8483dfe518
Signed-off-by: Marcin Wojtas <mw@semihalf.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/30479
Reviewed-by: Lior Amsalem <alior@marvell.com>
Tested-by: Star_Automation <star@marvell.com>
Reviewed-by: Wilson Ding <dingwei@marvell.com>
Reviewed-by: Hanna Hawa <hannah@marvell.com>
Reviewed-by: Nadav Haklai <nadavh@marvell.com>
---
 mm/page_alloc.c | 53 ++++++++++++++---------------------------------------
 1 file changed, 14 insertions(+), 39 deletions(-)

diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 6a11721..9937c66 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -2367,10 +2367,8 @@ static inline bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)
 #endif /* CONFIG_FAIL_PAGE_ALLOC */
 
 /*
- * Return true if free base pages are above 'mark'. For high-order checks it
- * will return true of the order-0 watermark is reached and there is at least
- * one free page of a suitable size. Checking now avoids taking the zone lock
- * to check in the allocation paths if no pages are free.
+ * Return true if free pages are above 'mark'. This takes into account the order
+ * of the allocation.
  */
 static bool __zone_watermark_ok(struct zone *z, unsigned int order,
 			unsigned long mark, int classzone_idx, int alloc_flags,
@@ -2378,7 +2376,7 @@ static bool __zone_watermark_ok(struct zone *z, unsigned int order,
 {
 	long min = mark;
 	int o;
-	const int alloc_harder = (alloc_flags & ALLOC_HARDER);
+	long free_cma = 0;
 
 	/* free_pages may go negative - that's OK */
 	free_pages -= (1 << order) - 1;
@@ -2391,7 +2389,7 @@ static bool __zone_watermark_ok(struct zone *z, unsigned int order,
 	 * the high-atomic reserves. This will over-estimate the size of the
 	 * atomic reserve but it avoids a search.
 	 */
-	if (likely(!alloc_harder))
+	if (likely(!(alloc_flags & ALLOC_HARDER)))
 		free_pages -= z->nr_reserved_highatomic;
 	else
 		min -= min / 4;
@@ -2399,45 +2397,22 @@ static bool __zone_watermark_ok(struct zone *z, unsigned int order,
 #ifdef CONFIG_CMA
 	/* If allocation can't use CMA areas don't use free CMA pages */
 	if (!(alloc_flags & ALLOC_CMA))
-		free_pages -= zone_page_state(z, NR_FREE_CMA_PAGES);
+		free_cma = zone_page_state(z, NR_FREE_CMA_PAGES);
 #endif
 
-	/*
-	 * Check watermarks for an order-0 allocation request. If these
-	 * are not met, then a high-order request also cannot go ahead
-	 * even if a suitable page happened to be free.
-	 */
-	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
+	if (free_pages - free_cma <= min + z->lowmem_reserve[classzone_idx])
 		return false;
+	for (o = 0; o < order; o++) {
+		/* At the next order, this order's pages become unavailable */
+		free_pages -= z->free_area[o].nr_free << o;
 
-	/* If this is an order-0 request then the watermark is fine */
-	if (!order)
-		return true;
-
-	/* For a high-order request, check at least one suitable page is free */
-	for (o = order; o < MAX_ORDER; o++) {
-		struct free_area *area = &z->free_area[o];
-		int mt;
-
-		if (!area->nr_free)
-			continue;
-
-		if (alloc_harder)
-			return true;
+		/* Require fewer higher order pages to be free */
+		min >>= 1;
 
-		for (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {
-			if (!list_empty(&area->free_list[mt]))
-				return true;
-		}
-
-#ifdef CONFIG_CMA
-		if ((alloc_flags & ALLOC_CMA) &&
-		    !list_empty(&area->free_list[MIGRATE_CMA])) {
-			return true;
-		}
-#endif
+		if (free_pages <= min)
+			return false;
 	}
-	return false;
+	return true;
 }
 
 bool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
-- 
2.7.4

