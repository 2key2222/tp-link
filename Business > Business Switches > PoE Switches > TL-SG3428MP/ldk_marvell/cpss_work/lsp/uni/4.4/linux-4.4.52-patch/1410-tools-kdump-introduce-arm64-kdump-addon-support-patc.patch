From b9b6b57b7f2cbaece6e1923893b562a0eade7aea Mon Sep 17 00:00:00 2001
From: Grzegorz Jaszczyk <jaz@semihalf.com>
Date: Fri, 3 Mar 2017 16:27:43 +0100
Subject: [PATCH 1410/2241] tools: kdump: introduce arm64 kdump addon support
 patches for LSP

The kdump support for arm64 is still on the kernel mainling list.
Therefore patches are provided as an addon which can be applied on
top of Marvel LSP release.

Patches was ported from:
git: https://git.linaro.org/people/takahiro.akashi/linux-aarch64.git
branch: origin/lsk/kdump/for-v4.4

It is also worth mentioning that the patches can't be ported directly from
the mainline lists, since the gap between kernel v4.4 and mainline 4.10 is
big and the whole support touches a lot of subsystem including generic mm
and arm64. Because of that the amount of patches which are required before
applying the main arm64's  kexec/kdump patches is so big.

Change-Id: I7dd43301f2c53fdf6ef79b2fcd8323a9164296ee
Signed-off-by: Grzegorz Jaszczyk <jaz@semihalf.com>
Reviewed-on: http://vgitil04.il.marvell.com:8080/37197
Reviewed-by: Nadav Haklai <nadavh@marvell.com>
Reviewed-by: Hanna Hawa <hannah@marvell.com>
Reviewed-by: Omri Itach <omrii@marvell.com>
Tested-by: Omri Itach <omrii@marvell.com>
---
 ...dd-HAVE_REGS_AND_STACK_ACCESS_API-feature.patch |  227 +++
 ...2-arm64-Add-more-test-functions-to-insn.c.patch |  157 ++
 ...onditional-instruction-simulation-support.patch |  206 +++
 ...rm64-Kprobes-with-single-stepping-support.patch | 1110 ++++++++++++++
 ...05-arm64-Blacklist-non-kprobe-able-symbol.patch |  279 ++++
 ...4-Treat-all-entry-code-as-non-kprobe-able.patch |   94 ++
 ...64-kprobes-instruction-simulation-support.patch |  513 +++++++
 ...-arm64-Add-trampoline-code-for-kretprobes.patch |  177 +++
 ...d-kernel-return-probes-support-kretprobes.patch |  137 ++
 ...s-Add-arm64-case-in-kprobe-example-module.patch |   47 +
 ...es-WARN-if-attempting-to-step-with-PSTATE.patch |   37 +
 ...64-kprobes-Fix-overflow-when-saving-stack.patch |   78 +
 .../0013-arm64-kprobes-Cleanup-jprobe_return.patch |   83 ++
 ...es-Add-KASAN-instrumentation-around-stack.patch |   74 +
 ...emove-stack-duplicating-code-from-jprobes.patch |  104 ++
 ...ARM-8458-1-bL_switcher-add-GIC-dependency.patch |   48 +
 ...510-1-rework-ARM_CPU_SUSPEND-dependencies.patch |   51 +
 .../0018-ARM-8478-2-arm-arm64-add-arm-smccc.patch  |  152 ++
 ...M-8479-2-add-implementation-for-arm-smccc.patch |  137 ++
 ...-2-arm64-add-implementation-for-arm-smccc.patch |  141 ++
 ...-drivers-psci-replace-psci-firmware-calls.patch |  189 +++
 ...ARM64-kernel-PSCI-move-PSCI-idle-manageme.patch |  352 +++++
 ...023-arm64-mm-fold-alternatives-into-.init.patch |  101 ++
 ...-Fix-local-variable-shadow-in-__set_fixma.patch |   55 +
 ...25-arm64-mm-remove-pointless-PAGE_MASKing.patch |   47 +
 ...-arm64-mm-specialise-pagetable-allocators.patch |  221 +++
 ...0027-arm64-head.S-use-memset-to-clear-BSS.patch |   57 +
 ...028-arm64-mm-place-empty_zero_page-in-bss.patch |  118 ++
 tools/kdump/0029-arm64-unify-idmap-removal.patch   |  157 ++
 tools/kdump/0030-arm64-unmap-idmap-earlier.patch   |   66 +
 ...1-arm64-add-function-to-install-the-idmap.patch |   71 +
 .../0032-arm64-mm-place-__cpu_setup-in-.text.patch |   42 +
 ...4-mm-add-code-to-safely-replace-TTBR1_EL1.patch |  118 ++
 ...m64-Use-PoU-cache-instr-for-I-D-coherency.patch |  183 +++
 ...Add-macros-to-read-write-system-registers.patch |   69 +
 ...64-vgic-v3-Make-the-LR-indexing-macro-pub.patch |   73 +
 ...-arm64-KVM-Add-a-HYP-specific-header-file.patch |   61 +
 ...-arm64-KVM-Implement-vgic-v2-save-restore.patch |  149 ++
 ...39-arm64-KVM-Implement-timer-save-restore.patch |  146 ++
 ...VM-Implement-system-register-save-restore.patch |  140 ++
 ...mplement-32bit-system-register-save-resto.patch |   89 ++
 ...42-arm64-KVM-Implement-debug-save-restore.patch |  198 +++
 .../0043-arm64-KVM-Implement-guest-entry.patch     |  180 +++
 ...arm64-KVM-Add-patchable-function-selector.patch |   59 +
 ...arm64-KVM-Implement-the-core-world-switch.patch |  173 +++
 ...6-arm64-KVM-Implement-fpsimd-save-restore.patch |  184 +++
 .../0047-arm64-KVM-Implement-TLB-handling.patch    |  122 ++
 .../0048-arm64-KVM-HYP-mode-entry-points.patch     |  239 +++
 .../kdump/0049-arm64-KVM-Add-panic-handling.patch  |   93 ++
 ...-arm64-KVM-Implement-vgic-v3-save-restore.patch |  255 ++++
 .../0051-arm64-KVM-Add-compatibility-aliases.patch |  118 ++
 ...64-KVM-Map-the-kernel-RO-section-into-HYP.patch |   44 +
 ...ove-away-from-the-assembly-version-of-the.patch | 1556 ++++++++++++++++++++
 ...4-Add-new-is_kernel_in_hyp_mode-predicate.patch |   78 +
 ...rm64-Add-ARM64_HAS_VIRT_HOST_EXTN-feature.patch |   80 +
 .../0056-arm64-KVM-VHE-Patch-out-use-of-HVC.patch  |  110 ++
 ...M-Turn-system-register-numbers-to-an-enum.patch |  349 +++++
 .../0058-arm64-KVM-Cleanup-asm-offset.c.patch      |   74 +
 .../0059-arm64-KVM-Remove-weak-attributes.patch    |  148 ++
 .../0060-ARM-KVM-Cleanup-exception-injection.patch |  137 ++
 ...M-debug-Remove-spurious-inline-attributes.patch |  156 ++
 ...rm64-KVM-Remove-unreferenced-S2_PGD_ORDER.patch |   64 +
 ...-Make-kvm_arm.h-friendly-to-assembly-code.patch |  102 ++
 ...064-arm64-KVM-Add-support-for-16-bit-VMID.patch |  162 ++
 ...arm64-KVM-Detect-vGIC-presence-at-runtime.patch |  101 ++
 ...rm64-KVM-Add-hook-for-C-based-stage2-init.patch |   63 +
 ...arm64-Fold-proc-macros.S-into-assembler.h.patch |  272 ++++
 tools/kdump/0068-arm64-Cleanup-SCTLR-flags.patch   |  104 ++
 ...l-Rework-finisher-callback-out-of-__cpu_s.patch |  348 +++++
 ...e-cpu_resume-to-enable-mmu-early-then-acc.patch |  378 +++++
 ...4-kernel-Include-_AC-definition-in-page.h.patch |   35 +
 ...te-KERNEL_START-KERNEL_END-definitions-to.patch |   50 +
 .../0073-arm64-Add-new-asm-macro-copy_page.patch   |   64 +
 ...e-Call-flush_icache_range-on-pages-restor.patch |   89 ++
 tools/kdump/0075-arm64-mm-move-pte_-macros.patch   |   73 +
 ...m-add-functions-to-walk-page-tables-by-PA.patch |  129 ++
 ...l-Add-support-for-hibernate-suspend-to-di.patch |  804 ++++++++++
 ...dd-support-for-read-only-sysfs-attributes.patch |   77 +
 ...nate-Refuse-to-hibernate-if-the-boot-cpu-.patch |   79 +
 ...0080-arm64-mm-avoid-redundant-__pa-__va-x.patch |   56 +
 .../0081-arm64-mm-add-__-pud-pgd-_populate.patch   |   86 ++
 ...mm-add-functions-to-walk-tables-in-fixmap.patch |  126 ++
 ...4-mm-use-fixmap-when-creating-page-tables.patch |  205 +++
 ...084-arm64-mm-allocate-pagetables-anywhere.patch |   89 ++
 ...4-mm-allow-passing-a-pgdir-to-alloc_init_.patch |  131 ++
 ...ensure-_stext-and-_etext-are-page-aligned.patch |   59 +
 ...-create-new-fine-grained-mappings-at-boot.patch |  304 ++++
 ...64-kernel-implement-ACPI-parking-protocol.patch |  443 ++++++
 ...-vmalloc-regions-to-be-set-with-set_memor.patch |   73 +
 ...4-Drop-alloc-function-from-create_mapping.patch |  114 ++
 ...support-for-ARCH_SUPPORTS_DEBUG_PAGEALLOC.patch |  170 +++
 ...p-Indicate-whether-memory-should-be-fault.patch |   41 +
 ...duce-KIMAGE_VADDR-as-the-virtual-base-of-.patch |   87 ++
 ...eal-with-kernel-symbols-outside-of-linear.patch |  164 +++
 ...ove-lr-save-restore-from-do_el2_call-into.patch |  100 ++
 ...96-arm64-hyp-kvm-Make-hyp-stub-extensible.patch |  166 +++
 ...hyp-kvm-Make-hyp-stub-reject-kvm_call_hyp.patch |   85 ++
 ...kip-HYP-setup-when-already-running-in-HYP.patch |  318 ++++
 ...egister-CPU-notifiers-when-the-kernel-run.patch |  119 ++
 .../0100-arm64-kvm-allows-kvm-cpu-hotplug.patch    |  461 ++++++
 ...4-Add-a-helper-for-parking-CPUs-in-a-loop.patch |   54 +
 .../kdump/0102-arm64-Introduce-cpu_die_early.patch |   83 ++
 .../0103-arm64-Move-cpu_die_early-to-smp.c.patch   |   98 ++
 ...0104-arm64-Handle-early-CPU-boot-failures.patch |  313 ++++
 .../0105-arm64-Add-cpu_panic_kernel-helper.patch   |   47 +
 ...ixup-arm64-Handle-early-CPU-boot-failures.patch |   48 +
 ...dd-function-to-determine-if-cpus-are-stuc.patch |   85 ++
 .../0108-arm64-Add-back-cpu-reset-routines.patch   |  170 +++
 .../0109-arm64-kexec-Add-core-kexec-support.patch  |  443 ++++++
 ...kexec-Enable-kexec-in-the-arm64-defconfig.patch |   27 +
 ...-add-MEMBLOCK_NOMAP-attribute-to-memblock.patch |  128 ++
 ....c-memblock_is_memory-reserved-can-be-boo.patch |   61 +
 ....c-add-new-infrastructure-to-address-the-.patch |  145 ++
 ...14-memblock-add-memblock_cap_memory_range.patch |  111 ++
 ...-memory-regions-based-on-DT-property-usab.patch |   74 +
 ...dump-reserve-memory-for-crash-dump-kernel.patch |  193 +++
 ...64-kdump-implement-machine_crash_shutdown.patch |  309 ++++
 ...ump-add-VMCOREINFO-s-for-user-space-tools.patch |   54 +
 ...0119-arm64-kdump-provide-proc-vmcore-file.patch |  224 +++
 ...120-arm64-kdump-enable-kdump-in-defconfig.patch |   28 +
 ...1-Documentation-kdump-describe-arm64-port.patch |   73 +
 ...tion-dt-chosen-properties-for-arm64-kdump.patch |   85 ++
 ...-kdump-add-VMCOREINFO-s-for-user-space-to.patch |   27 +
 123 files changed, 19980 insertions(+)
 create mode 100644 tools/kdump/0001-arm64-Add-HAVE_REGS_AND_STACK_ACCESS_API-feature.patch
 create mode 100644 tools/kdump/0002-arm64-Add-more-test-functions-to-insn.c.patch
 create mode 100644 tools/kdump/0003-arm64-add-conditional-instruction-simulation-support.patch
 create mode 100644 tools/kdump/0004-arm64-Kprobes-with-single-stepping-support.patch
 create mode 100644 tools/kdump/0005-arm64-Blacklist-non-kprobe-able-symbol.patch
 create mode 100644 tools/kdump/0006-arm64-Treat-all-entry-code-as-non-kprobe-able.patch
 create mode 100644 tools/kdump/0007-arm64-kprobes-instruction-simulation-support.patch
 create mode 100644 tools/kdump/0008-arm64-Add-trampoline-code-for-kretprobes.patch
 create mode 100644 tools/kdump/0009-arm64-Add-kernel-return-probes-support-kretprobes.patch
 create mode 100644 tools/kdump/0010-kprobes-Add-arm64-case-in-kprobe-example-module.patch
 create mode 100644 tools/kdump/0011-arm64-kprobes-WARN-if-attempting-to-step-with-PSTATE.patch
 create mode 100644 tools/kdump/0012-arm64-kprobes-Fix-overflow-when-saving-stack.patch
 create mode 100644 tools/kdump/0013-arm64-kprobes-Cleanup-jprobe_return.patch
 create mode 100644 tools/kdump/0014-arm64-kprobes-Add-KASAN-instrumentation-around-stack.patch
 create mode 100644 tools/kdump/0015-arm64-Remove-stack-duplicating-code-from-jprobes.patch
 create mode 100644 tools/kdump/0016-ARM-8458-1-bL_switcher-add-GIC-dependency.patch
 create mode 100644 tools/kdump/0017-ARM-8510-1-rework-ARM_CPU_SUSPEND-dependencies.patch
 create mode 100644 tools/kdump/0018-ARM-8478-2-arm-arm64-add-arm-smccc.patch
 create mode 100644 tools/kdump/0019-ARM-8479-2-add-implementation-for-arm-smccc.patch
 create mode 100644 tools/kdump/0020-ARM-8480-2-arm64-add-implementation-for-arm-smccc.patch
 create mode 100644 tools/kdump/0021-ARM-8481-2-drivers-psci-replace-psci-firmware-calls.patch
 create mode 100644 tools/kdump/0022-ARM-8511-1-ARM64-kernel-PSCI-move-PSCI-idle-manageme.patch
 create mode 100644 tools/kdump/0023-arm64-mm-fold-alternatives-into-.init.patch
 create mode 100644 tools/kdump/0024-asm-generic-Fix-local-variable-shadow-in-__set_fixma.patch
 create mode 100644 tools/kdump/0025-arm64-mm-remove-pointless-PAGE_MASKing.patch
 create mode 100644 tools/kdump/0026-arm64-mm-specialise-pagetable-allocators.patch
 create mode 100644 tools/kdump/0027-arm64-head.S-use-memset-to-clear-BSS.patch
 create mode 100644 tools/kdump/0028-arm64-mm-place-empty_zero_page-in-bss.patch
 create mode 100644 tools/kdump/0029-arm64-unify-idmap-removal.patch
 create mode 100644 tools/kdump/0030-arm64-unmap-idmap-earlier.patch
 create mode 100644 tools/kdump/0031-arm64-add-function-to-install-the-idmap.patch
 create mode 100644 tools/kdump/0032-arm64-mm-place-__cpu_setup-in-.text.patch
 create mode 100644 tools/kdump/0033-arm64-mm-add-code-to-safely-replace-TTBR1_EL1.patch
 create mode 100644 tools/kdump/0034-arm64-Use-PoU-cache-instr-for-I-D-coherency.patch
 create mode 100644 tools/kdump/0035-arm64-Add-macros-to-read-write-system-registers.patch
 create mode 100644 tools/kdump/0036-KVM-arm-arm64-vgic-v3-Make-the-LR-indexing-macro-pub.patch
 create mode 100644 tools/kdump/0037-arm64-KVM-Add-a-HYP-specific-header-file.patch
 create mode 100644 tools/kdump/0038-arm64-KVM-Implement-vgic-v2-save-restore.patch
 create mode 100644 tools/kdump/0039-arm64-KVM-Implement-timer-save-restore.patch
 create mode 100644 tools/kdump/0040-arm64-KVM-Implement-system-register-save-restore.patch
 create mode 100644 tools/kdump/0041-arm64-KVM-Implement-32bit-system-register-save-resto.patch
 create mode 100644 tools/kdump/0042-arm64-KVM-Implement-debug-save-restore.patch
 create mode 100644 tools/kdump/0043-arm64-KVM-Implement-guest-entry.patch
 create mode 100644 tools/kdump/0044-arm64-KVM-Add-patchable-function-selector.patch
 create mode 100644 tools/kdump/0045-arm64-KVM-Implement-the-core-world-switch.patch
 create mode 100644 tools/kdump/0046-arm64-KVM-Implement-fpsimd-save-restore.patch
 create mode 100644 tools/kdump/0047-arm64-KVM-Implement-TLB-handling.patch
 create mode 100644 tools/kdump/0048-arm64-KVM-HYP-mode-entry-points.patch
 create mode 100644 tools/kdump/0049-arm64-KVM-Add-panic-handling.patch
 create mode 100644 tools/kdump/0050-arm64-KVM-Implement-vgic-v3-save-restore.patch
 create mode 100644 tools/kdump/0051-arm64-KVM-Add-compatibility-aliases.patch
 create mode 100644 tools/kdump/0052-arm64-KVM-Map-the-kernel-RO-section-into-HYP.patch
 create mode 100644 tools/kdump/0053-arm64-KVM-Move-away-from-the-assembly-version-of-the.patch
 create mode 100644 tools/kdump/0054-arm-arm64-Add-new-is_kernel_in_hyp_mode-predicate.patch
 create mode 100644 tools/kdump/0055-arm64-Add-ARM64_HAS_VIRT_HOST_EXTN-feature.patch
 create mode 100644 tools/kdump/0056-arm64-KVM-VHE-Patch-out-use-of-HVC.patch
 create mode 100644 tools/kdump/0057-arm64-KVM-Turn-system-register-numbers-to-an-enum.patch
 create mode 100644 tools/kdump/0058-arm64-KVM-Cleanup-asm-offset.c.patch
 create mode 100644 tools/kdump/0059-arm64-KVM-Remove-weak-attributes.patch
 create mode 100644 tools/kdump/0060-ARM-KVM-Cleanup-exception-injection.patch
 create mode 100644 tools/kdump/0061-arm64-KVM-debug-Remove-spurious-inline-attributes.patch
 create mode 100644 tools/kdump/0062-arm-arm64-KVM-Remove-unreferenced-S2_PGD_ORDER.patch
 create mode 100644 tools/kdump/0063-arm-KVM-Make-kvm_arm.h-friendly-to-assembly-code.patch
 create mode 100644 tools/kdump/0064-arm64-KVM-Add-support-for-16-bit-VMID.patch
 create mode 100644 tools/kdump/0065-arm-arm64-KVM-Detect-vGIC-presence-at-runtime.patch
 create mode 100644 tools/kdump/0066-arm-arm64-KVM-Add-hook-for-C-based-stage2-init.patch
 create mode 100644 tools/kdump/0067-arm64-Fold-proc-macros.S-into-assembler.h.patch
 create mode 100644 tools/kdump/0068-arm64-Cleanup-SCTLR-flags.patch
 create mode 100644 tools/kdump/0069-arm64-kernel-Rework-finisher-callback-out-of-__cpu_s.patch
 create mode 100644 tools/kdump/0070-arm64-Change-cpu_resume-to-enable-mmu-early-then-acc.patch
 create mode 100644 tools/kdump/0071-arm64-kernel-Include-_AC-definition-in-page.h.patch
 create mode 100644 tools/kdump/0072-arm64-Promote-KERNEL_START-KERNEL_END-definitions-to.patch
 create mode 100644 tools/kdump/0073-arm64-Add-new-asm-macro-copy_page.patch
 create mode 100644 tools/kdump/0074-PM-Hibernate-Call-flush_icache_range-on-pages-restor.patch
 create mode 100644 tools/kdump/0075-arm64-mm-move-pte_-macros.patch
 create mode 100644 tools/kdump/0076-arm64-mm-add-functions-to-walk-page-tables-by-PA.patch
 create mode 100644 tools/kdump/0077-arm64-kernel-Add-support-for-hibernate-suspend-to-di.patch
 create mode 100644 tools/kdump/0078-PM-sleep-Add-support-for-read-only-sysfs-attributes.patch
 create mode 100644 tools/kdump/0079-arm64-hibernate-Refuse-to-hibernate-if-the-boot-cpu-.patch
 create mode 100644 tools/kdump/0080-arm64-mm-avoid-redundant-__pa-__va-x.patch
 create mode 100644 tools/kdump/0081-arm64-mm-add-__-pud-pgd-_populate.patch
 create mode 100644 tools/kdump/0082-arm64-mm-add-functions-to-walk-tables-in-fixmap.patch
 create mode 100644 tools/kdump/0083-arm64-mm-use-fixmap-when-creating-page-tables.patch
 create mode 100644 tools/kdump/0084-arm64-mm-allocate-pagetables-anywhere.patch
 create mode 100644 tools/kdump/0085-arm64-mm-allow-passing-a-pgdir-to-alloc_init_.patch
 create mode 100644 tools/kdump/0086-arm64-ensure-_stext-and-_etext-are-page-aligned.patch
 create mode 100644 tools/kdump/0087-arm64-mm-create-new-fine-grained-mappings-at-boot.patch
 create mode 100644 tools/kdump/0088-arm64-kernel-implement-ACPI-parking-protocol.patch
 create mode 100644 tools/kdump/0089-arm64-allow-vmalloc-regions-to-be-set-with-set_memor.patch
 create mode 100644 tools/kdump/0090-arm64-Drop-alloc-function-from-create_mapping.patch
 create mode 100644 tools/kdump/0091-arm64-Add-support-for-ARCH_SUPPORTS_DEBUG_PAGEALLOC.patch
 create mode 100644 tools/kdump/0092-arm64-ptdump-Indicate-whether-memory-should-be-fault.patch
 create mode 100644 tools/kdump/0093-arm64-introduce-KIMAGE_VADDR-as-the-virtual-base-of-.patch
 create mode 100644 tools/kdump/0094-arm64-kvm-deal-with-kernel-symbols-outside-of-linear.patch
 create mode 100644 tools/kdump/0095-arm64-kvm-Move-lr-save-restore-from-do_el2_call-into.patch
 create mode 100644 tools/kdump/0096-arm64-hyp-kvm-Make-hyp-stub-extensible.patch
 create mode 100644 tools/kdump/0097-arm64-hyp-kvm-Make-hyp-stub-reject-kvm_call_hyp.patch
 create mode 100644 tools/kdump/0098-arm64-KVM-Skip-HYP-setup-when-already-running-in-HYP.patch
 create mode 100644 tools/kdump/0099-arm64-KVM-Register-CPU-notifiers-when-the-kernel-run.patch
 create mode 100644 tools/kdump/0100-arm64-kvm-allows-kvm-cpu-hotplug.patch
 create mode 100644 tools/kdump/0101-arm64-Add-a-helper-for-parking-CPUs-in-a-loop.patch
 create mode 100644 tools/kdump/0102-arm64-Introduce-cpu_die_early.patch
 create mode 100644 tools/kdump/0103-arm64-Move-cpu_die_early-to-smp.c.patch
 create mode 100644 tools/kdump/0104-arm64-Handle-early-CPU-boot-failures.patch
 create mode 100644 tools/kdump/0105-arm64-Add-cpu_panic_kernel-helper.patch
 create mode 100644 tools/kdump/0106-fixup-arm64-Handle-early-CPU-boot-failures.patch
 create mode 100644 tools/kdump/0107-arm64-smp-Add-function-to-determine-if-cpus-are-stuc.patch
 create mode 100644 tools/kdump/0108-arm64-Add-back-cpu-reset-routines.patch
 create mode 100644 tools/kdump/0109-arm64-kexec-Add-core-kexec-support.patch
 create mode 100644 tools/kdump/0110-arm64-kexec-Enable-kexec-in-the-arm64-defconfig.patch
 create mode 100644 tools/kdump/0111-mm-memblock-add-MEMBLOCK_NOMAP-attribute-to-memblock.patch
 create mode 100644 tools/kdump/0112-mm-memblock.c-memblock_is_memory-reserved-can-be-boo.patch
 create mode 100644 tools/kdump/0113-mm-memblock.c-add-new-infrastructure-to-address-the-.patch
 create mode 100644 tools/kdump/0114-memblock-add-memblock_cap_memory_range.patch
 create mode 100644 tools/kdump/0115-arm64-limit-memory-regions-based-on-DT-property-usab.patch
 create mode 100644 tools/kdump/0116-arm64-kdump-reserve-memory-for-crash-dump-kernel.patch
 create mode 100644 tools/kdump/0117-arm64-kdump-implement-machine_crash_shutdown.patch
 create mode 100644 tools/kdump/0118-arm64-kdump-add-VMCOREINFO-s-for-user-space-tools.patch
 create mode 100644 tools/kdump/0119-arm64-kdump-provide-proc-vmcore-file.patch
 create mode 100644 tools/kdump/0120-arm64-kdump-enable-kdump-in-defconfig.patch
 create mode 100644 tools/kdump/0121-Documentation-kdump-describe-arm64-port.patch
 create mode 100644 tools/kdump/0122-Documentation-dt-chosen-properties-for-arm64-kdump.patch
 create mode 100644 tools/kdump/0123-fixup-arm64-kdump-add-VMCOREINFO-s-for-user-space-to.patch

diff --git a/tools/kdump/0001-arm64-Add-HAVE_REGS_AND_STACK_ACCESS_API-feature.patch b/tools/kdump/0001-arm64-Add-HAVE_REGS_AND_STACK_ACCESS_API-feature.patch
new file mode 100644
index 0000000..285b3c4
--- /dev/null
+++ b/tools/kdump/0001-arm64-Add-HAVE_REGS_AND_STACK_ACCESS_API-feature.patch
@@ -0,0 +1,227 @@
+From cb9f9305b6f6c5f216e57fe38589be55ca586134 Mon Sep 17 00:00:00 2001
+From: "David A. Long" <dave.long@linaro.org>
+Date: Wed, 28 Sep 2016 15:30:52 -0400
+Subject: [PATCH 001/123] arm64: Add HAVE_REGS_AND_STACK_ACCESS_API feature
+
+commit 0a8ea52c3eb157dd65e224fc95b7c9c99fcba9f7 upstream.
+
+Add HAVE_REGS_AND_STACK_ACCESS_API feature for arm64, including supporting
+functions and defines.
+
+[dave.long@linaro.org: Remove irq stack reference and use of bug.h
+inside arch/arm64/include/asm/ptrace.h. ]
+
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+[catalin.marinas@arm.com: Remove unused functions]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm/include/asm/ptrace.h   |   1 -
+ arch/arm64/Kconfig              |   1 +
+ arch/arm64/include/asm/ptrace.h |  47 +++++++++++++++++++
+ arch/arm64/kernel/ptrace.c      | 100 ++++++++++++++++++++++++++++++++++++++++
+ 4 files changed, 148 insertions(+), 1 deletion(-)
+
+diff --git a/arch/arm/include/asm/ptrace.h b/arch/arm/include/asm/ptrace.h
+index 51622ba..d3c0c23 100644
+--- a/arch/arm/include/asm/ptrace.h
++++ b/arch/arm/include/asm/ptrace.h
+@@ -121,7 +121,6 @@ extern unsigned long profile_pc(struct pt_regs *regs);
+ #define MAX_REG_OFFSET (offsetof(struct pt_regs, ARM_ORIG_r0))
+ 
+ extern int regs_query_register_offset(const char *name);
+-extern const char *regs_query_register_name(unsigned int offset);
+ extern bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr);
+ extern unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
+ 					       unsigned int n);
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index d9de79c..0290331 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -75,6 +75,7 @@ config ARM64
+ 	select HAVE_PERF_EVENTS
+ 	select HAVE_PERF_REGS
+ 	select HAVE_PERF_USER_STACK_DUMP
++	select HAVE_REGS_AND_STACK_ACCESS_API
+ 	select HAVE_RCU_TABLE_FREE
+ 	select HAVE_SYSCALL_TRACEPOINTS
+ 	select IOMMU_DMA if IOMMU_SUPPORT
+diff --git a/arch/arm64/include/asm/ptrace.h b/arch/arm64/include/asm/ptrace.h
+index 7f94755..cfe2b37 100644
+--- a/arch/arm64/include/asm/ptrace.h
++++ b/arch/arm64/include/asm/ptrace.h
+@@ -121,6 +121,8 @@ struct pt_regs {
+ 	u64 unused;	// maintain 16 byte alignment
+ };
+ 
++#define MAX_REG_OFFSET offsetof(struct pt_regs, pstate)
++
+ #define arch_has_single_step()	(1)
+ 
+ #ifdef CONFIG_COMPAT
+@@ -149,6 +151,51 @@ struct pt_regs {
+ #define user_stack_pointer(regs) \
+ 	(!compat_user_mode(regs) ? (regs)->sp : (regs)->compat_sp)
+ 
++extern int regs_query_register_offset(const char *name);
++extern const char *regs_query_register_name(unsigned int offset);
++extern unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
++					       unsigned int n);
++
++/**
++ * regs_get_register() - get register value from its offset
++ * @regs:	pt_regs from which register value is gotten
++ * @offset:	offset of the register.
++ *
++ * regs_get_register returns the value of a register whose offset from @regs.
++ * The @offset is the offset of the register in struct pt_regs.
++ * If @offset is bigger than MAX_REG_OFFSET, this returns 0.
++ */
++static inline u64 regs_get_register(struct pt_regs *regs, unsigned int offset)
++{
++	u64 val = 0;
++
++	offset >>= 3;
++	switch (offset) {
++	case 0 ... 30:
++		val = regs->regs[offset];
++		break;
++	case offsetof(struct pt_regs, sp) >> 3:
++		val = regs->sp;
++		break;
++	case offsetof(struct pt_regs, pc) >> 3:
++		val = regs->pc;
++		break;
++	case offsetof(struct pt_regs, pstate) >> 3:
++		val = regs->pstate;
++		break;
++	default:
++		val = 0;
++	}
++
++	return val;
++}
++
++/* Valid only for Kernel mode traps. */
++static inline unsigned long kernel_stack_pointer(struct pt_regs *regs)
++{
++	return regs->sp;
++}
++
+ static inline unsigned long regs_return_value(struct pt_regs *regs)
+ {
+ 	return regs->regs[0];
+diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
+index 55909b2..c5ef059 100644
+--- a/arch/arm64/kernel/ptrace.c
++++ b/arch/arm64/kernel/ptrace.c
+@@ -49,6 +49,106 @@
+ #define CREATE_TRACE_POINTS
+ #include <trace/events/syscalls.h>
+ 
++struct pt_regs_offset {
++	const char *name;
++	int offset;
++};
++
++#define REG_OFFSET_NAME(r) {.name = #r, .offset = offsetof(struct pt_regs, r)}
++#define REG_OFFSET_END {.name = NULL, .offset = 0}
++#define GPR_OFFSET_NAME(r) \
++	{.name = "x" #r, .offset = offsetof(struct pt_regs, regs[r])}
++
++static const struct pt_regs_offset regoffset_table[] = {
++	GPR_OFFSET_NAME(0),
++	GPR_OFFSET_NAME(1),
++	GPR_OFFSET_NAME(2),
++	GPR_OFFSET_NAME(3),
++	GPR_OFFSET_NAME(4),
++	GPR_OFFSET_NAME(5),
++	GPR_OFFSET_NAME(6),
++	GPR_OFFSET_NAME(7),
++	GPR_OFFSET_NAME(8),
++	GPR_OFFSET_NAME(9),
++	GPR_OFFSET_NAME(10),
++	GPR_OFFSET_NAME(11),
++	GPR_OFFSET_NAME(12),
++	GPR_OFFSET_NAME(13),
++	GPR_OFFSET_NAME(14),
++	GPR_OFFSET_NAME(15),
++	GPR_OFFSET_NAME(16),
++	GPR_OFFSET_NAME(17),
++	GPR_OFFSET_NAME(18),
++	GPR_OFFSET_NAME(19),
++	GPR_OFFSET_NAME(20),
++	GPR_OFFSET_NAME(21),
++	GPR_OFFSET_NAME(22),
++	GPR_OFFSET_NAME(23),
++	GPR_OFFSET_NAME(24),
++	GPR_OFFSET_NAME(25),
++	GPR_OFFSET_NAME(26),
++	GPR_OFFSET_NAME(27),
++	GPR_OFFSET_NAME(28),
++	GPR_OFFSET_NAME(29),
++	GPR_OFFSET_NAME(30),
++	{.name = "lr", .offset = offsetof(struct pt_regs, regs[30])},
++	REG_OFFSET_NAME(sp),
++	REG_OFFSET_NAME(pc),
++	REG_OFFSET_NAME(pstate),
++	REG_OFFSET_END,
++};
++
++/**
++ * regs_query_register_offset() - query register offset from its name
++ * @name:	the name of a register
++ *
++ * regs_query_register_offset() returns the offset of a register in struct
++ * pt_regs from its name. If the name is invalid, this returns -EINVAL;
++ */
++int regs_query_register_offset(const char *name)
++{
++	const struct pt_regs_offset *roff;
++
++	for (roff = regoffset_table; roff->name != NULL; roff++)
++		if (!strcmp(roff->name, name))
++			return roff->offset;
++	return -EINVAL;
++}
++
++/**
++ * regs_within_kernel_stack() - check the address in the stack
++ * @regs:      pt_regs which contains kernel stack pointer.
++ * @addr:      address which is checked.
++ *
++ * regs_within_kernel_stack() checks @addr is within the kernel stack page(s).
++ * If @addr is within the kernel stack, it returns true. If not, returns false.
++ */
++static bool regs_within_kernel_stack(struct pt_regs *regs, unsigned long addr)
++{
++	return ((addr & ~(THREAD_SIZE - 1))  ==
++		(kernel_stack_pointer(regs) & ~(THREAD_SIZE - 1)));
++}
++
++/**
++ * regs_get_kernel_stack_nth() - get Nth entry of the stack
++ * @regs:	pt_regs which contains kernel stack pointer.
++ * @n:		stack entry number.
++ *
++ * regs_get_kernel_stack_nth() returns @n th entry of the kernel stack which
++ * is specified by @regs. If the @n th entry is NOT in the kernel stack,
++ * this returns 0.
++ */
++unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs, unsigned int n)
++{
++	unsigned long *addr = (unsigned long *)kernel_stack_pointer(regs);
++
++	addr += n;
++	if (regs_within_kernel_stack(regs, (unsigned long)addr))
++		return *addr;
++	else
++		return 0;
++}
++
+ /*
+  * TODO: does not yet catch signals sent when the child dies.
+  * in exit.c or in signal.c.
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0002-arm64-Add-more-test-functions-to-insn.c.patch b/tools/kdump/0002-arm64-Add-more-test-functions-to-insn.c.patch
new file mode 100644
index 0000000..900dde2
--- /dev/null
+++ b/tools/kdump/0002-arm64-Add-more-test-functions-to-insn.c.patch
@@ -0,0 +1,157 @@
+From 7d25dbec4d21659ea371b8150fead919151b3c94 Mon Sep 17 00:00:00 2001
+From: "David A. Long" <dave.long@linaro.org>
+Date: Fri, 8 Jul 2016 12:35:46 -0400
+Subject: [PATCH 002/123] arm64: Add more test functions to insn.c
+
+commit d59bee887231191c80f2ee674d7ec19179eb40ec upstream.
+
+Certain instructions are hard to execute correctly out-of-line (as in
+kprobes).  Test functions are added to insn.[hc] to identify these.  The
+instructions include any that use PC-relative addressing, change the PC,
+or change interrupt masking. For efficiency and simplicity test
+functions are also added for small collections of related instructions.
+
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/include/asm/insn.h | 36 ++++++++++++++++++++++++++++++++++++
+ arch/arm64/kernel/insn.c      | 34 ++++++++++++++++++++++++++++++++++
+ 2 files changed, 70 insertions(+)
+
+diff --git a/arch/arm64/include/asm/insn.h b/arch/arm64/include/asm/insn.h
+index 30e50eb..497f7a2 100644
+--- a/arch/arm64/include/asm/insn.h
++++ b/arch/arm64/include/asm/insn.h
+@@ -120,6 +120,29 @@ enum aarch64_insn_register {
+ 	AARCH64_INSN_REG_SP = 31  /* Stack pointer: as load/store base reg */
+ };
+ 
++enum aarch64_insn_special_register {
++	AARCH64_INSN_SPCLREG_SPSR_EL1	= 0xC200,
++	AARCH64_INSN_SPCLREG_ELR_EL1	= 0xC201,
++	AARCH64_INSN_SPCLREG_SP_EL0	= 0xC208,
++	AARCH64_INSN_SPCLREG_SPSEL	= 0xC210,
++	AARCH64_INSN_SPCLREG_CURRENTEL	= 0xC212,
++	AARCH64_INSN_SPCLREG_DAIF	= 0xDA11,
++	AARCH64_INSN_SPCLREG_NZCV	= 0xDA10,
++	AARCH64_INSN_SPCLREG_FPCR	= 0xDA20,
++	AARCH64_INSN_SPCLREG_DSPSR_EL0	= 0xDA28,
++	AARCH64_INSN_SPCLREG_DLR_EL0	= 0xDA29,
++	AARCH64_INSN_SPCLREG_SPSR_EL2	= 0xE200,
++	AARCH64_INSN_SPCLREG_ELR_EL2	= 0xE201,
++	AARCH64_INSN_SPCLREG_SP_EL1	= 0xE208,
++	AARCH64_INSN_SPCLREG_SPSR_INQ	= 0xE218,
++	AARCH64_INSN_SPCLREG_SPSR_ABT	= 0xE219,
++	AARCH64_INSN_SPCLREG_SPSR_UND	= 0xE21A,
++	AARCH64_INSN_SPCLREG_SPSR_FIQ	= 0xE21B,
++	AARCH64_INSN_SPCLREG_SPSR_EL3	= 0xF200,
++	AARCH64_INSN_SPCLREG_ELR_EL3	= 0xF201,
++	AARCH64_INSN_SPCLREG_SP_EL2	= 0xF210
++};
++
+ enum aarch64_insn_variant {
+ 	AARCH64_INSN_VARIANT_32BIT,
+ 	AARCH64_INSN_VARIANT_64BIT
+@@ -223,8 +246,13 @@ static __always_inline bool aarch64_insn_is_##abbr(u32 code) \
+ static __always_inline u32 aarch64_insn_get_##abbr##_value(void) \
+ { return (val); }
+ 
++__AARCH64_INSN_FUNCS(adr_adrp,	0x1F000000, 0x10000000)
++__AARCH64_INSN_FUNCS(prfm_lit,	0xFF000000, 0xD8000000)
+ __AARCH64_INSN_FUNCS(str_reg,	0x3FE0EC00, 0x38206800)
+ __AARCH64_INSN_FUNCS(ldr_reg,	0x3FE0EC00, 0x38606800)
++__AARCH64_INSN_FUNCS(ldr_lit,	0xBF000000, 0x18000000)
++__AARCH64_INSN_FUNCS(ldrsw_lit,	0xFF000000, 0x98000000)
++__AARCH64_INSN_FUNCS(exclusive,	0x3F800000, 0x08000000)
+ __AARCH64_INSN_FUNCS(stp_post,	0x7FC00000, 0x28800000)
+ __AARCH64_INSN_FUNCS(ldp_post,	0x7FC00000, 0x28C00000)
+ __AARCH64_INSN_FUNCS(stp_pre,	0x7FC00000, 0x29800000)
+@@ -273,10 +301,15 @@ __AARCH64_INSN_FUNCS(svc,	0xFFE0001F, 0xD4000001)
+ __AARCH64_INSN_FUNCS(hvc,	0xFFE0001F, 0xD4000002)
+ __AARCH64_INSN_FUNCS(smc,	0xFFE0001F, 0xD4000003)
+ __AARCH64_INSN_FUNCS(brk,	0xFFE0001F, 0xD4200000)
++__AARCH64_INSN_FUNCS(exception,	0xFF000000, 0xD4000000)
+ __AARCH64_INSN_FUNCS(hint,	0xFFFFF01F, 0xD503201F)
+ __AARCH64_INSN_FUNCS(br,	0xFFFFFC1F, 0xD61F0000)
+ __AARCH64_INSN_FUNCS(blr,	0xFFFFFC1F, 0xD63F0000)
+ __AARCH64_INSN_FUNCS(ret,	0xFFFFFC1F, 0xD65F0000)
++__AARCH64_INSN_FUNCS(eret,	0xFFFFFFFF, 0xD69F03E0)
++__AARCH64_INSN_FUNCS(mrs,	0xFFF00000, 0xD5300000)
++__AARCH64_INSN_FUNCS(msr_imm,	0xFFF8F01F, 0xD500401F)
++__AARCH64_INSN_FUNCS(msr_reg,	0xFFF00000, 0xD5100000)
+ 
+ #undef	__AARCH64_INSN_FUNCS
+ 
+@@ -286,6 +319,8 @@ bool aarch64_insn_is_branch_imm(u32 insn);
+ int aarch64_insn_read(void *addr, u32 *insnp);
+ int aarch64_insn_write(void *addr, u32 insn);
+ enum aarch64_insn_encoding_class aarch64_get_insn_class(u32 insn);
++bool aarch64_insn_uses_literal(u32 insn);
++bool aarch64_insn_is_branch(u32 insn);
+ u64 aarch64_insn_decode_immediate(enum aarch64_insn_imm_type type, u32 insn);
+ u32 aarch64_insn_encode_immediate(enum aarch64_insn_imm_type type,
+ 				  u32 insn, u64 imm);
+@@ -367,6 +402,7 @@ bool aarch32_insn_is_wide(u32 insn);
+ #define A32_RT_OFFSET	12
+ #define A32_RT2_OFFSET	 0
+ 
++u32 aarch64_insn_extract_system_reg(u32 insn);
+ u32 aarch32_insn_extract_reg_num(u32 insn, int offset);
+ u32 aarch32_insn_mcr_extract_opc2(u32 insn);
+ u32 aarch32_insn_mcr_extract_crm(u32 insn);
+diff --git a/arch/arm64/kernel/insn.c b/arch/arm64/kernel/insn.c
+index c08b9ad..f8e05b6 100644
+--- a/arch/arm64/kernel/insn.c
++++ b/arch/arm64/kernel/insn.c
+@@ -162,6 +162,32 @@ static bool __kprobes __aarch64_insn_hotpatch_safe(u32 insn)
+ 		aarch64_insn_is_nop(insn);
+ }
+ 
++bool __kprobes aarch64_insn_uses_literal(u32 insn)
++{
++	/* ldr/ldrsw (literal), prfm */
++
++	return aarch64_insn_is_ldr_lit(insn) ||
++		aarch64_insn_is_ldrsw_lit(insn) ||
++		aarch64_insn_is_adr_adrp(insn) ||
++		aarch64_insn_is_prfm_lit(insn);
++}
++
++bool __kprobes aarch64_insn_is_branch(u32 insn)
++{
++	/* b, bl, cb*, tb*, b.cond, br, blr */
++
++	return aarch64_insn_is_b(insn) ||
++		aarch64_insn_is_bl(insn) ||
++		aarch64_insn_is_cbz(insn) ||
++		aarch64_insn_is_cbnz(insn) ||
++		aarch64_insn_is_tbz(insn) ||
++		aarch64_insn_is_tbnz(insn) ||
++		aarch64_insn_is_ret(insn) ||
++		aarch64_insn_is_br(insn) ||
++		aarch64_insn_is_blr(insn) ||
++		aarch64_insn_is_bcond(insn);
++}
++
+ /*
+  * ARM Architecture Reference Manual for ARMv8 Profile-A, Issue A.a
+  * Section B2.6.5 "Concurrent modification and execution of instructions":
+@@ -1116,6 +1142,14 @@ u32 aarch64_set_branch_offset(u32 insn, s32 offset)
+ 	BUG();
+ }
+ 
++/*
++ * Extract the Op/CR data from a msr/mrs instruction.
++ */
++u32 aarch64_insn_extract_system_reg(u32 insn)
++{
++	return (insn & 0x1FFFE0) >> 5;
++}
++
+ bool aarch32_insn_is_wide(u32 insn)
+ {
+ 	return insn >= 0xe800;
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0003-arm64-add-conditional-instruction-simulation-support.patch b/tools/kdump/0003-arm64-add-conditional-instruction-simulation-support.patch
new file mode 100644
index 0000000..bfbbd90
--- /dev/null
+++ b/tools/kdump/0003-arm64-add-conditional-instruction-simulation-support.patch
@@ -0,0 +1,206 @@
+From 8813d2a42d0f06384a0dfb6a750b7876b5e18b3d Mon Sep 17 00:00:00 2001
+From: "David A. Long" <dave.long@linaro.org>
+Date: Thu, 29 Sep 2016 17:28:13 -0400
+Subject: [PATCH 003/123] arm64: add conditional instruction simulation support
+
+commit 2af3ec08b414ceb9c32fad2bb0f87252f3f18de8 upstream.
+
+Cease using the arm32 arm_check_condition() function and replace it with
+a local version for use in deprecated instruction support on arm64. Also
+make the function table used by this available for future use by kprobes
+and/or uprobes.
+
+This function is derived from code written by Sandeepa Prabhu.
+
+Signed-off-by: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/include/asm/insn.h        |  3 ++
+ arch/arm64/kernel/Makefile           |  3 +-
+ arch/arm64/kernel/armv8_deprecated.c | 19 ++++++-
+ arch/arm64/kernel/insn.c             | 98 ++++++++++++++++++++++++++++++++++++
+ 4 files changed, 119 insertions(+), 4 deletions(-)
+
+diff --git a/arch/arm64/include/asm/insn.h b/arch/arm64/include/asm/insn.h
+index 497f7a2..a44abbd 100644
+--- a/arch/arm64/include/asm/insn.h
++++ b/arch/arm64/include/asm/insn.h
+@@ -406,6 +406,9 @@ u32 aarch64_insn_extract_system_reg(u32 insn);
+ u32 aarch32_insn_extract_reg_num(u32 insn, int offset);
+ u32 aarch32_insn_mcr_extract_opc2(u32 insn);
+ u32 aarch32_insn_mcr_extract_crm(u32 insn);
++
++typedef bool (pstate_check_t)(unsigned long);
++extern pstate_check_t * const aarch32_opcode_cond_checks[16];
+ #endif /* __ASSEMBLY__ */
+ 
+ #endif	/* __ASM_INSN_H */
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index 474691f..fed7fc5 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -26,8 +26,7 @@ $(obj)/%.stub.o: $(obj)/%.o FORCE
+ 	$(call if_changed,objcopy)
+ 
+ arm64-obj-$(CONFIG_COMPAT)		+= sys32.o kuser32.o signal32.o 	\
+-					   sys_compat.o entry32.o		\
+-					   ../../arm/kernel/opcodes.o
++					   sys_compat.o entry32.o
+ arm64-obj-$(CONFIG_FUNCTION_TRACER)	+= ftrace.o entry-ftrace.o
+ arm64-obj-$(CONFIG_MODULES)		+= arm64ksyms.o module.o
+ arm64-obj-$(CONFIG_PERF_EVENTS)		+= perf_regs.o perf_callchain.o
+diff --git a/arch/arm64/kernel/armv8_deprecated.c b/arch/arm64/kernel/armv8_deprecated.c
+index 937f5e5..313404b 100644
+--- a/arch/arm64/kernel/armv8_deprecated.c
++++ b/arch/arm64/kernel/armv8_deprecated.c
+@@ -369,6 +369,21 @@ static int emulate_swpX(unsigned int address, unsigned int *data,
+ 	return res;
+ }
+ 
++#define	ARM_OPCODE_CONDITION_UNCOND	0xf
++
++static unsigned int __kprobes aarch32_check_condition(u32 opcode, u32 psr)
++{
++	u32 cc_bits  = opcode >> 28;
++
++	if (cc_bits != ARM_OPCODE_CONDITION_UNCOND) {
++		if ((*aarch32_opcode_cond_checks[cc_bits])(psr))
++			return ARM_OPCODE_CONDTEST_PASS;
++		else
++			return ARM_OPCODE_CONDTEST_FAIL;
++	}
++	return ARM_OPCODE_CONDTEST_UNCOND;
++}
++
+ /*
+  * swp_handler logs the id of calling process, dissects the instruction, sanity
+  * checks the memory location, calls emulate_swpX for the actual operation and
+@@ -383,7 +398,7 @@ static int swp_handler(struct pt_regs *regs, u32 instr)
+ 
+ 	type = instr & TYPE_SWPB;
+ 
+-	switch (arm_check_condition(instr, regs->pstate)) {
++	switch (aarch32_check_condition(instr, regs->pstate)) {
+ 	case ARM_OPCODE_CONDTEST_PASS:
+ 		break;
+ 	case ARM_OPCODE_CONDTEST_FAIL:
+@@ -464,7 +479,7 @@ static int cp15barrier_handler(struct pt_regs *regs, u32 instr)
+ {
+ 	perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, regs->pc);
+ 
+-	switch (arm_check_condition(instr, regs->pstate)) {
++	switch (aarch32_check_condition(instr, regs->pstate)) {
+ 	case ARM_OPCODE_CONDTEST_PASS:
+ 		break;
+ 	case ARM_OPCODE_CONDTEST_FAIL:
+diff --git a/arch/arm64/kernel/insn.c b/arch/arm64/kernel/insn.c
+index f8e05b6..b438070 100644
+--- a/arch/arm64/kernel/insn.c
++++ b/arch/arm64/kernel/insn.c
+@@ -1175,3 +1175,101 @@ u32 aarch32_insn_mcr_extract_crm(u32 insn)
+ {
+ 	return insn & CRM_MASK;
+ }
++
++static bool __kprobes __check_eq(unsigned long pstate)
++{
++	return (pstate & PSR_Z_BIT) != 0;
++}
++
++static bool __kprobes __check_ne(unsigned long pstate)
++{
++	return (pstate & PSR_Z_BIT) == 0;
++}
++
++static bool __kprobes __check_cs(unsigned long pstate)
++{
++	return (pstate & PSR_C_BIT) != 0;
++}
++
++static bool __kprobes __check_cc(unsigned long pstate)
++{
++	return (pstate & PSR_C_BIT) == 0;
++}
++
++static bool __kprobes __check_mi(unsigned long pstate)
++{
++	return (pstate & PSR_N_BIT) != 0;
++}
++
++static bool __kprobes __check_pl(unsigned long pstate)
++{
++	return (pstate & PSR_N_BIT) == 0;
++}
++
++static bool __kprobes __check_vs(unsigned long pstate)
++{
++	return (pstate & PSR_V_BIT) != 0;
++}
++
++static bool __kprobes __check_vc(unsigned long pstate)
++{
++	return (pstate & PSR_V_BIT) == 0;
++}
++
++static bool __kprobes __check_hi(unsigned long pstate)
++{
++	pstate &= ~(pstate >> 1);	/* PSR_C_BIT &= ~PSR_Z_BIT */
++	return (pstate & PSR_C_BIT) != 0;
++}
++
++static bool __kprobes __check_ls(unsigned long pstate)
++{
++	pstate &= ~(pstate >> 1);	/* PSR_C_BIT &= ~PSR_Z_BIT */
++	return (pstate & PSR_C_BIT) == 0;
++}
++
++static bool __kprobes __check_ge(unsigned long pstate)
++{
++	pstate ^= (pstate << 3);	/* PSR_N_BIT ^= PSR_V_BIT */
++	return (pstate & PSR_N_BIT) == 0;
++}
++
++static bool __kprobes __check_lt(unsigned long pstate)
++{
++	pstate ^= (pstate << 3);	/* PSR_N_BIT ^= PSR_V_BIT */
++	return (pstate & PSR_N_BIT) != 0;
++}
++
++static bool __kprobes __check_gt(unsigned long pstate)
++{
++	/*PSR_N_BIT ^= PSR_V_BIT */
++	unsigned long temp = pstate ^ (pstate << 3);
++
++	temp |= (pstate << 1);	/*PSR_N_BIT |= PSR_Z_BIT */
++	return (temp & PSR_N_BIT) == 0;
++}
++
++static bool __kprobes __check_le(unsigned long pstate)
++{
++	/*PSR_N_BIT ^= PSR_V_BIT */
++	unsigned long temp = pstate ^ (pstate << 3);
++
++	temp |= (pstate << 1);	/*PSR_N_BIT |= PSR_Z_BIT */
++	return (temp & PSR_N_BIT) != 0;
++}
++
++static bool __kprobes __check_al(unsigned long pstate)
++{
++	return true;
++}
++
++/*
++ * Note that the ARMv8 ARM calls condition code 0b1111 "nv", but states that
++ * it behaves identically to 0b1110 ("al").
++ */
++pstate_check_t * const aarch32_opcode_cond_checks[16] = {
++	__check_eq, __check_ne, __check_cs, __check_cc,
++	__check_mi, __check_pl, __check_vs, __check_vc,
++	__check_hi, __check_ls, __check_ge, __check_lt,
++	__check_gt, __check_le, __check_al, __check_al
++};
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0004-arm64-Kprobes-with-single-stepping-support.patch b/tools/kdump/0004-arm64-Kprobes-with-single-stepping-support.patch
new file mode 100644
index 0000000..77306b7
--- /dev/null
+++ b/tools/kdump/0004-arm64-Kprobes-with-single-stepping-support.patch
@@ -0,0 +1,1110 @@
+From e7aea7fc28fb820c5d847d86f96c2fa56ca8c067 Mon Sep 17 00:00:00 2001
+From: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Date: Thu, 29 Sep 2016 17:47:39 -0400
+Subject: [PATCH 004/123] arm64: Kprobes with single stepping support
+
+commit 2dd0e8d2d2a157dbc83295a78336c2217110f2f8 upstream.
+
+Add support for basic kernel probes(kprobes) and jump probes
+(jprobes) for ARM64.
+
+Kprobes utilizes software breakpoint and single step debug
+exceptions supported on ARM v8.
+
+A software breakpoint is placed at the probe address to trap the
+kernel execution into the kprobe handler.
+
+ARM v8 supports enabling single stepping before the break exception
+return (ERET), with next PC in exception return address (ELR_EL1). The
+kprobe handler prepares an executable memory slot for out-of-line
+execution with a copy of the original instruction being probed, and
+enables single stepping. The PC is set to the out-of-line slot address
+before the ERET. With this scheme, the instruction is executed with the
+exact same register context except for the PC (and DAIF) registers.
+
+Debug mask (PSTATE.D) is enabled only when single stepping a recursive
+kprobe, e.g.: during kprobes reenter so that probed instruction can be
+single stepped within the kprobe handler -exception- context.
+The recursion depth of kprobe is always 2, i.e. upon probe re-entry,
+any further re-entry is prevented by not calling handlers and the case
+counted as a missed kprobe).
+
+Single stepping from the x-o-l slot has a drawback for PC-relative accesses
+like branching and symbolic literals access as the offset from the new PC
+(slot address) may not be ensured to fit in the immediate value of
+the opcode. Such instructions need simulation, so reject
+probing them.
+
+Instructions generating exceptions or cpu mode change are rejected
+for probing.
+
+Exclusive load/store instructions are rejected too.  Additionally, the
+code is checked to see if it is inside an exclusive load/store sequence
+(code from Pratyush).
+
+System instructions are mostly enabled for stepping, except MSR/MRS
+accesses to "DAIF" flags in PSTATE, which are not safe for
+probing.
+
+[<dave.long@linaro.org>: changed to remove irq_stack references]
+
+This also changes arch/arm64/include/asm/ptrace.h to use
+include/asm-generic/ptrace.h.
+
+Thanks to Steve Capper and Pratyush Anand for several suggested
+Changes.
+
+Signed-off-by: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Signed-off-by: Pratyush Anand <panand@redhat.com>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/Kconfig                      |   1 +
+ arch/arm64/include/asm/debug-monitors.h |   5 +
+ arch/arm64/include/asm/insn.h           |   2 +
+ arch/arm64/include/asm/kprobes.h        |  60 ++++
+ arch/arm64/include/asm/probes.h         |  34 +++
+ arch/arm64/include/asm/ptrace.h         |  14 +-
+ arch/arm64/kernel/Makefile              |   2 +-
+ arch/arm64/kernel/debug-monitors.c      |  16 +-
+ arch/arm64/kernel/probes/Makefile       |   1 +
+ arch/arm64/kernel/probes/decode-insn.c  | 143 +++++++++
+ arch/arm64/kernel/probes/decode-insn.h  |  34 +++
+ arch/arm64/kernel/probes/kprobes.c      | 522 ++++++++++++++++++++++++++++++++
+ arch/arm64/kernel/vmlinux.lds.S         |   1 +
+ arch/arm64/mm/fault.c                   |  26 ++
+ 14 files changed, 856 insertions(+), 5 deletions(-)
+ create mode 100644 arch/arm64/include/asm/kprobes.h
+ create mode 100644 arch/arm64/include/asm/probes.h
+ create mode 100644 arch/arm64/kernel/probes/Makefile
+ create mode 100644 arch/arm64/kernel/probes/decode-insn.c
+ create mode 100644 arch/arm64/kernel/probes/decode-insn.h
+ create mode 100644 arch/arm64/kernel/probes/kprobes.c
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index 0290331..e6370c4 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -78,6 +78,7 @@ config ARM64
+ 	select HAVE_REGS_AND_STACK_ACCESS_API
+ 	select HAVE_RCU_TABLE_FREE
+ 	select HAVE_SYSCALL_TRACEPOINTS
++	select HAVE_KPROBES
+ 	select IOMMU_DMA if IOMMU_SUPPORT
+ 	select IRQ_DOMAIN
+ 	select IRQ_FORCED_THREADING
+diff --git a/arch/arm64/include/asm/debug-monitors.h b/arch/arm64/include/asm/debug-monitors.h
+index 279c85b5..274ab60 100644
+--- a/arch/arm64/include/asm/debug-monitors.h
++++ b/arch/arm64/include/asm/debug-monitors.h
+@@ -78,6 +78,11 @@
+ 
+ #define CACHE_FLUSH_IS_SAFE		1
+ 
++/* kprobes BRK opcodes with ESR encoding  */
++#define BRK64_ESR_MASK		0xFFFF
++#define BRK64_ESR_KPROBES	0x0004
++#define BRK64_OPCODE_KPROBES	(AARCH64_BREAK_MON | (BRK64_ESR_KPROBES << 5))
++
+ /* AArch32 */
+ #define DBG_ESR_EVT_BKPT	0x4
+ #define DBG_ESR_EVT_VECC	0x5
+diff --git a/arch/arm64/include/asm/insn.h b/arch/arm64/include/asm/insn.h
+index a44abbd..1dbaa90 100644
+--- a/arch/arm64/include/asm/insn.h
++++ b/arch/arm64/include/asm/insn.h
+@@ -253,6 +253,8 @@ __AARCH64_INSN_FUNCS(ldr_reg,	0x3FE0EC00, 0x38606800)
+ __AARCH64_INSN_FUNCS(ldr_lit,	0xBF000000, 0x18000000)
+ __AARCH64_INSN_FUNCS(ldrsw_lit,	0xFF000000, 0x98000000)
+ __AARCH64_INSN_FUNCS(exclusive,	0x3F800000, 0x08000000)
++__AARCH64_INSN_FUNCS(load_ex,	0x3F400000, 0x08400000)
++__AARCH64_INSN_FUNCS(store_ex,	0x3F400000, 0x08000000)
+ __AARCH64_INSN_FUNCS(stp_post,	0x7FC00000, 0x28800000)
+ __AARCH64_INSN_FUNCS(ldp_post,	0x7FC00000, 0x28C00000)
+ __AARCH64_INSN_FUNCS(stp_pre,	0x7FC00000, 0x29800000)
+diff --git a/arch/arm64/include/asm/kprobes.h b/arch/arm64/include/asm/kprobes.h
+new file mode 100644
+index 0000000..79c9511
+--- /dev/null
++++ b/arch/arm64/include/asm/kprobes.h
+@@ -0,0 +1,60 @@
++/*
++ * arch/arm64/include/asm/kprobes.h
++ *
++ * Copyright (C) 2013 Linaro Limited
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ */
++
++#ifndef _ARM_KPROBES_H
++#define _ARM_KPROBES_H
++
++#include <linux/types.h>
++#include <linux/ptrace.h>
++#include <linux/percpu.h>
++
++#define __ARCH_WANT_KPROBES_INSN_SLOT
++#define MAX_INSN_SIZE			1
++#define MAX_STACK_SIZE			128
++
++#define flush_insn_slot(p)		do { } while (0)
++#define kretprobe_blacklist_size	0
++
++#include <asm/probes.h>
++
++struct prev_kprobe {
++	struct kprobe *kp;
++	unsigned int status;
++};
++
++/* Single step context for kprobe */
++struct kprobe_step_ctx {
++	unsigned long ss_pending;
++	unsigned long match_addr;
++};
++
++/* per-cpu kprobe control block */
++struct kprobe_ctlblk {
++	unsigned int kprobe_status;
++	unsigned long saved_irqflag;
++	struct prev_kprobe prev_kprobe;
++	struct kprobe_step_ctx ss_ctx;
++	struct pt_regs jprobe_saved_regs;
++	char jprobes_stack[MAX_STACK_SIZE];
++};
++
++void arch_remove_kprobe(struct kprobe *);
++int kprobe_fault_handler(struct pt_regs *regs, unsigned int fsr);
++int kprobe_exceptions_notify(struct notifier_block *self,
++			     unsigned long val, void *data);
++int kprobe_breakpoint_handler(struct pt_regs *regs, unsigned int esr);
++int kprobe_single_step_handler(struct pt_regs *regs, unsigned int esr);
++
++#endif /* _ARM_KPROBES_H */
+diff --git a/arch/arm64/include/asm/probes.h b/arch/arm64/include/asm/probes.h
+new file mode 100644
+index 0000000..1e8a21a
+--- /dev/null
++++ b/arch/arm64/include/asm/probes.h
+@@ -0,0 +1,34 @@
++/*
++ * arch/arm64/include/asm/probes.h
++ *
++ * Copyright (C) 2013 Linaro Limited
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ */
++#ifndef _ARM_PROBES_H
++#define _ARM_PROBES_H
++
++struct kprobe;
++struct arch_specific_insn;
++
++typedef u32 kprobe_opcode_t;
++typedef unsigned long (kprobes_pstate_check_t)(unsigned long);
++typedef void (kprobes_handler_t) (u32 opcode, long addr, struct pt_regs *);
++
++/* architecture specific copy of original instruction */
++struct arch_specific_insn {
++	kprobe_opcode_t *insn;
++	kprobes_pstate_check_t *pstate_cc;
++	kprobes_handler_t *handler;
++	/* restore address after step xol */
++	unsigned long restore;
++};
++
++#endif
+diff --git a/arch/arm64/include/asm/ptrace.h b/arch/arm64/include/asm/ptrace.h
+index cfe2b37..1528d52e 100644
+--- a/arch/arm64/include/asm/ptrace.h
++++ b/arch/arm64/include/asm/ptrace.h
+@@ -148,9 +148,12 @@ struct pt_regs {
+ #define fast_interrupts_enabled(regs) \
+ 	(!((regs)->pstate & PSR_F_BIT))
+ 
+-#define user_stack_pointer(regs) \
++#define GET_USP(regs) \
+ 	(!compat_user_mode(regs) ? (regs)->sp : (regs)->compat_sp)
+ 
++#define SET_USP(ptregs, value) \
++	(!compat_user_mode(regs) ? ((regs)->sp = value) : ((regs)->compat_sp = value))
++
+ extern int regs_query_register_offset(const char *name);
+ extern const char *regs_query_register_name(unsigned int offset);
+ extern unsigned long regs_get_kernel_stack_nth(struct pt_regs *regs,
+@@ -205,8 +208,15 @@ static inline unsigned long regs_return_value(struct pt_regs *regs)
+ struct task_struct;
+ int valid_user_regs(struct user_pt_regs *regs, struct task_struct *task);
+ 
+-#define instruction_pointer(regs)	((unsigned long)(regs)->pc)
++#define GET_IP(regs)		((unsigned long)(regs)->pc)
++#define SET_IP(regs, value)	((regs)->pc = ((u64) (value)))
++
++#define GET_FP(ptregs)		((unsigned long)(ptregs)->regs[29])
++#define SET_FP(ptregs, value)	((ptregs)->regs[29] = ((u64) (value)))
++
++#include <asm-generic/ptrace.h>
+ 
++#undef profile_pc
+ extern unsigned long profile_pc(struct pt_regs *regs);
+ 
+ #endif /* __ASSEMBLY__ */
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index fed7fc5..cfa290b 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -41,7 +41,7 @@ arm64-obj-$(CONFIG_PCI)			+= pci.o
+ arm64-obj-$(CONFIG_ARMV8_DEPRECATED)	+= armv8_deprecated.o
+ arm64-obj-$(CONFIG_ACPI)		+= acpi.o
+ 
+-obj-y					+= $(arm64-obj-y) vdso/
++obj-y					+= $(arm64-obj-y) vdso/ probes/
+ obj-m					+= $(arm64-obj-m)
+ head-y					:= head.o
+ extra-y					+= $(head-y) vmlinux.lds
+diff --git a/arch/arm64/kernel/debug-monitors.c b/arch/arm64/kernel/debug-monitors.c
+index c8875b6..17a4d37 100644
+--- a/arch/arm64/kernel/debug-monitors.c
++++ b/arch/arm64/kernel/debug-monitors.c
+@@ -23,6 +23,7 @@
+ #include <linux/hardirq.h>
+ #include <linux/init.h>
+ #include <linux/ptrace.h>
++#include <linux/kprobes.h>
+ #include <linux/stat.h>
+ #include <linux/uaccess.h>
+ 
+@@ -253,6 +254,10 @@ static int single_step_handler(unsigned long addr, unsigned int esr,
+ 		 */
+ 		user_rewind_single_step(current);
+ 	} else {
++#ifdef	CONFIG_KPROBES
++		if (kprobe_single_step_handler(regs, esr) == DBG_HOOK_HANDLED)
++			return 0;
++#endif
+ 		if (call_step_hook(regs, esr) == DBG_HOOK_HANDLED)
+ 			return 0;
+ 
+@@ -318,8 +323,15 @@ static int brk_handler(unsigned long addr, unsigned int esr,
+ 		};
+ 
+ 		force_sig_info(SIGTRAP, &info, current);
+-	} else if (call_break_hook(regs, esr) != DBG_HOOK_HANDLED) {
+-		pr_warning("Unexpected kernel BRK exception at EL1\n");
++	}
++#ifdef	CONFIG_KPROBES
++	else if ((esr & BRK64_ESR_MASK) == BRK64_ESR_KPROBES) {
++		if (kprobe_breakpoint_handler(regs, esr) != DBG_HOOK_HANDLED)
++			return -EFAULT;
++	}
++#endif
++	else if (call_break_hook(regs, esr) != DBG_HOOK_HANDLED) {
++		pr_warn("Unexpected kernel BRK exception at EL1\n");
+ 		return -EFAULT;
+ 	}
+ 
+diff --git a/arch/arm64/kernel/probes/Makefile b/arch/arm64/kernel/probes/Makefile
+new file mode 100644
+index 0000000..bc159bf
+--- /dev/null
++++ b/arch/arm64/kernel/probes/Makefile
+@@ -0,0 +1 @@
++obj-$(CONFIG_KPROBES)		+= kprobes.o decode-insn.o
+diff --git a/arch/arm64/kernel/probes/decode-insn.c b/arch/arm64/kernel/probes/decode-insn.c
+new file mode 100644
+index 0000000..0518df1
+--- /dev/null
++++ b/arch/arm64/kernel/probes/decode-insn.c
+@@ -0,0 +1,143 @@
++/*
++ * arch/arm64/kernel/probes/decode-insn.c
++ *
++ * Copyright (C) 2013 Linaro Limited.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ */
++
++#include <linux/kernel.h>
++#include <linux/kprobes.h>
++#include <linux/module.h>
++#include <asm/kprobes.h>
++#include <asm/insn.h>
++#include <asm/sections.h>
++
++#include "decode-insn.h"
++
++static bool __kprobes aarch64_insn_is_steppable(u32 insn)
++{
++	/*
++	 * Branch instructions will write a new value into the PC which is
++	 * likely to be relative to the XOL address and therefore invalid.
++	 * Deliberate generation of an exception during stepping is also not
++	 * currently safe. Lastly, MSR instructions can do any number of nasty
++	 * things we can't handle during single-stepping.
++	 */
++	if (aarch64_get_insn_class(insn) == AARCH64_INSN_CLS_BR_SYS) {
++		if (aarch64_insn_is_branch(insn) ||
++		    aarch64_insn_is_msr_imm(insn) ||
++		    aarch64_insn_is_msr_reg(insn) ||
++		    aarch64_insn_is_exception(insn) ||
++		    aarch64_insn_is_eret(insn))
++			return false;
++
++		/*
++		 * The MRS instruction may not return a correct value when
++		 * executing in the single-stepping environment. We do make one
++		 * exception, for reading the DAIF bits.
++		 */
++		if (aarch64_insn_is_mrs(insn))
++			return aarch64_insn_extract_system_reg(insn)
++			     != AARCH64_INSN_SPCLREG_DAIF;
++
++		/*
++		 * The HINT instruction is is problematic when single-stepping,
++		 * except for the NOP case.
++		 */
++		if (aarch64_insn_is_hint(insn))
++			return aarch64_insn_is_nop(insn);
++
++		return true;
++	}
++
++	/*
++	 * Instructions which load PC relative literals are not going to work
++	 * when executed from an XOL slot. Instructions doing an exclusive
++	 * load/store are not going to complete successfully when single-step
++	 * exception handling happens in the middle of the sequence.
++	 */
++	if (aarch64_insn_uses_literal(insn) ||
++	    aarch64_insn_is_exclusive(insn))
++		return false;
++
++	return true;
++}
++
++/* Return:
++ *   INSN_REJECTED     If instruction is one not allowed to kprobe,
++ *   INSN_GOOD         If instruction is supported and uses instruction slot,
++ */
++static enum kprobe_insn __kprobes
++arm_probe_decode_insn(kprobe_opcode_t insn, struct arch_specific_insn *asi)
++{
++	/*
++	 * Instructions reading or modifying the PC won't work from the XOL
++	 * slot.
++	 */
++	if (aarch64_insn_is_steppable(insn))
++		return INSN_GOOD;
++	else
++		return INSN_REJECTED;
++}
++
++static bool __kprobes
++is_probed_address_atomic(kprobe_opcode_t *scan_start, kprobe_opcode_t *scan_end)
++{
++	while (scan_start > scan_end) {
++		/*
++		 * atomic region starts from exclusive load and ends with
++		 * exclusive store.
++		 */
++		if (aarch64_insn_is_store_ex(le32_to_cpu(*scan_start)))
++			return false;
++		else if (aarch64_insn_is_load_ex(le32_to_cpu(*scan_start)))
++			return true;
++		scan_start--;
++	}
++
++	return false;
++}
++
++enum kprobe_insn __kprobes
++arm_kprobe_decode_insn(kprobe_opcode_t *addr, struct arch_specific_insn *asi)
++{
++	enum kprobe_insn decoded;
++	kprobe_opcode_t insn = le32_to_cpu(*addr);
++	kprobe_opcode_t *scan_start = addr - 1;
++	kprobe_opcode_t *scan_end = addr - MAX_ATOMIC_CONTEXT_SIZE;
++#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)
++	struct module *mod;
++#endif
++
++	if (addr >= (kprobe_opcode_t *)_text &&
++	    scan_end < (kprobe_opcode_t *)_text)
++		scan_end = (kprobe_opcode_t *)_text;
++#if defined(CONFIG_MODULES) && defined(MODULES_VADDR)
++	else {
++		preempt_disable();
++		mod = __module_address((unsigned long)addr);
++		if (mod && within_module_init((unsigned long)addr, mod) &&
++			!within_module_init((unsigned long)scan_end, mod))
++			scan_end = (kprobe_opcode_t *)mod->module_init;
++		else if (mod && within_module_core((unsigned long)addr, mod) &&
++			!within_module_core((unsigned long)scan_end, mod))
++			scan_end = (kprobe_opcode_t *)mod->module_core;
++		preempt_enable();
++	}
++#endif
++	decoded = arm_probe_decode_insn(insn, asi);
++
++	if (decoded == INSN_REJECTED ||
++			is_probed_address_atomic(scan_start, scan_end))
++		return INSN_REJECTED;
++
++	return decoded;
++}
+diff --git a/arch/arm64/kernel/probes/decode-insn.h b/arch/arm64/kernel/probes/decode-insn.h
+new file mode 100644
+index 0000000..ad5ba9c
+--- /dev/null
++++ b/arch/arm64/kernel/probes/decode-insn.h
+@@ -0,0 +1,34 @@
++/*
++ * arch/arm64/kernel/probes/decode-insn.h
++ *
++ * Copyright (C) 2013 Linaro Limited.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ */
++
++#ifndef _ARM_KERNEL_KPROBES_ARM64_H
++#define _ARM_KERNEL_KPROBES_ARM64_H
++
++/*
++ * ARM strongly recommends a limit of 128 bytes between LoadExcl and
++ * StoreExcl instructions in a single thread of execution. So keep the
++ * max atomic context size as 32.
++ */
++#define MAX_ATOMIC_CONTEXT_SIZE	(128 / sizeof(kprobe_opcode_t))
++
++enum kprobe_insn {
++	INSN_REJECTED,
++	INSN_GOOD,
++};
++
++enum kprobe_insn __kprobes
++arm_kprobe_decode_insn(kprobe_opcode_t *addr, struct arch_specific_insn *asi);
++
++#endif /* _ARM_KERNEL_KPROBES_ARM64_H */
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+new file mode 100644
+index 0000000..55e3e01
+--- /dev/null
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -0,0 +1,522 @@
++/*
++ * arch/arm64/kernel/probes/kprobes.c
++ *
++ * Kprobes support for ARM64
++ *
++ * Copyright (C) 2013 Linaro Limited.
++ * Author: Sandeepa Prabhu <sandeepa.prabhu@linaro.org>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ *
++ */
++#include <linux/kernel.h>
++#include <linux/kprobes.h>
++#include <linux/module.h>
++#include <linux/slab.h>
++#include <linux/stop_machine.h>
++#include <linux/stringify.h>
++#include <asm/traps.h>
++#include <asm/ptrace.h>
++#include <asm/cacheflush.h>
++#include <asm/debug-monitors.h>
++#include <asm/system_misc.h>
++#include <asm/insn.h>
++#include <asm/uaccess.h>
++#include <asm/irq.h>
++
++#include "decode-insn.h"
++
++#define MIN_STACK_SIZE(addr)	min((unsigned long)MAX_STACK_SIZE,	\
++	(unsigned long)current_thread_info() + THREAD_START_SP - (addr))
++
++void jprobe_return_break(void);
++
++DEFINE_PER_CPU(struct kprobe *, current_kprobe) = NULL;
++DEFINE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
++
++static void __kprobes arch_prepare_ss_slot(struct kprobe *p)
++{
++	/* prepare insn slot */
++	p->ainsn.insn[0] = cpu_to_le32(p->opcode);
++
++	flush_icache_range((uintptr_t) (p->ainsn.insn),
++			   (uintptr_t) (p->ainsn.insn) +
++			   MAX_INSN_SIZE * sizeof(kprobe_opcode_t));
++
++	/*
++	 * Needs restoring of return address after stepping xol.
++	 */
++	p->ainsn.restore = (unsigned long) p->addr +
++	  sizeof(kprobe_opcode_t);
++}
++
++int __kprobes arch_prepare_kprobe(struct kprobe *p)
++{
++	unsigned long probe_addr = (unsigned long)p->addr;
++	extern char __start_rodata[];
++	extern char __end_rodata[];
++
++	if (probe_addr & 0x3)
++		return -EINVAL;
++
++	/* copy instruction */
++	p->opcode = le32_to_cpu(*p->addr);
++
++	if (in_exception_text(probe_addr))
++		return -EINVAL;
++	if (probe_addr >= (unsigned long) __start_rodata &&
++	    probe_addr <= (unsigned long) __end_rodata)
++		return -EINVAL;
++
++	/* decode instruction */
++	switch (arm_kprobe_decode_insn(p->addr, &p->ainsn)) {
++	case INSN_REJECTED:	/* insn not supported */
++		return -EINVAL;
++
++	case INSN_GOOD:	/* instruction uses slot */
++		p->ainsn.insn = get_insn_slot();
++		if (!p->ainsn.insn)
++			return -ENOMEM;
++		break;
++	};
++
++	/* prepare the instruction */
++	arch_prepare_ss_slot(p);
++
++	return 0;
++}
++
++static int __kprobes patch_text(kprobe_opcode_t *addr, u32 opcode)
++{
++	void *addrs[1];
++	u32 insns[1];
++
++	addrs[0] = (void *)addr;
++	insns[0] = (u32)opcode;
++
++	return aarch64_insn_patch_text(addrs, insns, 1);
++}
++
++/* arm kprobe: install breakpoint in text */
++void __kprobes arch_arm_kprobe(struct kprobe *p)
++{
++	patch_text(p->addr, BRK64_OPCODE_KPROBES);
++}
++
++/* disarm kprobe: remove breakpoint from text */
++void __kprobes arch_disarm_kprobe(struct kprobe *p)
++{
++	patch_text(p->addr, p->opcode);
++}
++
++void __kprobes arch_remove_kprobe(struct kprobe *p)
++{
++	if (p->ainsn.insn) {
++		free_insn_slot(p->ainsn.insn, 0);
++		p->ainsn.insn = NULL;
++	}
++}
++
++static void __kprobes save_previous_kprobe(struct kprobe_ctlblk *kcb)
++{
++	kcb->prev_kprobe.kp = kprobe_running();
++	kcb->prev_kprobe.status = kcb->kprobe_status;
++}
++
++static void __kprobes restore_previous_kprobe(struct kprobe_ctlblk *kcb)
++{
++	__this_cpu_write(current_kprobe, kcb->prev_kprobe.kp);
++	kcb->kprobe_status = kcb->prev_kprobe.status;
++}
++
++static void __kprobes set_current_kprobe(struct kprobe *p)
++{
++	__this_cpu_write(current_kprobe, p);
++}
++
++/*
++ * The D-flag (Debug mask) is set (masked) upon debug exception entry.
++ * Kprobes needs to clear (unmask) D-flag -ONLY- in case of recursive
++ * probe i.e. when probe hit from kprobe handler context upon
++ * executing the pre/post handlers. In this case we return with
++ * D-flag clear so that single-stepping can be carried-out.
++ *
++ * Leave D-flag set in all other cases.
++ */
++static void __kprobes
++spsr_set_debug_flag(struct pt_regs *regs, int mask)
++{
++	unsigned long spsr = regs->pstate;
++
++	if (mask)
++		spsr |= PSR_D_BIT;
++	else
++		spsr &= ~PSR_D_BIT;
++
++	regs->pstate = spsr;
++}
++
++/*
++ * Interrupts need to be disabled before single-step mode is set, and not
++ * reenabled until after single-step mode ends.
++ * Without disabling interrupt on local CPU, there is a chance of
++ * interrupt occurrence in the period of exception return and  start of
++ * out-of-line single-step, that result in wrongly single stepping
++ * into the interrupt handler.
++ */
++static void __kprobes kprobes_save_local_irqflag(struct kprobe_ctlblk *kcb,
++						struct pt_regs *regs)
++{
++	kcb->saved_irqflag = regs->pstate;
++	regs->pstate |= PSR_I_BIT;
++}
++
++static void __kprobes kprobes_restore_local_irqflag(struct kprobe_ctlblk *kcb,
++						struct pt_regs *regs)
++{
++	if (kcb->saved_irqflag & PSR_I_BIT)
++		regs->pstate |= PSR_I_BIT;
++	else
++		regs->pstate &= ~PSR_I_BIT;
++}
++
++static void __kprobes
++set_ss_context(struct kprobe_ctlblk *kcb, unsigned long addr)
++{
++	kcb->ss_ctx.ss_pending = true;
++	kcb->ss_ctx.match_addr = addr + sizeof(kprobe_opcode_t);
++}
++
++static void __kprobes clear_ss_context(struct kprobe_ctlblk *kcb)
++{
++	kcb->ss_ctx.ss_pending = false;
++	kcb->ss_ctx.match_addr = 0;
++}
++
++static void __kprobes setup_singlestep(struct kprobe *p,
++				       struct pt_regs *regs,
++				       struct kprobe_ctlblk *kcb, int reenter)
++{
++	unsigned long slot;
++
++	if (reenter) {
++		save_previous_kprobe(kcb);
++		set_current_kprobe(p);
++		kcb->kprobe_status = KPROBE_REENTER;
++	} else {
++		kcb->kprobe_status = KPROBE_HIT_SS;
++	}
++
++	BUG_ON(!p->ainsn.insn);
++
++	/* prepare for single stepping */
++	slot = (unsigned long)p->ainsn.insn;
++
++	set_ss_context(kcb, slot);	/* mark pending ss */
++
++	if (kcb->kprobe_status == KPROBE_REENTER)
++		spsr_set_debug_flag(regs, 0);
++
++	/* IRQs and single stepping do not mix well. */
++	kprobes_save_local_irqflag(kcb, regs);
++	kernel_enable_single_step(regs);
++	instruction_pointer_set(regs, slot);
++}
++
++static int __kprobes reenter_kprobe(struct kprobe *p,
++				    struct pt_regs *regs,
++				    struct kprobe_ctlblk *kcb)
++{
++	switch (kcb->kprobe_status) {
++	case KPROBE_HIT_SSDONE:
++	case KPROBE_HIT_ACTIVE:
++		kprobes_inc_nmissed_count(p);
++		setup_singlestep(p, regs, kcb, 1);
++		break;
++	case KPROBE_HIT_SS:
++	case KPROBE_REENTER:
++		pr_warn("Unrecoverable kprobe detected at %p.\n", p->addr);
++		dump_kprobe(p);
++		BUG();
++		break;
++	default:
++		WARN_ON(1);
++		return 0;
++	}
++
++	return 1;
++}
++
++static void __kprobes
++post_kprobe_handler(struct kprobe_ctlblk *kcb, struct pt_regs *regs)
++{
++	struct kprobe *cur = kprobe_running();
++
++	if (!cur)
++		return;
++
++	/* return addr restore if non-branching insn */
++	if (cur->ainsn.restore != 0)
++		instruction_pointer_set(regs, cur->ainsn.restore);
++
++	/* restore back original saved kprobe variables and continue */
++	if (kcb->kprobe_status == KPROBE_REENTER) {
++		restore_previous_kprobe(kcb);
++		return;
++	}
++	/* call post handler */
++	kcb->kprobe_status = KPROBE_HIT_SSDONE;
++	if (cur->post_handler)	{
++		/* post_handler can hit breakpoint and single step
++		 * again, so we enable D-flag for recursive exception.
++		 */
++		cur->post_handler(cur, regs, 0);
++	}
++
++	reset_current_kprobe();
++}
++
++int __kprobes kprobe_fault_handler(struct pt_regs *regs, unsigned int fsr)
++{
++	struct kprobe *cur = kprobe_running();
++	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
++
++	switch (kcb->kprobe_status) {
++	case KPROBE_HIT_SS:
++	case KPROBE_REENTER:
++		/*
++		 * We are here because the instruction being single
++		 * stepped caused a page fault. We reset the current
++		 * kprobe and the ip points back to the probe address
++		 * and allow the page fault handler to continue as a
++		 * normal page fault.
++		 */
++		instruction_pointer_set(regs, (unsigned long) cur->addr);
++		if (!instruction_pointer(regs))
++			BUG();
++
++		kernel_disable_single_step();
++		if (kcb->kprobe_status == KPROBE_REENTER)
++			spsr_set_debug_flag(regs, 1);
++
++		if (kcb->kprobe_status == KPROBE_REENTER)
++			restore_previous_kprobe(kcb);
++		else
++			reset_current_kprobe();
++
++		break;
++	case KPROBE_HIT_ACTIVE:
++	case KPROBE_HIT_SSDONE:
++		/*
++		 * We increment the nmissed count for accounting,
++		 * we can also use npre/npostfault count for accounting
++		 * these specific fault cases.
++		 */
++		kprobes_inc_nmissed_count(cur);
++
++		/*
++		 * We come here because instructions in the pre/post
++		 * handler caused the page_fault, this could happen
++		 * if handler tries to access user space by
++		 * copy_from_user(), get_user() etc. Let the
++		 * user-specified handler try to fix it first.
++		 */
++		if (cur->fault_handler && cur->fault_handler(cur, regs, fsr))
++			return 1;
++
++		/*
++		 * In case the user-specified fault handler returned
++		 * zero, try to fix up.
++		 */
++		if (fixup_exception(regs))
++			return 1;
++	}
++	return 0;
++}
++
++int __kprobes kprobe_exceptions_notify(struct notifier_block *self,
++				       unsigned long val, void *data)
++{
++	return NOTIFY_DONE;
++}
++
++static void __kprobes kprobe_handler(struct pt_regs *regs)
++{
++	struct kprobe *p, *cur_kprobe;
++	struct kprobe_ctlblk *kcb;
++	unsigned long addr = instruction_pointer(regs);
++
++	kcb = get_kprobe_ctlblk();
++	cur_kprobe = kprobe_running();
++
++	p = get_kprobe((kprobe_opcode_t *) addr);
++
++	if (p) {
++		if (cur_kprobe) {
++			if (reenter_kprobe(p, regs, kcb))
++				return;
++		} else {
++			/* Probe hit */
++			set_current_kprobe(p);
++			kcb->kprobe_status = KPROBE_HIT_ACTIVE;
++
++			/*
++			 * If we have no pre-handler or it returned 0, we
++			 * continue with normal processing.  If we have a
++			 * pre-handler and it returned non-zero, it prepped
++			 * for calling the break_handler below on re-entry,
++			 * so get out doing nothing more here.
++			 *
++			 * pre_handler can hit a breakpoint and can step thru
++			 * before return, keep PSTATE D-flag enabled until
++			 * pre_handler return back.
++			 */
++			if (!p->pre_handler || !p->pre_handler(p, regs)) {
++				setup_singlestep(p, regs, kcb, 0);
++				return;
++			}
++		}
++	} else if ((le32_to_cpu(*(kprobe_opcode_t *) addr) ==
++	    BRK64_OPCODE_KPROBES) && cur_kprobe) {
++		/* We probably hit a jprobe.  Call its break handler. */
++		if (cur_kprobe->break_handler  &&
++		     cur_kprobe->break_handler(cur_kprobe, regs)) {
++			setup_singlestep(cur_kprobe, regs, kcb, 0);
++			return;
++		}
++	}
++	/*
++	 * The breakpoint instruction was removed right
++	 * after we hit it.  Another cpu has removed
++	 * either a probepoint or a debugger breakpoint
++	 * at this address.  In either case, no further
++	 * handling of this interrupt is appropriate.
++	 * Return back to original instruction, and continue.
++	 */
++}
++
++static int __kprobes
++kprobe_ss_hit(struct kprobe_ctlblk *kcb, unsigned long addr)
++{
++	if ((kcb->ss_ctx.ss_pending)
++	    && (kcb->ss_ctx.match_addr == addr)) {
++		clear_ss_context(kcb);	/* clear pending ss */
++		return DBG_HOOK_HANDLED;
++	}
++	/* not ours, kprobes should ignore it */
++	return DBG_HOOK_ERROR;
++}
++
++int __kprobes
++kprobe_single_step_handler(struct pt_regs *regs, unsigned int esr)
++{
++	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
++	int retval;
++
++	/* return error if this is not our step */
++	retval = kprobe_ss_hit(kcb, instruction_pointer(regs));
++
++	if (retval == DBG_HOOK_HANDLED) {
++		kprobes_restore_local_irqflag(kcb, regs);
++		kernel_disable_single_step();
++
++		if (kcb->kprobe_status == KPROBE_REENTER)
++			spsr_set_debug_flag(regs, 1);
++
++		post_kprobe_handler(kcb, regs);
++	}
++
++	return retval;
++}
++
++int __kprobes
++kprobe_breakpoint_handler(struct pt_regs *regs, unsigned int esr)
++{
++	kprobe_handler(regs);
++	return DBG_HOOK_HANDLED;
++}
++
++int __kprobes setjmp_pre_handler(struct kprobe *p, struct pt_regs *regs)
++{
++	struct jprobe *jp = container_of(p, struct jprobe, kp);
++	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
++	long stack_ptr = kernel_stack_pointer(regs);
++
++	kcb->jprobe_saved_regs = *regs;
++	/*
++	 * As Linus pointed out, gcc assumes that the callee
++	 * owns the argument space and could overwrite it, e.g.
++	 * tailcall optimization. So, to be absolutely safe
++	 * we also save and restore enough stack bytes to cover
++	 * the argument area.
++	 */
++	memcpy(kcb->jprobes_stack, (void *)stack_ptr,
++	       MIN_STACK_SIZE(stack_ptr));
++
++	instruction_pointer_set(regs, (unsigned long) jp->entry);
++	preempt_disable();
++	pause_graph_tracing();
++	return 1;
++}
++
++void __kprobes jprobe_return(void)
++{
++	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
++
++	/*
++	 * Jprobe handler return by entering break exception,
++	 * encoded same as kprobe, but with following conditions
++	 * -a magic number in x0 to identify from rest of other kprobes.
++	 * -restore stack addr to original saved pt_regs
++	 */
++	asm volatile ("ldr x0, [%0]\n\t"
++		      "mov sp, x0\n\t"
++		      ".globl jprobe_return_break\n\t"
++		      "jprobe_return_break:\n\t"
++		      "brk %1\n\t"
++		      :
++		      : "r"(&kcb->jprobe_saved_regs.sp),
++		      "I"(BRK64_ESR_KPROBES)
++		      : "memory");
++}
++
++int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
++{
++	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
++	long stack_addr = kcb->jprobe_saved_regs.sp;
++	long orig_sp = kernel_stack_pointer(regs);
++	struct jprobe *jp = container_of(p, struct jprobe, kp);
++
++	if (instruction_pointer(regs) != (u64) jprobe_return_break)
++		return 0;
++
++	if (orig_sp != stack_addr) {
++		struct pt_regs *saved_regs =
++		    (struct pt_regs *)kcb->jprobe_saved_regs.sp;
++		pr_err("current sp %lx does not match saved sp %lx\n",
++		       orig_sp, stack_addr);
++		pr_err("Saved registers for jprobe %p\n", jp);
++		show_regs(saved_regs);
++		pr_err("Current registers\n");
++		show_regs(regs);
++		BUG();
++	}
++	unpause_graph_tracing();
++	*regs = kcb->jprobe_saved_regs;
++	memcpy((void *)stack_addr, kcb->jprobes_stack,
++	       MIN_STACK_SIZE(stack_addr));
++	preempt_enable_no_resched();
++	return 1;
++}
++
++int __init arch_init_kprobes(void)
++{
++	return 0;
++}
+diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
+index 71426a78..bc6b631 100644
+--- a/arch/arm64/kernel/vmlinux.lds.S
++++ b/arch/arm64/kernel/vmlinux.lds.S
+@@ -105,6 +105,7 @@ SECTIONS
+ 			TEXT_TEXT
+ 			SCHED_TEXT
+ 			LOCK_TEXT
++			KPROBES_TEXT
+ 			HYPERVISOR_TEXT
+ 			IDMAP_TEXT
+ 			*(.fixup)
+diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
+index 247bae7..e19cd15 100644
+--- a/arch/arm64/mm/fault.c
++++ b/arch/arm64/mm/fault.c
+@@ -43,6 +43,28 @@
+ 
+ static const char *fault_name(unsigned int esr);
+ 
++#ifdef CONFIG_KPROBES
++static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
++{
++	int ret = 0;
++
++	/* kprobe_running() needs smp_processor_id() */
++	if (!user_mode(regs)) {
++		preempt_disable();
++		if (kprobe_running() && kprobe_fault_handler(regs, esr))
++			ret = 1;
++		preempt_enable();
++	}
++
++	return ret;
++}
++#else
++static inline int notify_page_fault(struct pt_regs *regs, unsigned int esr)
++{
++	return 0;
++}
++#endif
++
+ /*
+  * Dump out the page tables associated with 'addr' in mm 'mm'.
+  */
+@@ -253,6 +275,9 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,
+ 	unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
+ 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
+ 
++	if (notify_page_fault(regs, esr))
++		return 0;
++
+ 	tsk = current;
+ 	mm  = tsk->mm;
+ 
+@@ -606,6 +631,7 @@ asmlinkage int __exception do_debug_exception(unsigned long addr,
+ 
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(do_debug_exception);
+ 
+ #ifdef CONFIG_ARM64_PAN
+ int cpu_enable_pan(void *__unused)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0005-arm64-Blacklist-non-kprobe-able-symbol.patch b/tools/kdump/0005-arm64-Blacklist-non-kprobe-able-symbol.patch
new file mode 100644
index 0000000..1f89854
--- /dev/null
+++ b/tools/kdump/0005-arm64-Blacklist-non-kprobe-able-symbol.patch
@@ -0,0 +1,279 @@
+From d3bf1306c7f53b22f289f4e2fef44311ed70e40b Mon Sep 17 00:00:00 2001
+From: Pratyush Anand <panand@redhat.com>
+Date: Fri, 8 Jul 2016 12:35:49 -0400
+Subject: [PATCH 005/123] arm64: Blacklist non-kprobe-able symbol
+
+commit 44b53f67c99d0fc53af3066a05d9e7ca5080a850 upstream.
+
+Add all function symbols which are called from do_debug_exception under
+NOKPROBE_SYMBOL, as they can not kprobed.
+
+Signed-off-by: Pratyush Anand <panand@redhat.com>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+---
+ arch/arm64/kernel/arm64ksyms.c     |  2 ++
+ arch/arm64/kernel/debug-monitors.c | 17 +++++++++++++++++
+ arch/arm64/kernel/hw_breakpoint.c  |  8 ++++++++
+ arch/arm64/kernel/kgdb.c           |  4 ++++
+ 4 files changed, 31 insertions(+)
+
+diff --git a/arch/arm64/kernel/arm64ksyms.c b/arch/arm64/kernel/arm64ksyms.c
+index 3b6d8cc..fbec403 100644
+--- a/arch/arm64/kernel/arm64ksyms.c
++++ b/arch/arm64/kernel/arm64ksyms.c
+@@ -26,6 +26,7 @@
+ #include <linux/syscalls.h>
+ #include <linux/uaccess.h>
+ #include <linux/io.h>
++#include <linux/kprobes.h>
+ 
+ #include <asm/checksum.h>
+ 
+@@ -67,4 +68,5 @@ EXPORT_SYMBOL(test_and_change_bit);
+ 
+ #ifdef CONFIG_FUNCTION_TRACER
+ EXPORT_SYMBOL(_mcount);
++NOKPROBE_SYMBOL(_mcount);
+ #endif
+diff --git a/arch/arm64/kernel/debug-monitors.c b/arch/arm64/kernel/debug-monitors.c
+index 17a4d37..6de6d9f 100644
+--- a/arch/arm64/kernel/debug-monitors.c
++++ b/arch/arm64/kernel/debug-monitors.c
+@@ -49,6 +49,7 @@ static void mdscr_write(u32 mdscr)
+ 	asm volatile("msr mdscr_el1, %0" :: "r" (mdscr));
+ 	local_dbg_restore(flags);
+ }
++NOKPROBE_SYMBOL(mdscr_write);
+ 
+ static u32 mdscr_read(void)
+ {
+@@ -56,6 +57,7 @@ static u32 mdscr_read(void)
+ 	asm volatile("mrs %0, mdscr_el1" : "=r" (mdscr));
+ 	return mdscr;
+ }
++NOKPROBE_SYMBOL(mdscr_read);
+ 
+ /*
+  * Allow root to disable self-hosted debug from userspace.
+@@ -104,6 +106,7 @@ void enable_debug_monitors(enum dbg_active_el el)
+ 		mdscr_write(mdscr);
+ 	}
+ }
++NOKPROBE_SYMBOL(enable_debug_monitors);
+ 
+ void disable_debug_monitors(enum dbg_active_el el)
+ {
+@@ -124,6 +127,7 @@ void disable_debug_monitors(enum dbg_active_el el)
+ 		mdscr_write(mdscr);
+ 	}
+ }
++NOKPROBE_SYMBOL(disable_debug_monitors);
+ 
+ /*
+  * OS lock clearing.
+@@ -174,6 +178,7 @@ static void set_regs_spsr_ss(struct pt_regs *regs)
+ 	spsr |= DBG_SPSR_SS;
+ 	regs->pstate = spsr;
+ }
++NOKPROBE_SYMBOL(set_regs_spsr_ss);
+ 
+ static void clear_regs_spsr_ss(struct pt_regs *regs)
+ {
+@@ -183,6 +188,7 @@ static void clear_regs_spsr_ss(struct pt_regs *regs)
+ 	spsr &= ~DBG_SPSR_SS;
+ 	regs->pstate = spsr;
+ }
++NOKPROBE_SYMBOL(clear_regs_spsr_ss);
+ 
+ /* EL1 Single Step Handler hooks */
+ static LIST_HEAD(step_hook);
+@@ -226,6 +232,7 @@ static int call_step_hook(struct pt_regs *regs, unsigned int esr)
+ 
+ 	return retval;
+ }
++NOKPROBE_SYMBOL(call_step_hook);
+ 
+ static int single_step_handler(unsigned long addr, unsigned int esr,
+ 			       struct pt_regs *regs)
+@@ -271,6 +278,7 @@ static int single_step_handler(unsigned long addr, unsigned int esr,
+ 
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(single_step_handler);
+ 
+ /*
+  * Breakpoint handler is re-entrant as another breakpoint can
+@@ -308,6 +316,7 @@ static int call_break_hook(struct pt_regs *regs, unsigned int esr)
+ 
+ 	return fn ? fn(regs, esr) : DBG_HOOK_ERROR;
+ }
++NOKPROBE_SYMBOL(call_break_hook);
+ 
+ static int brk_handler(unsigned long addr, unsigned int esr,
+ 		       struct pt_regs *regs)
+@@ -337,6 +346,7 @@ static int brk_handler(unsigned long addr, unsigned int esr,
+ 
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(brk_handler);
+ 
+ int aarch32_break_handler(struct pt_regs *regs)
+ {
+@@ -381,6 +391,7 @@ int aarch32_break_handler(struct pt_regs *regs)
+ 	force_sig_info(SIGTRAP, &info, current);
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(aarch32_break_handler);
+ 
+ static int __init debug_traps_init(void)
+ {
+@@ -402,6 +413,7 @@ void user_rewind_single_step(struct task_struct *task)
+ 	if (test_ti_thread_flag(task_thread_info(task), TIF_SINGLESTEP))
+ 		set_regs_spsr_ss(task_pt_regs(task));
+ }
++NOKPROBE_SYMBOL(user_rewind_single_step);
+ 
+ void user_fastforward_single_step(struct task_struct *task)
+ {
+@@ -417,6 +429,7 @@ void kernel_enable_single_step(struct pt_regs *regs)
+ 	mdscr_write(mdscr_read() | DBG_MDSCR_SS);
+ 	enable_debug_monitors(DBG_ACTIVE_EL1);
+ }
++NOKPROBE_SYMBOL(kernel_enable_single_step);
+ 
+ void kernel_disable_single_step(void)
+ {
+@@ -424,12 +437,14 @@ void kernel_disable_single_step(void)
+ 	mdscr_write(mdscr_read() & ~DBG_MDSCR_SS);
+ 	disable_debug_monitors(DBG_ACTIVE_EL1);
+ }
++NOKPROBE_SYMBOL(kernel_disable_single_step);
+ 
+ int kernel_active_single_step(void)
+ {
+ 	WARN_ON(!irqs_disabled());
+ 	return mdscr_read() & DBG_MDSCR_SS;
+ }
++NOKPROBE_SYMBOL(kernel_active_single_step);
+ 
+ /* ptrace API */
+ void user_enable_single_step(struct task_struct *task)
+@@ -439,8 +454,10 @@ void user_enable_single_step(struct task_struct *task)
+ 	if (!test_and_set_ti_thread_flag(ti, TIF_SINGLESTEP))
+ 		set_regs_spsr_ss(task_pt_regs(task));
+ }
++NOKPROBE_SYMBOL(user_enable_single_step);
+ 
+ void user_disable_single_step(struct task_struct *task)
+ {
+ 	clear_ti_thread_flag(task_thread_info(task), TIF_SINGLESTEP);
+ }
++NOKPROBE_SYMBOL(user_disable_single_step);
+diff --git a/arch/arm64/kernel/hw_breakpoint.c b/arch/arm64/kernel/hw_breakpoint.c
+index b45c95d..367a954 100644
+--- a/arch/arm64/kernel/hw_breakpoint.c
++++ b/arch/arm64/kernel/hw_breakpoint.c
+@@ -24,6 +24,7 @@
+ #include <linux/cpu_pm.h>
+ #include <linux/errno.h>
+ #include <linux/hw_breakpoint.h>
++#include <linux/kprobes.h>
+ #include <linux/perf_event.h>
+ #include <linux/ptrace.h>
+ #include <linux/smp.h>
+@@ -127,6 +128,7 @@ static u64 read_wb_reg(int reg, int n)
+ 
+ 	return val;
+ }
++NOKPROBE_SYMBOL(read_wb_reg);
+ 
+ static void write_wb_reg(int reg, int n, u64 val)
+ {
+@@ -140,6 +142,7 @@ static void write_wb_reg(int reg, int n, u64 val)
+ 	}
+ 	isb();
+ }
++NOKPROBE_SYMBOL(write_wb_reg);
+ 
+ /*
+  * Convert a breakpoint privilege level to the corresponding exception
+@@ -157,6 +160,7 @@ static enum dbg_active_el debug_exception_level(int privilege)
+ 		return -EINVAL;
+ 	}
+ }
++NOKPROBE_SYMBOL(debug_exception_level);
+ 
+ enum hw_breakpoint_ops {
+ 	HW_BREAKPOINT_INSTALL,
+@@ -575,6 +579,7 @@ static void toggle_bp_registers(int reg, enum dbg_active_el el, int enable)
+ 		write_wb_reg(reg, i, ctrl);
+ 	}
+ }
++NOKPROBE_SYMBOL(toggle_bp_registers);
+ 
+ /*
+  * Debug exception handlers.
+@@ -654,6 +659,7 @@ unlock:
+ 
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(breakpoint_handler);
+ 
+ static int watchpoint_handler(unsigned long addr, unsigned int esr,
+ 			      struct pt_regs *regs)
+@@ -756,6 +762,7 @@ unlock:
+ 
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(watchpoint_handler);
+ 
+ /*
+  * Handle single-step exception.
+@@ -813,6 +820,7 @@ int reinstall_suspended_bps(struct pt_regs *regs)
+ 
+ 	return !handled_exception;
+ }
++NOKPROBE_SYMBOL(reinstall_suspended_bps);
+ 
+ /*
+  * Context-switcher for restoring suspended breakpoints.
+diff --git a/arch/arm64/kernel/kgdb.c b/arch/arm64/kernel/kgdb.c
+index bcac81e..814d0c5 100644
+--- a/arch/arm64/kernel/kgdb.c
++++ b/arch/arm64/kernel/kgdb.c
+@@ -22,6 +22,7 @@
+ #include <linux/irq.h>
+ #include <linux/kdebug.h>
+ #include <linux/kgdb.h>
++#include <linux/kprobes.h>
+ #include <asm/traps.h>
+ 
+ struct dbg_reg_def_t dbg_reg_def[DBG_MAX_REG_NUM] = {
+@@ -218,6 +219,7 @@ static int kgdb_brk_fn(struct pt_regs *regs, unsigned int esr)
+ 	kgdb_handle_exception(1, SIGTRAP, 0, regs);
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(kgdb_brk_fn)
+ 
+ static int kgdb_compiled_brk_fn(struct pt_regs *regs, unsigned int esr)
+ {
+@@ -226,12 +228,14 @@ static int kgdb_compiled_brk_fn(struct pt_regs *regs, unsigned int esr)
+ 
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(kgdb_compiled_brk_fn);
+ 
+ static int kgdb_step_brk_fn(struct pt_regs *regs, unsigned int esr)
+ {
+ 	kgdb_handle_exception(1, SIGTRAP, 0, regs);
+ 	return 0;
+ }
++NOKPROBE_SYMBOL(kgdb_step_brk_fn);
+ 
+ static struct break_hook kgdb_brkpt_hook = {
+ 	.esr_mask	= 0xffffffff,
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0006-arm64-Treat-all-entry-code-as-non-kprobe-able.patch b/tools/kdump/0006-arm64-Treat-all-entry-code-as-non-kprobe-able.patch
new file mode 100644
index 0000000..a9ece1e
--- /dev/null
+++ b/tools/kdump/0006-arm64-Treat-all-entry-code-as-non-kprobe-able.patch
@@ -0,0 +1,94 @@
+From 012166d18ca4a8fe8ffc57a496343322ce3c7267 Mon Sep 17 00:00:00 2001
+From: Pratyush Anand <panand@redhat.com>
+Date: Thu, 29 Sep 2016 18:14:59 -0400
+Subject: [PATCH 006/123] arm64: Treat all entry code as non-kprobe-able
+
+commit 888b3c8720e0a4033db09ba2364afde6a4763638 upstream.
+
+Entry symbols are not kprobe safe. So blacklist them for kprobing.
+
+[dave.long@linaro.org: Remove check for hypervisor text]
+
+Signed-off-by: Pratyush Anand <panand@redhat.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+[catalin.marinas@arm.com: Do not include syscall wrappers in .entry.text]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/kernel/entry.S          |  3 +++
+ arch/arm64/kernel/probes/kprobes.c | 18 ++++++++++++++++++
+ arch/arm64/kernel/vmlinux.lds.S    |  1 +
+ 3 files changed, 22 insertions(+)
+
+diff --git a/arch/arm64/kernel/entry.S b/arch/arm64/kernel/entry.S
+index 698c4c6..e88854e 100644
+--- a/arch/arm64/kernel/entry.S
++++ b/arch/arm64/kernel/entry.S
+@@ -207,6 +207,7 @@ tsk	.req	x28		// current thread_info
+ /*
+  * Exception vectors.
+  */
++	.pushsection ".entry.text", "ax"
+ 
+ 	.align	11
+ ENTRY(vectors)
+@@ -748,6 +749,8 @@ __ni_sys_trace:
+ 	bl	do_ni_syscall
+ 	b	__sys_trace_return
+ 
++	.popsection				// .entry.text
++
+ /*
+  * Special system call wrappers.
+  */
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index 55e3e01..bb3d380 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -30,6 +30,7 @@
+ #include <asm/insn.h>
+ #include <asm/uaccess.h>
+ #include <asm/irq.h>
++#include <asm-generic/sections.h>
+ 
+ #include "decode-insn.h"
+ 
+@@ -516,6 +517,23 @@ int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
+ 	return 1;
+ }
+ 
++bool arch_within_kprobe_blacklist(unsigned long addr)
++{
++	extern char __idmap_text_start[], __idmap_text_end[];
++
++	if ((addr >= (unsigned long)__kprobes_text_start &&
++	    addr < (unsigned long)__kprobes_text_end) ||
++	    (addr >= (unsigned long)__entry_text_start &&
++	    addr < (unsigned long)__entry_text_end) ||
++	    (addr >= (unsigned long)__idmap_text_start &&
++	    addr < (unsigned long)__idmap_text_end) ||
++	    !!search_exception_tables(addr))
++		return true;
++
++
++	return false;
++}
++
+ int __init arch_init_kprobes(void)
+ {
+ 	return 0;
+diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
+index bc6b631..2fb8560 100644
+--- a/arch/arm64/kernel/vmlinux.lds.S
++++ b/arch/arm64/kernel/vmlinux.lds.S
+@@ -102,6 +102,7 @@ SECTIONS
+ 			*(.exception.text)
+ 			__exception_text_end = .;
+ 			IRQENTRY_TEXT
++			ENTRY_TEXT
+ 			TEXT_TEXT
+ 			SCHED_TEXT
+ 			LOCK_TEXT
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0007-arm64-kprobes-instruction-simulation-support.patch b/tools/kdump/0007-arm64-kprobes-instruction-simulation-support.patch
new file mode 100644
index 0000000..0812971
--- /dev/null
+++ b/tools/kdump/0007-arm64-kprobes-instruction-simulation-support.patch
@@ -0,0 +1,513 @@
+From bad1d25486643b5d5636076cf5c4b6b3bce1db2f Mon Sep 17 00:00:00 2001
+From: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Date: Fri, 8 Jul 2016 12:35:51 -0400
+Subject: [PATCH 007/123] arm64: kprobes instruction simulation support
+
+commit 39a67d49ba353630d144a8eb775500c041c89e7a upstream.
+
+Kprobes needs simulation of instructions that cannot be stepped
+from a different memory location, e.g.: those instructions
+that uses PC-relative addressing. In simulation, the behaviour
+of the instruction is implemented using a copy of pt_regs.
+
+The following instruction categories are simulated:
+ - All branching instructions(conditional, register, and immediate)
+ - Literal access instructions(load-literal, adr/adrp)
+
+Conditional execution is limited to branching instructions in
+ARM v8. If conditions at PSTATE do not match the condition fields
+of opcode, the instruction is effectively NOP.
+
+Thanks to Will Cohen for assorted suggested changes.
+
+Signed-off-by: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Signed-off-by: William Cohen <wcohen@redhat.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+[catalin.marinas@arm.com: removed linux/module.h include]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/include/asm/probes.h          |   5 +-
+ arch/arm64/kernel/insn.c                 |   1 +
+ arch/arm64/kernel/probes/Makefile        |   3 +-
+ arch/arm64/kernel/probes/decode-insn.c   |  33 ++++-
+ arch/arm64/kernel/probes/decode-insn.h   |   1 +
+ arch/arm64/kernel/probes/kprobes.c       |  53 ++++++--
+ arch/arm64/kernel/probes/simulate-insn.c | 217 +++++++++++++++++++++++++++++++
+ arch/arm64/kernel/probes/simulate-insn.h |  28 ++++
+ 8 files changed, 326 insertions(+), 15 deletions(-)
+ create mode 100644 arch/arm64/kernel/probes/simulate-insn.c
+ create mode 100644 arch/arm64/kernel/probes/simulate-insn.h
+
+diff --git a/arch/arm64/include/asm/probes.h b/arch/arm64/include/asm/probes.h
+index 1e8a21a..5af574d 100644
+--- a/arch/arm64/include/asm/probes.h
++++ b/arch/arm64/include/asm/probes.h
+@@ -15,17 +15,18 @@
+ #ifndef _ARM_PROBES_H
+ #define _ARM_PROBES_H
+ 
++#include <asm/opcodes.h>
++
+ struct kprobe;
+ struct arch_specific_insn;
+ 
+ typedef u32 kprobe_opcode_t;
+-typedef unsigned long (kprobes_pstate_check_t)(unsigned long);
+ typedef void (kprobes_handler_t) (u32 opcode, long addr, struct pt_regs *);
+ 
+ /* architecture specific copy of original instruction */
+ struct arch_specific_insn {
+ 	kprobe_opcode_t *insn;
+-	kprobes_pstate_check_t *pstate_cc;
++	pstate_check_t *pstate_cc;
+ 	kprobes_handler_t *handler;
+ 	/* restore address after step xol */
+ 	unsigned long restore;
+diff --git a/arch/arm64/kernel/insn.c b/arch/arm64/kernel/insn.c
+index b438070..750f422 100644
+--- a/arch/arm64/kernel/insn.c
++++ b/arch/arm64/kernel/insn.c
+@@ -30,6 +30,7 @@
+ #include <asm/cacheflush.h>
+ #include <asm/debug-monitors.h>
+ #include <asm/fixmap.h>
++#include <asm/opcodes.h>
+ #include <asm/insn.h>
+ 
+ #define AARCH64_INSN_SF_BIT	BIT(31)
+diff --git a/arch/arm64/kernel/probes/Makefile b/arch/arm64/kernel/probes/Makefile
+index bc159bf..e184d00 100644
+--- a/arch/arm64/kernel/probes/Makefile
++++ b/arch/arm64/kernel/probes/Makefile
+@@ -1 +1,2 @@
+-obj-$(CONFIG_KPROBES)		+= kprobes.o decode-insn.o
++obj-$(CONFIG_KPROBES)		+= kprobes.o decode-insn.o	\
++				   simulate-insn.o
+diff --git a/arch/arm64/kernel/probes/decode-insn.c b/arch/arm64/kernel/probes/decode-insn.c
+index 0518df1..f7931d9 100644
+--- a/arch/arm64/kernel/probes/decode-insn.c
++++ b/arch/arm64/kernel/probes/decode-insn.c
+@@ -21,6 +21,7 @@
+ #include <asm/sections.h>
+ 
+ #include "decode-insn.h"
++#include "simulate-insn.h"
+ 
+ static bool __kprobes aarch64_insn_is_steppable(u32 insn)
+ {
+@@ -74,6 +75,7 @@ static bool __kprobes aarch64_insn_is_steppable(u32 insn)
+ /* Return:
+  *   INSN_REJECTED     If instruction is one not allowed to kprobe,
+  *   INSN_GOOD         If instruction is supported and uses instruction slot,
++ *   INSN_GOOD_NO_SLOT If instruction is supported but doesn't use its slot.
+  */
+ static enum kprobe_insn __kprobes
+ arm_probe_decode_insn(kprobe_opcode_t insn, struct arch_specific_insn *asi)
+@@ -84,8 +86,37 @@ arm_probe_decode_insn(kprobe_opcode_t insn, struct arch_specific_insn *asi)
+ 	 */
+ 	if (aarch64_insn_is_steppable(insn))
+ 		return INSN_GOOD;
+-	else
++
++	if (aarch64_insn_is_bcond(insn)) {
++		asi->handler = simulate_b_cond;
++	} else if (aarch64_insn_is_cbz(insn) ||
++	    aarch64_insn_is_cbnz(insn)) {
++		asi->handler = simulate_cbz_cbnz;
++	} else if (aarch64_insn_is_tbz(insn) ||
++	    aarch64_insn_is_tbnz(insn)) {
++		asi->handler = simulate_tbz_tbnz;
++	} else if (aarch64_insn_is_adr_adrp(insn)) {
++		asi->handler = simulate_adr_adrp;
++	} else if (aarch64_insn_is_b(insn) ||
++	    aarch64_insn_is_bl(insn)) {
++		asi->handler = simulate_b_bl;
++	} else if (aarch64_insn_is_br(insn) ||
++	    aarch64_insn_is_blr(insn) ||
++	    aarch64_insn_is_ret(insn)) {
++		asi->handler = simulate_br_blr_ret;
++	} else if (aarch64_insn_is_ldr_lit(insn)) {
++		asi->handler = simulate_ldr_literal;
++	} else if (aarch64_insn_is_ldrsw_lit(insn)) {
++		asi->handler = simulate_ldrsw_literal;
++	} else {
++		/*
++		 * Instruction cannot be stepped out-of-line and we don't
++		 * (yet) simulate it.
++		 */
+ 		return INSN_REJECTED;
++	}
++
++	return INSN_GOOD_NO_SLOT;
+ }
+ 
+ static bool __kprobes
+diff --git a/arch/arm64/kernel/probes/decode-insn.h b/arch/arm64/kernel/probes/decode-insn.h
+index ad5ba9c..d438289 100644
+--- a/arch/arm64/kernel/probes/decode-insn.h
++++ b/arch/arm64/kernel/probes/decode-insn.h
+@@ -25,6 +25,7 @@
+ 
+ enum kprobe_insn {
+ 	INSN_REJECTED,
++	INSN_GOOD_NO_SLOT,
+ 	INSN_GOOD,
+ };
+ 
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index bb3d380..aff80b3 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -42,6 +42,9 @@ void jprobe_return_break(void);
+ DEFINE_PER_CPU(struct kprobe *, current_kprobe) = NULL;
+ DEFINE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
+ 
++static void __kprobes
++post_kprobe_handler(struct kprobe_ctlblk *, struct pt_regs *);
++
+ static void __kprobes arch_prepare_ss_slot(struct kprobe *p)
+ {
+ 	/* prepare insn slot */
+@@ -58,6 +61,23 @@ static void __kprobes arch_prepare_ss_slot(struct kprobe *p)
+ 	  sizeof(kprobe_opcode_t);
+ }
+ 
++static void __kprobes arch_prepare_simulate(struct kprobe *p)
++{
++	/* This instructions is not executed xol. No need to adjust the PC */
++	p->ainsn.restore = 0;
++}
++
++static void __kprobes arch_simulate_insn(struct kprobe *p, struct pt_regs *regs)
++{
++	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
++
++	if (p->ainsn.handler)
++		p->ainsn.handler((u32)p->opcode, (long)p->addr, regs);
++
++	/* single step simulated, now go for post processing */
++	post_kprobe_handler(kcb, regs);
++}
++
+ int __kprobes arch_prepare_kprobe(struct kprobe *p)
+ {
+ 	unsigned long probe_addr = (unsigned long)p->addr;
+@@ -81,6 +101,10 @@ int __kprobes arch_prepare_kprobe(struct kprobe *p)
+ 	case INSN_REJECTED:	/* insn not supported */
+ 		return -EINVAL;
+ 
++	case INSN_GOOD_NO_SLOT:	/* insn need simulation */
++		p->ainsn.insn = NULL;
++		break;
++
+ 	case INSN_GOOD:	/* instruction uses slot */
+ 		p->ainsn.insn = get_insn_slot();
+ 		if (!p->ainsn.insn)
+@@ -89,7 +113,10 @@ int __kprobes arch_prepare_kprobe(struct kprobe *p)
+ 	};
+ 
+ 	/* prepare the instruction */
+-	arch_prepare_ss_slot(p);
++	if (p->ainsn.insn)
++		arch_prepare_ss_slot(p);
++	else
++		arch_prepare_simulate(p);
+ 
+ 	return 0;
+ }
+@@ -215,20 +242,24 @@ static void __kprobes setup_singlestep(struct kprobe *p,
+ 		kcb->kprobe_status = KPROBE_HIT_SS;
+ 	}
+ 
+-	BUG_ON(!p->ainsn.insn);
+ 
+-	/* prepare for single stepping */
+-	slot = (unsigned long)p->ainsn.insn;
++	if (p->ainsn.insn) {
++		/* prepare for single stepping */
++		slot = (unsigned long)p->ainsn.insn;
+ 
+-	set_ss_context(kcb, slot);	/* mark pending ss */
++		set_ss_context(kcb, slot);	/* mark pending ss */
+ 
+-	if (kcb->kprobe_status == KPROBE_REENTER)
+-		spsr_set_debug_flag(regs, 0);
++		if (kcb->kprobe_status == KPROBE_REENTER)
++			spsr_set_debug_flag(regs, 0);
+ 
+-	/* IRQs and single stepping do not mix well. */
+-	kprobes_save_local_irqflag(kcb, regs);
+-	kernel_enable_single_step(regs);
+-	instruction_pointer_set(regs, slot);
++		/* IRQs and single stepping do not mix well. */
++		kprobes_save_local_irqflag(kcb, regs);
++		kernel_enable_single_step(regs);
++		instruction_pointer_set(regs, slot);
++	} else {
++		/* insn simulation */
++		arch_simulate_insn(p, regs);
++	}
+ }
+ 
+ static int __kprobes reenter_kprobe(struct kprobe *p,
+diff --git a/arch/arm64/kernel/probes/simulate-insn.c b/arch/arm64/kernel/probes/simulate-insn.c
+new file mode 100644
+index 0000000..8977ce9
+--- /dev/null
++++ b/arch/arm64/kernel/probes/simulate-insn.c
+@@ -0,0 +1,217 @@
++/*
++ * arch/arm64/kernel/probes/simulate-insn.c
++ *
++ * Copyright (C) 2013 Linaro Limited.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ */
++
++#include <linux/kernel.h>
++#include <linux/kprobes.h>
++
++#include "simulate-insn.h"
++
++#define sign_extend(x, signbit)		\
++	((x) | (0 - ((x) & (1 << (signbit)))))
++
++#define bbl_displacement(insn)		\
++	sign_extend(((insn) & 0x3ffffff) << 2, 27)
++
++#define bcond_displacement(insn)	\
++	sign_extend(((insn >> 5) & 0x7ffff) << 2, 20)
++
++#define cbz_displacement(insn)	\
++	sign_extend(((insn >> 5) & 0x7ffff) << 2, 20)
++
++#define tbz_displacement(insn)	\
++	sign_extend(((insn >> 5) & 0x3fff) << 2, 15)
++
++#define ldr_displacement(insn)	\
++	sign_extend(((insn >> 5) & 0x7ffff) << 2, 20)
++
++static inline void set_x_reg(struct pt_regs *regs, int reg, u64 val)
++{
++	if (reg < 31)
++		regs->regs[reg] = val;
++}
++
++static inline void set_w_reg(struct pt_regs *regs, int reg, u64 val)
++{
++	if (reg < 31)
++		regs->regs[reg] = lower_32_bits(val);
++}
++
++static inline u64 get_x_reg(struct pt_regs *regs, int reg)
++{
++	if (reg < 31)
++		return regs->regs[reg];
++	else
++		return 0;
++}
++
++static inline u32 get_w_reg(struct pt_regs *regs, int reg)
++{
++	if (reg < 31)
++		return lower_32_bits(regs->regs[reg]);
++	else
++		return 0;
++}
++
++static bool __kprobes check_cbz(u32 opcode, struct pt_regs *regs)
++{
++	int xn = opcode & 0x1f;
++
++	return (opcode & (1 << 31)) ?
++	    (get_x_reg(regs, xn) == 0) : (get_w_reg(regs, xn) == 0);
++}
++
++static bool __kprobes check_cbnz(u32 opcode, struct pt_regs *regs)
++{
++	int xn = opcode & 0x1f;
++
++	return (opcode & (1 << 31)) ?
++	    (get_x_reg(regs, xn) != 0) : (get_w_reg(regs, xn) != 0);
++}
++
++static bool __kprobes check_tbz(u32 opcode, struct pt_regs *regs)
++{
++	int xn = opcode & 0x1f;
++	int bit_pos = ((opcode & (1 << 31)) >> 26) | ((opcode >> 19) & 0x1f);
++
++	return ((get_x_reg(regs, xn) >> bit_pos) & 0x1) == 0;
++}
++
++static bool __kprobes check_tbnz(u32 opcode, struct pt_regs *regs)
++{
++	int xn = opcode & 0x1f;
++	int bit_pos = ((opcode & (1 << 31)) >> 26) | ((opcode >> 19) & 0x1f);
++
++	return ((get_x_reg(regs, xn) >> bit_pos) & 0x1) != 0;
++}
++
++/*
++ * instruction simulation functions
++ */
++void __kprobes
++simulate_adr_adrp(u32 opcode, long addr, struct pt_regs *regs)
++{
++	long imm, xn, val;
++
++	xn = opcode & 0x1f;
++	imm = ((opcode >> 3) & 0x1ffffc) | ((opcode >> 29) & 0x3);
++	imm = sign_extend(imm, 20);
++	if (opcode & 0x80000000)
++		val = (imm<<12) + (addr & 0xfffffffffffff000);
++	else
++		val = imm + addr;
++
++	set_x_reg(regs, xn, val);
++
++	instruction_pointer_set(regs, instruction_pointer(regs) + 4);
++}
++
++void __kprobes
++simulate_b_bl(u32 opcode, long addr, struct pt_regs *regs)
++{
++	int disp = bbl_displacement(opcode);
++
++	/* Link register is x30 */
++	if (opcode & (1 << 31))
++		set_x_reg(regs, 30, addr + 4);
++
++	instruction_pointer_set(regs, addr + disp);
++}
++
++void __kprobes
++simulate_b_cond(u32 opcode, long addr, struct pt_regs *regs)
++{
++	int disp = 4;
++
++	if (aarch32_opcode_cond_checks[opcode & 0xf](regs->pstate & 0xffffffff))
++		disp = bcond_displacement(opcode);
++
++	instruction_pointer_set(regs, addr + disp);
++}
++
++void __kprobes
++simulate_br_blr_ret(u32 opcode, long addr, struct pt_regs *regs)
++{
++	int xn = (opcode >> 5) & 0x1f;
++
++	/* update pc first in case we're doing a "blr lr" */
++	instruction_pointer_set(regs, get_x_reg(regs, xn));
++
++	/* Link register is x30 */
++	if (((opcode >> 21) & 0x3) == 1)
++		set_x_reg(regs, 30, addr + 4);
++}
++
++void __kprobes
++simulate_cbz_cbnz(u32 opcode, long addr, struct pt_regs *regs)
++{
++	int disp = 4;
++
++	if (opcode & (1 << 24)) {
++		if (check_cbnz(opcode, regs))
++			disp = cbz_displacement(opcode);
++	} else {
++		if (check_cbz(opcode, regs))
++			disp = cbz_displacement(opcode);
++	}
++	instruction_pointer_set(regs, addr + disp);
++}
++
++void __kprobes
++simulate_tbz_tbnz(u32 opcode, long addr, struct pt_regs *regs)
++{
++	int disp = 4;
++
++	if (opcode & (1 << 24)) {
++		if (check_tbnz(opcode, regs))
++			disp = tbz_displacement(opcode);
++	} else {
++		if (check_tbz(opcode, regs))
++			disp = tbz_displacement(opcode);
++	}
++	instruction_pointer_set(regs, addr + disp);
++}
++
++void __kprobes
++simulate_ldr_literal(u32 opcode, long addr, struct pt_regs *regs)
++{
++	u64 *load_addr;
++	int xn = opcode & 0x1f;
++	int disp;
++
++	disp = ldr_displacement(opcode);
++	load_addr = (u64 *) (addr + disp);
++
++	if (opcode & (1 << 30))	/* x0-x30 */
++		set_x_reg(regs, xn, *load_addr);
++	else			/* w0-w30 */
++		set_w_reg(regs, xn, *load_addr);
++
++	instruction_pointer_set(regs, instruction_pointer(regs) + 4);
++}
++
++void __kprobes
++simulate_ldrsw_literal(u32 opcode, long addr, struct pt_regs *regs)
++{
++	s32 *load_addr;
++	int xn = opcode & 0x1f;
++	int disp;
++
++	disp = ldr_displacement(opcode);
++	load_addr = (s32 *) (addr + disp);
++
++	set_x_reg(regs, xn, *load_addr);
++
++	instruction_pointer_set(regs, instruction_pointer(regs) + 4);
++}
+diff --git a/arch/arm64/kernel/probes/simulate-insn.h b/arch/arm64/kernel/probes/simulate-insn.h
+new file mode 100644
+index 0000000..050bde6
+--- /dev/null
++++ b/arch/arm64/kernel/probes/simulate-insn.h
+@@ -0,0 +1,28 @@
++/*
++ * arch/arm64/kernel/probes/simulate-insn.h
++ *
++ * Copyright (C) 2013 Linaro Limited
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
++ * General Public License for more details.
++ */
++
++#ifndef _ARM_KERNEL_KPROBES_SIMULATE_INSN_H
++#define _ARM_KERNEL_KPROBES_SIMULATE_INSN_H
++
++void simulate_adr_adrp(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_b_bl(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_b_cond(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_br_blr_ret(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_cbz_cbnz(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_tbz_tbnz(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_ldr_literal(u32 opcode, long addr, struct pt_regs *regs);
++void simulate_ldrsw_literal(u32 opcode, long addr, struct pt_regs *regs);
++
++#endif /* _ARM_KERNEL_KPROBES_SIMULATE_INSN_H */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0008-arm64-Add-trampoline-code-for-kretprobes.patch b/tools/kdump/0008-arm64-Add-trampoline-code-for-kretprobes.patch
new file mode 100644
index 0000000..0967066
--- /dev/null
+++ b/tools/kdump/0008-arm64-Add-trampoline-code-for-kretprobes.patch
@@ -0,0 +1,177 @@
+From e0f735f1df6c7aaf434751457a5e4b0a61fed376 Mon Sep 17 00:00:00 2001
+From: William Cohen <wcohen@redhat.com>
+Date: Fri, 8 Jul 2016 12:35:52 -0400
+Subject: [PATCH 008/123] arm64: Add trampoline code for kretprobes
+
+commit da6a91252ad98d49b49e83b76c1f032cdf6e5258 upstream.
+
+The trampoline code is used by kretprobes to capture a return from a probed
+function.  This is done by saving the registers, calling the handler, and
+restoring the registers. The code then returns to the original saved caller
+return address. It is necessary to do this directly instead of using a
+software breakpoint because the code used in processing that breakpoint
+could itself be kprobe'd and cause a problematic reentry into the debug
+exception handler.
+
+Signed-off-by: William Cohen <wcohen@redhat.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+[catalin.marinas@arm.com: removed unnecessary masking of the PSTATE bits]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/include/asm/kprobes.h              |  2 +
+ arch/arm64/kernel/asm-offsets.c               | 11 ++++
+ arch/arm64/kernel/probes/Makefile             |  1 +
+ arch/arm64/kernel/probes/kprobes.c            |  5 ++
+ arch/arm64/kernel/probes/kprobes_trampoline.S | 81 +++++++++++++++++++++++++++
+ 5 files changed, 100 insertions(+)
+ create mode 100644 arch/arm64/kernel/probes/kprobes_trampoline.S
+
+diff --git a/arch/arm64/include/asm/kprobes.h b/arch/arm64/include/asm/kprobes.h
+index 79c9511..61b4915 100644
+--- a/arch/arm64/include/asm/kprobes.h
++++ b/arch/arm64/include/asm/kprobes.h
+@@ -56,5 +56,7 @@ int kprobe_exceptions_notify(struct notifier_block *self,
+ 			     unsigned long val, void *data);
+ int kprobe_breakpoint_handler(struct pt_regs *regs, unsigned int esr);
+ int kprobe_single_step_handler(struct pt_regs *regs, unsigned int esr);
++void kretprobe_trampoline(void);
++void __kprobes *trampoline_probe_handler(struct pt_regs *regs);
+ 
+ #endif /* _ARM_KPROBES_H */
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 087cf9a..dd292519 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -49,6 +49,17 @@ int main(void)
+   DEFINE(S_X5,			offsetof(struct pt_regs, regs[5]));
+   DEFINE(S_X6,			offsetof(struct pt_regs, regs[6]));
+   DEFINE(S_X7,			offsetof(struct pt_regs, regs[7]));
++  DEFINE(S_X8,			offsetof(struct pt_regs, regs[8]));
++  DEFINE(S_X10,			offsetof(struct pt_regs, regs[10]));
++  DEFINE(S_X12,			offsetof(struct pt_regs, regs[12]));
++  DEFINE(S_X14,			offsetof(struct pt_regs, regs[14]));
++  DEFINE(S_X16,			offsetof(struct pt_regs, regs[16]));
++  DEFINE(S_X18,			offsetof(struct pt_regs, regs[18]));
++  DEFINE(S_X20,			offsetof(struct pt_regs, regs[20]));
++  DEFINE(S_X22,			offsetof(struct pt_regs, regs[22]));
++  DEFINE(S_X24,			offsetof(struct pt_regs, regs[24]));
++  DEFINE(S_X26,			offsetof(struct pt_regs, regs[26]));
++  DEFINE(S_X28,			offsetof(struct pt_regs, regs[28]));
+   DEFINE(S_LR,			offsetof(struct pt_regs, regs[30]));
+   DEFINE(S_SP,			offsetof(struct pt_regs, sp));
+ #ifdef CONFIG_COMPAT
+diff --git a/arch/arm64/kernel/probes/Makefile b/arch/arm64/kernel/probes/Makefile
+index e184d00..ce06312 100644
+--- a/arch/arm64/kernel/probes/Makefile
++++ b/arch/arm64/kernel/probes/Makefile
+@@ -1,2 +1,3 @@
+ obj-$(CONFIG_KPROBES)		+= kprobes.o decode-insn.o	\
++				   kprobes_trampoline.o		\
+ 				   simulate-insn.o
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index aff80b3..5619041 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -565,6 +565,11 @@ bool arch_within_kprobe_blacklist(unsigned long addr)
+ 	return false;
+ }
+ 
++void __kprobes __used *trampoline_probe_handler(struct pt_regs *regs)
++{
++	return NULL;
++}
++
+ int __init arch_init_kprobes(void)
+ {
+ 	return 0;
+diff --git a/arch/arm64/kernel/probes/kprobes_trampoline.S b/arch/arm64/kernel/probes/kprobes_trampoline.S
+new file mode 100644
+index 0000000..5d6e7f1
+--- /dev/null
++++ b/arch/arm64/kernel/probes/kprobes_trampoline.S
+@@ -0,0 +1,81 @@
++/*
++ * trampoline entry and return code for kretprobes.
++ */
++
++#include <linux/linkage.h>
++#include <asm/asm-offsets.h>
++#include <asm/assembler.h>
++
++	.text
++
++	.macro	save_all_base_regs
++	stp x0, x1, [sp, #S_X0]
++	stp x2, x3, [sp, #S_X2]
++	stp x4, x5, [sp, #S_X4]
++	stp x6, x7, [sp, #S_X6]
++	stp x8, x9, [sp, #S_X8]
++	stp x10, x11, [sp, #S_X10]
++	stp x12, x13, [sp, #S_X12]
++	stp x14, x15, [sp, #S_X14]
++	stp x16, x17, [sp, #S_X16]
++	stp x18, x19, [sp, #S_X18]
++	stp x20, x21, [sp, #S_X20]
++	stp x22, x23, [sp, #S_X22]
++	stp x24, x25, [sp, #S_X24]
++	stp x26, x27, [sp, #S_X26]
++	stp x28, x29, [sp, #S_X28]
++	add x0, sp, #S_FRAME_SIZE
++	stp lr, x0, [sp, #S_LR]
++	/*
++	 * Construct a useful saved PSTATE
++	 */
++	mrs x0, nzcv
++	mrs x1, daif
++	orr x0, x0, x1
++	mrs x1, CurrentEL
++	orr x0, x0, x1
++	mrs x1, SPSel
++	orr x0, x0, x1
++	stp xzr, x0, [sp, #S_PC]
++	.endm
++
++	.macro	restore_all_base_regs
++	ldr x0, [sp, #S_PSTATE]
++	and x0, x0, #(PSR_N_BIT | PSR_Z_BIT | PSR_C_BIT | PSR_V_BIT)
++	msr nzcv, x0
++	ldp x0, x1, [sp, #S_X0]
++	ldp x2, x3, [sp, #S_X2]
++	ldp x4, x5, [sp, #S_X4]
++	ldp x6, x7, [sp, #S_X6]
++	ldp x8, x9, [sp, #S_X8]
++	ldp x10, x11, [sp, #S_X10]
++	ldp x12, x13, [sp, #S_X12]
++	ldp x14, x15, [sp, #S_X14]
++	ldp x16, x17, [sp, #S_X16]
++	ldp x18, x19, [sp, #S_X18]
++	ldp x20, x21, [sp, #S_X20]
++	ldp x22, x23, [sp, #S_X22]
++	ldp x24, x25, [sp, #S_X24]
++	ldp x26, x27, [sp, #S_X26]
++	ldp x28, x29, [sp, #S_X28]
++	.endm
++
++ENTRY(kretprobe_trampoline)
++	sub sp, sp, #S_FRAME_SIZE
++
++	save_all_base_regs
++
++	mov x0, sp
++	bl trampoline_probe_handler
++	/*
++	 * Replace trampoline address in lr with actual orig_ret_addr return
++	 * address.
++	 */
++	mov lr, x0
++
++	restore_all_base_regs
++
++	add sp, sp, #S_FRAME_SIZE
++	ret
++
++ENDPROC(kretprobe_trampoline)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0009-arm64-Add-kernel-return-probes-support-kretprobes.patch b/tools/kdump/0009-arm64-Add-kernel-return-probes-support-kretprobes.patch
new file mode 100644
index 0000000..1e9555e
--- /dev/null
+++ b/tools/kdump/0009-arm64-Add-kernel-return-probes-support-kretprobes.patch
@@ -0,0 +1,137 @@
+From ae5c49e1570bebeef750c1020d4199b23380fb39 Mon Sep 17 00:00:00 2001
+From: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Date: Fri, 8 Jul 2016 12:35:53 -0400
+Subject: [PATCH 009/123] arm64: Add kernel return probes support (kretprobes)
+
+commit fcfd708b8cf86b8c1ca6ce014d50287f61c0eb88 upstream.
+
+The pre-handler of this special 'trampoline' kprobe executes the return
+probe handler functions and restores original return address in ELR_EL1.
+This way the saved pt_regs still hold the original register context to be
+carried back to the probed kernel function.
+
+Signed-off-by: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/Kconfig                 |  1 +
+ arch/arm64/kernel/probes/kprobes.c | 90 +++++++++++++++++++++++++++++++++++++-
+ 2 files changed, 90 insertions(+), 1 deletion(-)
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index e6370c4..7a2b7a2 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -79,6 +79,7 @@ config ARM64
+ 	select HAVE_RCU_TABLE_FREE
+ 	select HAVE_SYSCALL_TRACEPOINTS
+ 	select HAVE_KPROBES
++	select HAVE_KRETPROBES if HAVE_KPROBES
+ 	select IOMMU_DMA if IOMMU_SUPPORT
+ 	select IRQ_DOMAIN
+ 	select IRQ_FORCED_THREADING
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index 5619041..f1498b3 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -567,7 +567,95 @@ bool arch_within_kprobe_blacklist(unsigned long addr)
+ 
+ void __kprobes __used *trampoline_probe_handler(struct pt_regs *regs)
+ {
+-	return NULL;
++	struct kretprobe_instance *ri = NULL;
++	struct hlist_head *head, empty_rp;
++	struct hlist_node *tmp;
++	unsigned long flags, orig_ret_address = 0;
++	unsigned long trampoline_address =
++		(unsigned long)&kretprobe_trampoline;
++	kprobe_opcode_t *correct_ret_addr = NULL;
++
++	INIT_HLIST_HEAD(&empty_rp);
++	kretprobe_hash_lock(current, &head, &flags);
++
++	/*
++	 * It is possible to have multiple instances associated with a given
++	 * task either because multiple functions in the call path have
++	 * return probes installed on them, and/or more than one
++	 * return probe was registered for a target function.
++	 *
++	 * We can handle this because:
++	 *     - instances are always pushed into the head of the list
++	 *     - when multiple return probes are registered for the same
++	 *	 function, the (chronologically) first instance's ret_addr
++	 *	 will be the real return address, and all the rest will
++	 *	 point to kretprobe_trampoline.
++	 */
++	hlist_for_each_entry_safe(ri, tmp, head, hlist) {
++		if (ri->task != current)
++			/* another task is sharing our hash bucket */
++			continue;
++
++		orig_ret_address = (unsigned long)ri->ret_addr;
++
++		if (orig_ret_address != trampoline_address)
++			/*
++			 * This is the real return address. Any other
++			 * instances associated with this task are for
++			 * other calls deeper on the call stack
++			 */
++			break;
++	}
++
++	kretprobe_assert(ri, orig_ret_address, trampoline_address);
++
++	correct_ret_addr = ri->ret_addr;
++	hlist_for_each_entry_safe(ri, tmp, head, hlist) {
++		if (ri->task != current)
++			/* another task is sharing our hash bucket */
++			continue;
++
++		orig_ret_address = (unsigned long)ri->ret_addr;
++		if (ri->rp && ri->rp->handler) {
++			__this_cpu_write(current_kprobe, &ri->rp->kp);
++			get_kprobe_ctlblk()->kprobe_status = KPROBE_HIT_ACTIVE;
++			ri->ret_addr = correct_ret_addr;
++			ri->rp->handler(ri, regs);
++			__this_cpu_write(current_kprobe, NULL);
++		}
++
++		recycle_rp_inst(ri, &empty_rp);
++
++		if (orig_ret_address != trampoline_address)
++			/*
++			 * This is the real return address. Any other
++			 * instances associated with this task are for
++			 * other calls deeper on the call stack
++			 */
++			break;
++	}
++
++	kretprobe_hash_unlock(current, &flags);
++
++	hlist_for_each_entry_safe(ri, tmp, &empty_rp, hlist) {
++		hlist_del(&ri->hlist);
++		kfree(ri);
++	}
++	return (void *)orig_ret_address;
++}
++
++void __kprobes arch_prepare_kretprobe(struct kretprobe_instance *ri,
++				      struct pt_regs *regs)
++{
++	ri->ret_addr = (kprobe_opcode_t *)regs->regs[30];
++
++	/* replace return addr (x30) with trampoline */
++	regs->regs[30] = (long)&kretprobe_trampoline;
++}
++
++int __kprobes arch_trampoline_kprobe(struct kprobe *p)
++{
++	return 0;
+ }
+ 
+ int __init arch_init_kprobes(void)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0010-kprobes-Add-arm64-case-in-kprobe-example-module.patch b/tools/kdump/0010-kprobes-Add-arm64-case-in-kprobe-example-module.patch
new file mode 100644
index 0000000..6698805
--- /dev/null
+++ b/tools/kdump/0010-kprobes-Add-arm64-case-in-kprobe-example-module.patch
@@ -0,0 +1,47 @@
+From 168c92517f470e39973cd813cd3426fe95ac647b Mon Sep 17 00:00:00 2001
+From: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Date: Fri, 8 Jul 2016 12:35:54 -0400
+Subject: [PATCH 010/123] kprobes: Add arm64 case in kprobe example module
+
+commit af78cede8bfc772baf424fc03f7cd3c8f9437733 upstream.
+
+Add info prints in sample kprobe handlers for ARM64
+
+Signed-off-by: Sandeepa Prabhu <sandeepa.s.prabhu@gmail.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ samples/kprobes/kprobe_example.c | 9 +++++++++
+ 1 file changed, 9 insertions(+)
+
+diff --git a/samples/kprobes/kprobe_example.c b/samples/kprobes/kprobe_example.c
+index 727eb21..8379543 100644
+--- a/samples/kprobes/kprobe_example.c
++++ b/samples/kprobes/kprobe_example.c
+@@ -42,6 +42,11 @@ static int handler_pre(struct kprobe *p, struct pt_regs *regs)
+ 			" ex1 = 0x%lx\n",
+ 		p->addr, regs->pc, regs->ex1);
+ #endif
++#ifdef CONFIG_ARM64
++	pr_info("<%s> pre_handler: p->addr = 0x%p, pc = 0x%lx,"
++			" pstate = 0x%lx\n",
++		p->symbol_name, p->addr, (long)regs->pc, (long)regs->pstate);
++#endif
+ 
+ 	/* A dump_stack() here will give a stack backtrace */
+ 	return 0;
+@@ -67,6 +72,10 @@ static void handler_post(struct kprobe *p, struct pt_regs *regs,
+ 	printk(KERN_INFO "post_handler: p->addr = 0x%p, ex1 = 0x%lx\n",
+ 		p->addr, regs->ex1);
+ #endif
++#ifdef CONFIG_ARM64
++	pr_info("<%s> post_handler: p->addr = 0x%p, pstate = 0x%lx\n",
++		p->symbol_name, p->addr, (long)regs->pstate);
++#endif
+ }
+ 
+ /*
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0011-arm64-kprobes-WARN-if-attempting-to-step-with-PSTATE.patch b/tools/kdump/0011-arm64-kprobes-WARN-if-attempting-to-step-with-PSTATE.patch
new file mode 100644
index 0000000..186f198
--- /dev/null
+++ b/tools/kdump/0011-arm64-kprobes-WARN-if-attempting-to-step-with-PSTATE.patch
@@ -0,0 +1,37 @@
+From aedefb3b52a0cfe1e34102b75e6415042c597751 Mon Sep 17 00:00:00 2001
+From: Will Deacon <will.deacon@arm.com>
+Date: Tue, 19 Jul 2016 15:07:39 +0100
+Subject: [PATCH 011/123] arm64: kprobes: WARN if attempting to step with
+ PSTATE.D=1
+
+commit 44bd887ce10eb8061f6a137f8a73f823957edd82 upstream.
+
+Stepping with PSTATE.D=1 is bad news. The step won't generate a debug
+exception and we'll likely walk off into random data structures. This
+should never happen, but when it does, it's a PITA to debug. Add a
+WARN_ON to shout if we realise this is about to take place.
+
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+Acked-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+---
+ arch/arm64/kernel/probes/kprobes.c | 2 ++
+ 1 file changed, 2 insertions(+)
+
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index f1498b3..a9b274c 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -251,6 +251,8 @@ static void __kprobes setup_singlestep(struct kprobe *p,
+ 
+ 		if (kcb->kprobe_status == KPROBE_REENTER)
+ 			spsr_set_debug_flag(regs, 0);
++		else
++			WARN_ON(regs->pstate & PSR_D_BIT);
+ 
+ 		/* IRQs and single stepping do not mix well. */
+ 		kprobes_save_local_irqflag(kcb, regs);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0012-arm64-kprobes-Fix-overflow-when-saving-stack.patch b/tools/kdump/0012-arm64-kprobes-Fix-overflow-when-saving-stack.patch
new file mode 100644
index 0000000..47c78d3
--- /dev/null
+++ b/tools/kdump/0012-arm64-kprobes-Fix-overflow-when-saving-stack.patch
@@ -0,0 +1,78 @@
+From 8caaa405b29a1b6bedf7653a2b0d2124d5f3cf63 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Thu, 29 Sep 2016 18:18:23 -0400
+Subject: [PATCH 012/123] arm64: kprobes: Fix overflow when saving stack
+
+commit ab4c1325d4bf111a590a1f773e3d93bde7f40201 upstream.
+
+The MIN_STACK_SIZE macro tries evaluate how much stack space needs
+to be saved in the jprobes_stack array, sized at 128 bytes.
+
+When using the IRQ stack, said macro can happily return up to
+IRQ_STACK_SIZE, which is 16kB. Mayhem follows.
+
+This patch fixes things by getting rid of the crazy macro and
+limiting the copy to be at most the size of the jprobes_stack
+array, no matter which stack we're on.
+
+[dave.long@linaro.org: Since there is no irq_stack in this kernel version
+this fix is not strictly necessary, but is included for completeness.]
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+---
+ arch/arm64/kernel/probes/kprobes.c | 16 +++++++++++-----
+ 1 file changed, 11 insertions(+), 5 deletions(-)
+
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index a9b274c..b118974 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -34,9 +34,6 @@
+ 
+ #include "decode-insn.h"
+ 
+-#define MIN_STACK_SIZE(addr)	min((unsigned long)MAX_STACK_SIZE,	\
+-	(unsigned long)current_thread_info() + THREAD_START_SP - (addr))
+-
+ void jprobe_return_break(void);
+ 
+ DEFINE_PER_CPU(struct kprobe *, current_kprobe) = NULL;
+@@ -45,6 +42,15 @@ DEFINE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
+ static void __kprobes
+ post_kprobe_handler(struct kprobe_ctlblk *, struct pt_regs *);
+ 
++static inline unsigned long min_stack_size(unsigned long addr)
++{
++	unsigned long size;
++
++	size = (unsigned long)current_thread_info() + THREAD_START_SP - addr;
++
++	return min(size, FIELD_SIZEOF(struct kprobe_ctlblk, jprobes_stack));
++}
++
+ static void __kprobes arch_prepare_ss_slot(struct kprobe *p)
+ {
+ 	/* prepare insn slot */
+@@ -492,7 +498,7 @@ int __kprobes setjmp_pre_handler(struct kprobe *p, struct pt_regs *regs)
+ 	 * the argument area.
+ 	 */
+ 	memcpy(kcb->jprobes_stack, (void *)stack_ptr,
+-	       MIN_STACK_SIZE(stack_ptr));
++	       min_stack_size(stack_ptr));
+ 
+ 	instruction_pointer_set(regs, (unsigned long) jp->entry);
+ 	preempt_disable();
+@@ -545,7 +551,7 @@ int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
+ 	unpause_graph_tracing();
+ 	*regs = kcb->jprobe_saved_regs;
+ 	memcpy((void *)stack_addr, kcb->jprobes_stack,
+-	       MIN_STACK_SIZE(stack_addr));
++	       min_stack_size(stack_addr));
+ 	preempt_enable_no_resched();
+ 	return 1;
+ }
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0013-arm64-kprobes-Cleanup-jprobe_return.patch b/tools/kdump/0013-arm64-kprobes-Cleanup-jprobe_return.patch
new file mode 100644
index 0000000..017b083
--- /dev/null
+++ b/tools/kdump/0013-arm64-kprobes-Cleanup-jprobe_return.patch
@@ -0,0 +1,83 @@
+From a6e5298aaf88fa7e703a1b22757c993a9ccd7109 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Thu, 21 Jul 2016 09:44:17 +0100
+Subject: [PATCH 013/123] arm64: kprobes: Cleanup jprobe_return
+
+commit 3b7d14e9f3f1efd4c4348800e977fd1ce4ca660e upstream.
+
+jprobe_return seems to have aged badly. Comments referring to
+non-existent behaviours, and a dangerous habit of messing
+with registers without telling the compiler.
+
+This patches applies the following remedies:
+- Fix the comments to describe the actual behaviour
+- Tidy up the asm sequence to directly assign the
+  stack pointer without clobbering extra registers
+- Mark the rest of the function as unreachable() so
+  that the compiler knows that there is no need for
+  an epilogue
+- Stop making jprobe_return_break a global function
+  (you really don't want to call that guy, and it isn't
+  even a function).
+
+Tested with tcp_probe.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+---
+ arch/arm64/kernel/probes/kprobes.c | 22 ++++++++++------------
+ 1 file changed, 10 insertions(+), 12 deletions(-)
+
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index b118974..2354554 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -34,8 +34,6 @@
+ 
+ #include "decode-insn.h"
+ 
+-void jprobe_return_break(void);
+-
+ DEFINE_PER_CPU(struct kprobe *, current_kprobe) = NULL;
+ DEFINE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
+ 
+@@ -513,18 +511,17 @@ void __kprobes jprobe_return(void)
+ 	/*
+ 	 * Jprobe handler return by entering break exception,
+ 	 * encoded same as kprobe, but with following conditions
+-	 * -a magic number in x0 to identify from rest of other kprobes.
++	 * -a special PC to identify it from the other kprobes.
+ 	 * -restore stack addr to original saved pt_regs
+ 	 */
+-	asm volatile ("ldr x0, [%0]\n\t"
+-		      "mov sp, x0\n\t"
+-		      ".globl jprobe_return_break\n\t"
+-		      "jprobe_return_break:\n\t"
+-		      "brk %1\n\t"
+-		      :
+-		      : "r"(&kcb->jprobe_saved_regs.sp),
+-		      "I"(BRK64_ESR_KPROBES)
+-		      : "memory");
++	asm volatile("				mov sp, %0	\n"
++		     "jprobe_return_break:	brk %1		\n"
++		     :
++		     : "r" (kcb->jprobe_saved_regs.sp),
++		       "I" (BRK64_ESR_KPROBES)
++		     : "memory");
++
++	unreachable();
+ }
+ 
+ int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
+@@ -533,6 +530,7 @@ int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
+ 	long stack_addr = kcb->jprobe_saved_regs.sp;
+ 	long orig_sp = kernel_stack_pointer(regs);
+ 	struct jprobe *jp = container_of(p, struct jprobe, kp);
++	extern const char jprobe_return_break[];
+ 
+ 	if (instruction_pointer(regs) != (u64) jprobe_return_break)
+ 		return 0;
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0014-arm64-kprobes-Add-KASAN-instrumentation-around-stack.patch b/tools/kdump/0014-arm64-kprobes-Add-KASAN-instrumentation-around-stack.patch
new file mode 100644
index 0000000..f2dd4f7
--- /dev/null
+++ b/tools/kdump/0014-arm64-kprobes-Add-KASAN-instrumentation-around-stack.patch
@@ -0,0 +1,74 @@
+From 66bd115d97975967a9006eccd41b5eb5fcb8951b Mon Sep 17 00:00:00 2001
+From: Catalin Marinas <catalin.marinas@arm.com>
+Date: Thu, 21 Jul 2016 10:54:54 +0100
+Subject: [PATCH 014/123] arm64: kprobes: Add KASAN instrumentation around
+ stack accesses
+
+commit f7e35c5ba4322838ce84b23a2f1a6d6b7f0b57ec upstream.
+
+This patch disables KASAN around the memcpy from/to the kernel or IRQ
+stacks to avoid warnings like below:
+
+BUG: KASAN: stack-out-of-bounds in setjmp_pre_handler+0xe4/0x170 at addr ffff800935cbbbc0
+Read of size 128 by task swapper/0/1
+page:ffff7e0024d72ec0 count:0 mapcount:0 mapping:          (null) index:0x0
+flags: 0x1000000000000000()
+page dumped because: kasan: bad access detected
+CPU: 4 PID: 1 Comm: swapper/0 Not tainted 4.7.0-rc4+ #1
+Hardware name: ARM Juno development board (r0) (DT)
+Call trace:
+[<ffff20000808ad88>] dump_backtrace+0x0/0x280
+[<ffff20000808b01c>] show_stack+0x14/0x20
+[<ffff200008563a64>] dump_stack+0xa4/0xc8
+[<ffff20000824a1fc>] kasan_report_error+0x4fc/0x528
+[<ffff20000824a5e8>] kasan_report+0x40/0x48
+[<ffff20000824948c>] check_memory_region+0x144/0x1a0
+[<ffff200008249814>] memcpy+0x34/0x68
+[<ffff200008c3ee2c>] setjmp_pre_handler+0xe4/0x170
+[<ffff200008c3ec5c>] kprobe_breakpoint_handler+0xec/0x1d8
+[<ffff2000080853a4>] brk_handler+0x5c/0xa0
+[<ffff2000080813f0>] do_debug_exception+0xa0/0x138
+
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: David A. Long <dave.long@linaro.org>
+---
+ arch/arm64/kernel/probes/kprobes.c | 5 +++++
+ 1 file changed, 5 insertions(+)
+
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index 2354554..6487b62 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -16,6 +16,7 @@
+  * General Public License for more details.
+  *
+  */
++#include <linux/kasan.h>
+ #include <linux/kernel.h>
+ #include <linux/kprobes.h>
+ #include <linux/module.h>
+@@ -495,8 +496,10 @@ int __kprobes setjmp_pre_handler(struct kprobe *p, struct pt_regs *regs)
+ 	 * we also save and restore enough stack bytes to cover
+ 	 * the argument area.
+ 	 */
++	kasan_disable_current();
+ 	memcpy(kcb->jprobes_stack, (void *)stack_ptr,
+ 	       min_stack_size(stack_ptr));
++	kasan_enable_current();
+ 
+ 	instruction_pointer_set(regs, (unsigned long) jp->entry);
+ 	preempt_disable();
+@@ -548,8 +551,10 @@ int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
+ 	}
+ 	unpause_graph_tracing();
+ 	*regs = kcb->jprobe_saved_regs;
++	kasan_disable_current();
+ 	memcpy((void *)stack_addr, kcb->jprobes_stack,
+ 	       min_stack_size(stack_addr));
++	kasan_enable_current();
+ 	preempt_enable_no_resched();
+ 	return 1;
+ }
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0015-arm64-Remove-stack-duplicating-code-from-jprobes.patch b/tools/kdump/0015-arm64-Remove-stack-duplicating-code-from-jprobes.patch
new file mode 100644
index 0000000..3fd4cc0
--- /dev/null
+++ b/tools/kdump/0015-arm64-Remove-stack-duplicating-code-from-jprobes.patch
@@ -0,0 +1,104 @@
+From ea920fcbe7d09b39f6e0ed80f8c2d5a7b89ec734 Mon Sep 17 00:00:00 2001
+From: "David A. Long" <dave.long@linaro.org>
+Date: Wed, 10 Aug 2016 16:44:51 -0400
+Subject: [PATCH 015/123] arm64: Remove stack duplicating code from jprobes
+
+commit ad05711cec12131e1277ce749a99d08ecf233aa7 upstream.
+
+Because the arm64 calling standard allows stacked function arguments to be
+anywhere in the stack frame, do not attempt to duplicate the stack frame for
+jprobes handler functions.
+
+Documentation changes to describe this issue have been broken out into a
+separate patch in order to simultaneously address them in other
+architecture(s).
+
+Signed-off-by: David A. Long <dave.long@linaro.org>
+Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+---
+ arch/arm64/include/asm/kprobes.h   |  2 --
+ arch/arm64/kernel/probes/kprobes.c | 28 +++++-----------------------
+ 2 files changed, 5 insertions(+), 25 deletions(-)
+
+diff --git a/arch/arm64/include/asm/kprobes.h b/arch/arm64/include/asm/kprobes.h
+index 61b4915..1737aec 100644
+--- a/arch/arm64/include/asm/kprobes.h
++++ b/arch/arm64/include/asm/kprobes.h
+@@ -22,7 +22,6 @@
+ 
+ #define __ARCH_WANT_KPROBES_INSN_SLOT
+ #define MAX_INSN_SIZE			1
+-#define MAX_STACK_SIZE			128
+ 
+ #define flush_insn_slot(p)		do { } while (0)
+ #define kretprobe_blacklist_size	0
+@@ -47,7 +46,6 @@ struct kprobe_ctlblk {
+ 	struct prev_kprobe prev_kprobe;
+ 	struct kprobe_step_ctx ss_ctx;
+ 	struct pt_regs jprobe_saved_regs;
+-	char jprobes_stack[MAX_STACK_SIZE];
+ };
+ 
+ void arch_remove_kprobe(struct kprobe *);
+diff --git a/arch/arm64/kernel/probes/kprobes.c b/arch/arm64/kernel/probes/kprobes.c
+index 6487b62..1ee93c7 100644
+--- a/arch/arm64/kernel/probes/kprobes.c
++++ b/arch/arm64/kernel/probes/kprobes.c
+@@ -41,15 +41,6 @@ DEFINE_PER_CPU(struct kprobe_ctlblk, kprobe_ctlblk);
+ static void __kprobes
+ post_kprobe_handler(struct kprobe_ctlblk *, struct pt_regs *);
+ 
+-static inline unsigned long min_stack_size(unsigned long addr)
+-{
+-	unsigned long size;
+-
+-	size = (unsigned long)current_thread_info() + THREAD_START_SP - addr;
+-
+-	return min(size, FIELD_SIZEOF(struct kprobe_ctlblk, jprobes_stack));
+-}
+-
+ static void __kprobes arch_prepare_ss_slot(struct kprobe *p)
+ {
+ 	/* prepare insn slot */
+@@ -486,20 +477,15 @@ int __kprobes setjmp_pre_handler(struct kprobe *p, struct pt_regs *regs)
+ {
+ 	struct jprobe *jp = container_of(p, struct jprobe, kp);
+ 	struct kprobe_ctlblk *kcb = get_kprobe_ctlblk();
+-	long stack_ptr = kernel_stack_pointer(regs);
+ 
+ 	kcb->jprobe_saved_regs = *regs;
+ 	/*
+-	 * As Linus pointed out, gcc assumes that the callee
+-	 * owns the argument space and could overwrite it, e.g.
+-	 * tailcall optimization. So, to be absolutely safe
+-	 * we also save and restore enough stack bytes to cover
+-	 * the argument area.
++	 * Since we can't be sure where in the stack frame "stacked"
++	 * pass-by-value arguments are stored we just don't try to
++	 * duplicate any of the stack. Do not use jprobes on functions that
++	 * use more than 64 bytes (after padding each to an 8 byte boundary)
++	 * of arguments, or pass individual arguments larger than 16 bytes.
+ 	 */
+-	kasan_disable_current();
+-	memcpy(kcb->jprobes_stack, (void *)stack_ptr,
+-	       min_stack_size(stack_ptr));
+-	kasan_enable_current();
+ 
+ 	instruction_pointer_set(regs, (unsigned long) jp->entry);
+ 	preempt_disable();
+@@ -551,10 +537,6 @@ int __kprobes longjmp_break_handler(struct kprobe *p, struct pt_regs *regs)
+ 	}
+ 	unpause_graph_tracing();
+ 	*regs = kcb->jprobe_saved_regs;
+-	kasan_disable_current();
+-	memcpy((void *)stack_addr, kcb->jprobes_stack,
+-	       min_stack_size(stack_addr));
+-	kasan_enable_current();
+ 	preempt_enable_no_resched();
+ 	return 1;
+ }
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0016-ARM-8458-1-bL_switcher-add-GIC-dependency.patch b/tools/kdump/0016-ARM-8458-1-bL_switcher-add-GIC-dependency.patch
new file mode 100644
index 0000000..e7ae104
--- /dev/null
+++ b/tools/kdump/0016-ARM-8458-1-bL_switcher-add-GIC-dependency.patch
@@ -0,0 +1,48 @@
+From ee504c1c0fa5e539684bc644c3025df51c52ce11 Mon Sep 17 00:00:00 2001
+From: Arnd Bergmann <arnd@arndb.de>
+Date: Thu, 19 Nov 2015 15:49:23 +0100
+Subject: [PATCH 016/123] ARM: 8458/1: bL_switcher: add GIC dependency
+
+It is not possible to build the bL_switcher code if the GIC
+driver is disabled, because it relies on calling into some
+gic specific interfaces, and that would result in this build
+error:
+
+arch/arm/common/built-in.o: In function `bL_switch_to':
+:(.text+0x1230): undefined reference to `gic_get_sgir_physaddr'
+:(.text+0x1244): undefined reference to `gic_send_sgi'
+:(.text+0x1268): undefined reference to `gic_migrate_target'
+arch/arm/common/built-in.o: In function `bL_switcher_enable.part.4':
+:(.text.unlikely+0x2f8): undefined reference to `gic_get_cpu_id'
+
+This adds a Kconfig dependency to ensure we only build the big-little
+switcher if the GIC driver is present as well.
+
+Almost all ARMv7 platforms come with a GIC anyway, but it is possible
+to build a kernel that disables all platforms.
+
+Signed-off-by: Arnd Bergmann <arnd@arndb.de>
+Acked-by: Nicolas Pitre <nico@linaro.org>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit 6c044fecdf78be3fda159a5036bb33700cdd5e59)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/Kconfig | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
+index 34e1569..ceee970 100644
+--- a/arch/arm/Kconfig
++++ b/arch/arm/Kconfig
+@@ -1422,7 +1422,7 @@ config BIG_LITTLE
+ 
+ config BL_SWITCHER
+ 	bool "big.LITTLE switcher support"
+-	depends on BIG_LITTLE && MCPM && HOTPLUG_CPU
++	depends on BIG_LITTLE && MCPM && HOTPLUG_CPU && ARM_GIC
+ 	select ARM_CPU_SUSPEND
+ 	select CPU_PM
+ 	help
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0017-ARM-8510-1-rework-ARM_CPU_SUSPEND-dependencies.patch b/tools/kdump/0017-ARM-8510-1-rework-ARM_CPU_SUSPEND-dependencies.patch
new file mode 100644
index 0000000..ec348d0
--- /dev/null
+++ b/tools/kdump/0017-ARM-8510-1-rework-ARM_CPU_SUSPEND-dependencies.patch
@@ -0,0 +1,51 @@
+From e662feff3c13d70a5ce737b3c0738b0815d86bea Mon Sep 17 00:00:00 2001
+From: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Date: Mon, 1 Feb 2016 18:01:29 +0100
+Subject: [PATCH 017/123] ARM: 8510/1: rework ARM_CPU_SUSPEND dependencies
+
+The code enabled by the ARM_CPU_SUSPEND config option is used by
+kernel subsystems for purposes that go beyond system suspend so its
+config entry should be augmented to take more default options into
+account and avoid forcing its selection to prevent dependencies
+override.
+
+To achieve this goal, this patch reworks the ARM_CPU_SUSPEND config
+entry and updates its default config value (by adding the BL_SWITCHER
+option to it) and its dependencies (ARCH_SUSPEND_POSSIBLE), so that the
+symbol is still selected by default by the subsystems requiring it and
+at the same time enforcing the dependencies correctly.
+
+Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Cc: Nicolas Pitre <nico@fluxnic.net>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit 1b9bdf5c1661873a10e193b8cbb803a87fe5c4a1)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/Kconfig | 4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
+index ceee970..20ae5d2 100644
+--- a/arch/arm/Kconfig
++++ b/arch/arm/Kconfig
+@@ -1423,7 +1423,6 @@ config BIG_LITTLE
+ config BL_SWITCHER
+ 	bool "big.LITTLE switcher support"
+ 	depends on BIG_LITTLE && MCPM && HOTPLUG_CPU && ARM_GIC
+-	select ARM_CPU_SUSPEND
+ 	select CPU_PM
+ 	help
+ 	  The big.LITTLE "switcher" provides the core functionality to
+@@ -2140,7 +2139,8 @@ config ARCH_SUSPEND_POSSIBLE
+ 	def_bool y
+ 
+ config ARM_CPU_SUSPEND
+-	def_bool PM_SLEEP
++	def_bool PM_SLEEP || BL_SWITCHER
++	depends on ARCH_SUSPEND_POSSIBLE
+ 
+ config ARCH_HIBERNATION_POSSIBLE
+ 	bool
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0018-ARM-8478-2-arm-arm64-add-arm-smccc.patch b/tools/kdump/0018-ARM-8478-2-arm-arm64-add-arm-smccc.patch
new file mode 100644
index 0000000..6b0713e
--- /dev/null
+++ b/tools/kdump/0018-ARM-8478-2-arm-arm64-add-arm-smccc.patch
@@ -0,0 +1,152 @@
+From 92ac47a0b21461701f60f866c9fad14eb02f1a2b Mon Sep 17 00:00:00 2001
+From: Jens Wiklander <jens.wiklander@linaro.org>
+Date: Mon, 4 Jan 2016 15:37:32 +0100
+Subject: [PATCH 018/123] ARM: 8478/2: arm/arm64: add arm-smccc
+
+Adds helpers to do SMC and HVC based on ARM SMC Calling Convention.
+CONFIG_HAVE_ARM_SMCCC is enabled for architectures that may support the
+SMC or HVC instruction. It's the responsibility of the caller to know if
+the SMC instruction is supported by the platform.
+
+This patch doesn't provide an implementation of the declared functions.
+Later patches will bring in implementations and set
+CONFIG_HAVE_ARM_SMCCC for ARM and ARM64 respectively.
+
+Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Signed-off-by: Jens Wiklander <jens.wiklander@linaro.org>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit 98dd64f34f47ce19b388d9015f767f48393a81eb)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ drivers/firmware/Kconfig  |   3 ++
+ include/linux/arm-smccc.h | 104 ++++++++++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 107 insertions(+)
+ create mode 100644 include/linux/arm-smccc.h
+
+diff --git a/drivers/firmware/Kconfig b/drivers/firmware/Kconfig
+index cf478fe..49a3a11 100644
+--- a/drivers/firmware/Kconfig
++++ b/drivers/firmware/Kconfig
+@@ -173,6 +173,9 @@ config QCOM_SCM_64
+ 	def_bool y
+ 	depends on QCOM_SCM && ARM64
+ 
++config HAVE_ARM_SMCCC
++	bool
++
+ source "drivers/firmware/broadcom/Kconfig"
+ source "drivers/firmware/google/Kconfig"
+ source "drivers/firmware/efi/Kconfig"
+diff --git a/include/linux/arm-smccc.h b/include/linux/arm-smccc.h
+new file mode 100644
+index 0000000..b5abfda
+--- /dev/null
++++ b/include/linux/arm-smccc.h
+@@ -0,0 +1,104 @@
++/*
++ * Copyright (c) 2015, Linaro Limited
++ *
++ * This software is licensed under the terms of the GNU General Public
++ * License version 2, as published by the Free Software Foundation, and
++ * may be copied, distributed, and modified under those terms.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ */
++#ifndef __LINUX_ARM_SMCCC_H
++#define __LINUX_ARM_SMCCC_H
++
++#include <linux/linkage.h>
++#include <linux/types.h>
++
++/*
++ * This file provides common defines for ARM SMC Calling Convention as
++ * specified in
++ * http://infocenter.arm.com/help/topic/com.arm.doc.den0028a/index.html
++ */
++
++#define ARM_SMCCC_STD_CALL		0
++#define ARM_SMCCC_FAST_CALL		1
++#define ARM_SMCCC_TYPE_SHIFT		31
++
++#define ARM_SMCCC_SMC_32		0
++#define ARM_SMCCC_SMC_64		1
++#define ARM_SMCCC_CALL_CONV_SHIFT	30
++
++#define ARM_SMCCC_OWNER_MASK		0x3F
++#define ARM_SMCCC_OWNER_SHIFT		24
++
++#define ARM_SMCCC_FUNC_MASK		0xFFFF
++
++#define ARM_SMCCC_IS_FAST_CALL(smc_val)	\
++	((smc_val) & (ARM_SMCCC_FAST_CALL << ARM_SMCCC_TYPE_SHIFT))
++#define ARM_SMCCC_IS_64(smc_val) \
++	((smc_val) & (ARM_SMCCC_SMC_64 << ARM_SMCCC_CALL_CONV_SHIFT))
++#define ARM_SMCCC_FUNC_NUM(smc_val)	((smc_val) & ARM_SMCCC_FUNC_MASK)
++#define ARM_SMCCC_OWNER_NUM(smc_val) \
++	(((smc_val) >> ARM_SMCCC_OWNER_SHIFT) & ARM_SMCCC_OWNER_MASK)
++
++#define ARM_SMCCC_CALL_VAL(type, calling_convention, owner, func_num) \
++	(((type) << ARM_SMCCC_TYPE_SHIFT) | \
++	((calling_convention) << ARM_SMCCC_CALL_CONV_SHIFT) | \
++	(((owner) & ARM_SMCCC_OWNER_MASK) << ARM_SMCCC_OWNER_SHIFT) | \
++	((func_num) & ARM_SMCCC_FUNC_MASK))
++
++#define ARM_SMCCC_OWNER_ARCH		0
++#define ARM_SMCCC_OWNER_CPU		1
++#define ARM_SMCCC_OWNER_SIP		2
++#define ARM_SMCCC_OWNER_OEM		3
++#define ARM_SMCCC_OWNER_STANDARD	4
++#define ARM_SMCCC_OWNER_TRUSTED_APP	48
++#define ARM_SMCCC_OWNER_TRUSTED_APP_END	49
++#define ARM_SMCCC_OWNER_TRUSTED_OS	50
++#define ARM_SMCCC_OWNER_TRUSTED_OS_END	63
++
++/**
++ * struct arm_smccc_res - Result from SMC/HVC call
++ * @a0-a3 result values from registers 0 to 3
++ */
++struct arm_smccc_res {
++	unsigned long a0;
++	unsigned long a1;
++	unsigned long a2;
++	unsigned long a3;
++};
++
++/**
++ * arm_smccc_smc() - make SMC calls
++ * @a0-a7: arguments passed in registers 0 to 7
++ * @res: result values from registers 0 to 3
++ *
++ * This function is used to make SMC calls following SMC Calling Convention.
++ * The content of the supplied param are copied to registers 0 to 7 prior
++ * to the SMC instruction. The return values are updated with the content
++ * from register 0 to 3 on return from the SMC instruction.
++ */
++asmlinkage void arm_smccc_smc(unsigned long a0, unsigned long a1,
++			unsigned long a2, unsigned long a3, unsigned long a4,
++			unsigned long a5, unsigned long a6, unsigned long a7,
++			struct arm_smccc_res *res);
++
++/**
++ * arm_smccc_hvc() - make HVC calls
++ * @a0-a7: arguments passed in registers 0 to 7
++ * @res: result values from registers 0 to 3
++ *
++ * This function is used to make HVC calls following SMC Calling
++ * Convention.  The content of the supplied param are copied to registers 0
++ * to 7 prior to the HVC instruction. The return values are updated with
++ * the content from register 0 to 3 on return from the HVC instruction.
++ */
++asmlinkage void arm_smccc_hvc(unsigned long a0, unsigned long a1,
++			unsigned long a2, unsigned long a3, unsigned long a4,
++			unsigned long a5, unsigned long a6, unsigned long a7,
++			struct arm_smccc_res *res);
++
++#endif /*__LINUX_ARM_SMCCC_H*/
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0019-ARM-8479-2-add-implementation-for-arm-smccc.patch b/tools/kdump/0019-ARM-8479-2-add-implementation-for-arm-smccc.patch
new file mode 100644
index 0000000..26a75ba
--- /dev/null
+++ b/tools/kdump/0019-ARM-8479-2-add-implementation-for-arm-smccc.patch
@@ -0,0 +1,137 @@
+From e0f91e51b75221eae4421a05cbf4d69f3ca504ce Mon Sep 17 00:00:00 2001
+From: Jens Wiklander <jens.wiklander@linaro.org>
+Date: Mon, 4 Jan 2016 15:42:55 +0100
+Subject: [PATCH 019/123] ARM: 8479/2: add implementation for arm-smccc
+
+Adds implementation for arm-smccc and enables CONFIG_HAVE_SMCCC for
+architectures that may support arm-smccc. It's the responsibility of the
+caller to know if the SMC instruction is supported by the platform.
+
+Reviewed-by: Lars Persson <lars.persson@axis.com>
+Signed-off-by: Jens Wiklander <jens.wiklander@linaro.org>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit b329f95d70f3f955093e9a2b18ac1ed3587a8f73)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/Kconfig             |  1 +
+ arch/arm/kernel/Makefile     |  2 ++
+ arch/arm/kernel/armksyms.c   |  6 +++++
+ arch/arm/kernel/smccc-call.S | 62 ++++++++++++++++++++++++++++++++++++++++++++
+ 4 files changed, 71 insertions(+)
+ create mode 100644 arch/arm/kernel/smccc-call.S
+
+diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
+index 20ae5d2..61bec5e 100644
+--- a/arch/arm/Kconfig
++++ b/arch/arm/Kconfig
+@@ -37,6 +37,7 @@ config ARM
+ 	select HAVE_ARCH_KGDB if !CPU_ENDIAN_BE32
+ 	select HAVE_ARCH_SECCOMP_FILTER if (AEABI && !OABI_COMPAT)
+ 	select HAVE_ARCH_TRACEHOOK
++	select HAVE_ARM_SMCCC if CPU_V7
+ 	select HAVE_BPF_JIT
+ 	select HAVE_CC_STACKPROTECTOR
+ 	select HAVE_CONTEXT_TRACKING
+diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
+index af9e59b..d2d0042 100644
+--- a/arch/arm/kernel/Makefile
++++ b/arch/arm/kernel/Makefile
+@@ -92,4 +92,6 @@ obj-y				+= psci-call.o
+ obj-$(CONFIG_SMP)		+= psci_smp.o
+ endif
+ 
++obj-$(CONFIG_HAVE_ARM_SMCCC)	+= smccc-call.o
++
+ extra-y := $(head-y) vmlinux.lds
+diff --git a/arch/arm/kernel/armksyms.c b/arch/arm/kernel/armksyms.c
+index f89811f..7e45f69 100644
+--- a/arch/arm/kernel/armksyms.c
++++ b/arch/arm/kernel/armksyms.c
+@@ -16,6 +16,7 @@
+ #include <linux/syscalls.h>
+ #include <linux/uaccess.h>
+ #include <linux/io.h>
++#include <linux/arm-smccc.h>
+ 
+ #include <asm/checksum.h>
+ #include <asm/ftrace.h>
+@@ -175,3 +176,8 @@ EXPORT_SYMBOL(__gnu_mcount_nc);
+ EXPORT_SYMBOL(__pv_phys_pfn_offset);
+ EXPORT_SYMBOL(__pv_offset);
+ #endif
++
++#ifdef CONFIG_HAVE_ARM_SMCCC
++EXPORT_SYMBOL(arm_smccc_smc);
++EXPORT_SYMBOL(arm_smccc_hvc);
++#endif
+diff --git a/arch/arm/kernel/smccc-call.S b/arch/arm/kernel/smccc-call.S
+new file mode 100644
+index 0000000..2e48b67
+--- /dev/null
++++ b/arch/arm/kernel/smccc-call.S
+@@ -0,0 +1,62 @@
++/*
++ * Copyright (c) 2015, Linaro Limited
++ *
++ * This software is licensed under the terms of the GNU General Public
++ * License version 2, as published by the Free Software Foundation, and
++ * may be copied, distributed, and modified under those terms.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ */
++#include <linux/linkage.h>
++
++#include <asm/opcodes-sec.h>
++#include <asm/opcodes-virt.h>
++#include <asm/unwind.h>
++
++	/*
++	 * Wrap c macros in asm macros to delay expansion until after the
++	 * SMCCC asm macro is expanded.
++	 */
++	.macro SMCCC_SMC
++	__SMC(0)
++	.endm
++
++	.macro SMCCC_HVC
++	__HVC(0)
++	.endm
++
++	.macro SMCCC instr
++UNWIND(	.fnstart)
++	mov	r12, sp
++	push	{r4-r7}
++UNWIND(	.save	{r4-r7})
++	ldm	r12, {r4-r7}
++	\instr
++	pop	{r4-r7}
++	ldr	r12, [sp, #(4 * 4)]
++	stm	r12, {r0-r3}
++	bx	lr
++UNWIND(	.fnend)
++	.endm
++
++/*
++ * void smccc_smc(unsigned long a0, unsigned long a1, unsigned long a2,
++ *		  unsigned long a3, unsigned long a4, unsigned long a5,
++ *		  unsigned long a6, unsigned long a7, struct arm_smccc_res *res)
++ */
++ENTRY(arm_smccc_smc)
++	SMCCC SMCCC_SMC
++ENDPROC(arm_smccc_smc)
++
++/*
++ * void smccc_hvc(unsigned long a0, unsigned long a1, unsigned long a2,
++ *		  unsigned long a3, unsigned long a4, unsigned long a5,
++ *		  unsigned long a6, unsigned long a7, struct arm_smccc_res *res)
++ */
++ENTRY(arm_smccc_hvc)
++	SMCCC SMCCC_HVC
++ENDPROC(arm_smccc_hvc)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0020-ARM-8480-2-arm64-add-implementation-for-arm-smccc.patch b/tools/kdump/0020-ARM-8480-2-arm64-add-implementation-for-arm-smccc.patch
new file mode 100644
index 0000000..fb50f69
--- /dev/null
+++ b/tools/kdump/0020-ARM-8480-2-arm64-add-implementation-for-arm-smccc.patch
@@ -0,0 +1,141 @@
+From 2e8ab1af32c8644ce07df46a1dbefc83f6d61b56 Mon Sep 17 00:00:00 2001
+From: Jens Wiklander <jens.wiklander@linaro.org>
+Date: Mon, 4 Jan 2016 15:44:32 +0100
+Subject: [PATCH 020/123] ARM: 8480/2: arm64: add implementation for arm-smccc
+
+Adds implementation for arm-smccc and enables CONFIG_HAVE_SMCCC.
+
+Acked-by: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Jens Wiklander <jens.wiklander@linaro.org>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit 14457459f9ca2ff8521686168ea179edc3a56a44)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kernel/arm64ksyms.c
+---
+ arch/arm64/Kconfig              |  1 +
+ arch/arm64/kernel/Makefile      |  2 +-
+ arch/arm64/kernel/arm64ksyms.c  |  5 +++++
+ arch/arm64/kernel/asm-offsets.c |  3 +++
+ arch/arm64/kernel/smccc-call.S  | 43 +++++++++++++++++++++++++++++++++++++++++
+ 5 files changed, 53 insertions(+), 1 deletion(-)
+ create mode 100644 arch/arm64/kernel/smccc-call.S
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index 7a2b7a2..41d2637 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -95,6 +95,7 @@ config ARM64
+ 	select SPARSE_IRQ
+ 	select SYSCTL_EXCEPTION_TRACE
+ 	select HAVE_CONTEXT_TRACKING
++	select HAVE_ARM_SMCCC
+ 	help
+ 	  ARM 64-bit (AArch64) Linux support.
+ 
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index cfa290b..0a987eb 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -17,7 +17,7 @@ arm64-obj-y		:= debug-monitors.o entry.o irq.o fpsimd.o		\
+ 			   hyp-stub.o psci.o psci-call.o cpu_ops.o insn.o	\
+ 			   return_address.o cpuinfo.o cpu_errata.o		\
+ 			   cpufeature.o alternative.o cacheinfo.o		\
+-			   smp.o smp_spin_table.o topology.o
++			   smp.o smp_spin_table.o topology.o smccc-call.o
+ 
+ extra-$(CONFIG_EFI)			:= efi-entry.o
+ 
+diff --git a/arch/arm64/kernel/arm64ksyms.c b/arch/arm64/kernel/arm64ksyms.c
+index fbec403..fce2b116 100644
+--- a/arch/arm64/kernel/arm64ksyms.c
++++ b/arch/arm64/kernel/arm64ksyms.c
+@@ -27,6 +27,7 @@
+ #include <linux/uaccess.h>
+ #include <linux/io.h>
+ #include <linux/kprobes.h>
++#include <linux/arm-smccc.h>
+ 
+ #include <asm/checksum.h>
+ 
+@@ -70,3 +71,7 @@ EXPORT_SYMBOL(test_and_change_bit);
+ EXPORT_SYMBOL(_mcount);
+ NOKPROBE_SYMBOL(_mcount);
+ #endif
++
++	/* arm-smccc */
++EXPORT_SYMBOL(arm_smccc_smc);
++EXPORT_SYMBOL(arm_smccc_hvc);
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index dd292519..2a948bf 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -28,6 +28,7 @@
+ #include <asm/suspend.h>
+ #include <asm/vdso_datapage.h>
+ #include <linux/kbuild.h>
++#include <linux/arm-smccc.h>
+ 
+ int main(void)
+ {
+@@ -173,5 +174,7 @@ int main(void)
+   DEFINE(SLEEP_SAVE_SP_PHYS,	offsetof(struct sleep_save_sp, save_ptr_stash_phys));
+   DEFINE(SLEEP_SAVE_SP_VIRT,	offsetof(struct sleep_save_sp, save_ptr_stash));
+ #endif
++  DEFINE(ARM_SMCCC_RES_X0_OFFS,	offsetof(struct arm_smccc_res, a0));
++  DEFINE(ARM_SMCCC_RES_X2_OFFS,	offsetof(struct arm_smccc_res, a2));
+   return 0;
+ }
+diff --git a/arch/arm64/kernel/smccc-call.S b/arch/arm64/kernel/smccc-call.S
+new file mode 100644
+index 0000000..ae0496f
+--- /dev/null
++++ b/arch/arm64/kernel/smccc-call.S
+@@ -0,0 +1,43 @@
++/*
++ * Copyright (c) 2015, Linaro Limited
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License Version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
++ * GNU General Public License for more details.
++ *
++ */
++#include <linux/linkage.h>
++#include <asm/asm-offsets.h>
++
++	.macro SMCCC instr
++	.cfi_startproc
++	\instr	#0
++	ldr	x4, [sp]
++	stp	x0, x1, [x4, #ARM_SMCCC_RES_X0_OFFS]
++	stp	x2, x3, [x4, #ARM_SMCCC_RES_X2_OFFS]
++	ret
++	.cfi_endproc
++	.endm
++
++/*
++ * void arm_smccc_smc(unsigned long a0, unsigned long a1, unsigned long a2,
++ *		  unsigned long a3, unsigned long a4, unsigned long a5,
++ *		  unsigned long a6, unsigned long a7, struct arm_smccc_res *res)
++ */
++ENTRY(arm_smccc_smc)
++	SMCCC	smc
++ENDPROC(arm_smccc_smc)
++
++/*
++ * void arm_smccc_hvc(unsigned long a0, unsigned long a1, unsigned long a2,
++ *		  unsigned long a3, unsigned long a4, unsigned long a5,
++ *		  unsigned long a6, unsigned long a7, struct arm_smccc_res *res)
++ */
++ENTRY(arm_smccc_hvc)
++	SMCCC	hvc
++ENDPROC(arm_smccc_hvc)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0021-ARM-8481-2-drivers-psci-replace-psci-firmware-calls.patch b/tools/kdump/0021-ARM-8481-2-drivers-psci-replace-psci-firmware-calls.patch
new file mode 100644
index 0000000..20e44ac
--- /dev/null
+++ b/tools/kdump/0021-ARM-8481-2-drivers-psci-replace-psci-firmware-calls.patch
@@ -0,0 +1,189 @@
+From 7d6ad9901af383f3f3492b25997d7e06669b0d80 Mon Sep 17 00:00:00 2001
+From: Jens Wiklander <jens.wiklander@linaro.org>
+Date: Mon, 4 Jan 2016 15:46:47 +0100
+Subject: [PATCH 021/123] ARM: 8481/2: drivers: psci: replace psci firmware
+ calls
+
+Switch to use a generic interface for issuing SMC/HVC based on ARM SMC
+Calling Convention. Removes now the now unused psci-call.S.
+
+Acked-by: Will Deacon <will.deacon@arm.com>
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Tested-by: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Tested-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Signed-off-by: Jens Wiklander <jens.wiklander@linaro.org>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit e679660dbb8347f275fe5d83a5dd59c1fb6c8e63)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/Kconfig              |  2 +-
+ arch/arm/kernel/Makefile      |  1 -
+ arch/arm/kernel/psci-call.S   | 31 -------------------------------
+ arch/arm64/kernel/Makefile    |  2 +-
+ arch/arm64/kernel/psci-call.S | 28 ----------------------------
+ drivers/firmware/psci.c       | 23 +++++++++++++++++++++--
+ 6 files changed, 23 insertions(+), 64 deletions(-)
+ delete mode 100644 arch/arm/kernel/psci-call.S
+ delete mode 100644 arch/arm64/kernel/psci-call.S
+
+diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
+index 61bec5e..2d5eebb 100644
+--- a/arch/arm/Kconfig
++++ b/arch/arm/Kconfig
+@@ -1481,7 +1481,7 @@ config HOTPLUG_CPU
+ 
+ config ARM_PSCI
+ 	bool "Support for the ARM Power State Coordination Interface (PSCI)"
+-	depends on CPU_V7
++	depends on HAVE_ARM_SMCCC
+ 	select ARM_PSCI_FW
+ 	help
+ 	  Say Y here if you want Linux to communicate with system firmware
+diff --git a/arch/arm/kernel/Makefile b/arch/arm/kernel/Makefile
+index d2d0042..80856de 100644
+--- a/arch/arm/kernel/Makefile
++++ b/arch/arm/kernel/Makefile
+@@ -88,7 +88,6 @@ obj-$(CONFIG_EARLY_PRINTK)	+= early_printk.o
+ 
+ obj-$(CONFIG_ARM_VIRT_EXT)	+= hyp-stub.o
+ ifeq ($(CONFIG_ARM_PSCI),y)
+-obj-y				+= psci-call.o
+ obj-$(CONFIG_SMP)		+= psci_smp.o
+ endif
+ 
+diff --git a/arch/arm/kernel/psci-call.S b/arch/arm/kernel/psci-call.S
+deleted file mode 100644
+index a78e9e1..0000000
+--- a/arch/arm/kernel/psci-call.S
++++ /dev/null
+@@ -1,31 +0,0 @@
+-/*
+- * This program is free software; you can redistribute it and/or modify
+- * it under the terms of the GNU General Public License version 2 as
+- * published by the Free Software Foundation.
+- *
+- * This program is distributed in the hope that it will be useful,
+- * but WITHOUT ANY WARRANTY; without even the implied warranty of
+- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- * GNU General Public License for more details.
+- *
+- * Copyright (C) 2015 ARM Limited
+- *
+- * Author: Mark Rutland <mark.rutland@arm.com>
+- */
+-
+-#include <linux/linkage.h>
+-
+-#include <asm/opcodes-sec.h>
+-#include <asm/opcodes-virt.h>
+-
+-/* int __invoke_psci_fn_hvc(u32 function_id, u32 arg0, u32 arg1, u32 arg2) */
+-ENTRY(__invoke_psci_fn_hvc)
+-	__HVC(0)
+-	bx	lr
+-ENDPROC(__invoke_psci_fn_hvc)
+-
+-/* int __invoke_psci_fn_smc(u32 function_id, u32 arg0, u32 arg1, u32 arg2) */
+-ENTRY(__invoke_psci_fn_smc)
+-	__SMC(0)
+-	bx	lr
+-ENDPROC(__invoke_psci_fn_smc)
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index 0a987eb..85f6924 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -14,7 +14,7 @@ CFLAGS_REMOVE_return_address.o = -pg
+ arm64-obj-y		:= debug-monitors.o entry.o irq.o fpsimd.o		\
+ 			   entry-fpsimd.o process.o ptrace.o setup.o signal.o	\
+ 			   sys.o stacktrace.o time.o traps.o io.o vdso.o	\
+-			   hyp-stub.o psci.o psci-call.o cpu_ops.o insn.o	\
++			   hyp-stub.o psci.o cpu_ops.o insn.o	\
+ 			   return_address.o cpuinfo.o cpu_errata.o		\
+ 			   cpufeature.o alternative.o cacheinfo.o		\
+ 			   smp.o smp_spin_table.o topology.o smccc-call.o
+diff --git a/arch/arm64/kernel/psci-call.S b/arch/arm64/kernel/psci-call.S
+deleted file mode 100644
+index cf83e61..0000000
+--- a/arch/arm64/kernel/psci-call.S
++++ /dev/null
+@@ -1,28 +0,0 @@
+-/*
+- * This program is free software; you can redistribute it and/or modify
+- * it under the terms of the GNU General Public License version 2 as
+- * published by the Free Software Foundation.
+- *
+- * This program is distributed in the hope that it will be useful,
+- * but WITHOUT ANY WARRANTY; without even the implied warranty of
+- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- * GNU General Public License for more details.
+- *
+- * Copyright (C) 2015 ARM Limited
+- *
+- * Author: Will Deacon <will.deacon@arm.com>
+- */
+-
+-#include <linux/linkage.h>
+-
+-/* int __invoke_psci_fn_hvc(u64 function_id, u64 arg0, u64 arg1, u64 arg2) */
+-ENTRY(__invoke_psci_fn_hvc)
+-	hvc	#0
+-	ret
+-ENDPROC(__invoke_psci_fn_hvc)
+-
+-/* int __invoke_psci_fn_smc(u64 function_id, u64 arg0, u64 arg1, u64 arg2) */
+-ENTRY(__invoke_psci_fn_smc)
+-	smc	#0
+-	ret
+-ENDPROC(__invoke_psci_fn_smc)
+diff --git a/drivers/firmware/psci.c b/drivers/firmware/psci.c
+index d24f35d..f25cd79 100644
+--- a/drivers/firmware/psci.c
++++ b/drivers/firmware/psci.c
+@@ -13,6 +13,7 @@
+ 
+ #define pr_fmt(fmt) "psci: " fmt
+ 
++#include <linux/arm-smccc.h>
+ #include <linux/errno.h>
+ #include <linux/linkage.h>
+ #include <linux/of.h>
+@@ -58,8 +59,6 @@ struct psci_operations psci_ops;
+ 
+ typedef unsigned long (psci_fn)(unsigned long, unsigned long,
+ 				unsigned long, unsigned long);
+-asmlinkage psci_fn __invoke_psci_fn_hvc;
+-asmlinkage psci_fn __invoke_psci_fn_smc;
+ static psci_fn *invoke_psci_fn;
+ 
+ enum psci_function {
+@@ -107,6 +106,26 @@ bool psci_power_state_is_valid(u32 state)
+ 	return !(state & ~valid_mask);
+ }
+ 
++static unsigned long __invoke_psci_fn_hvc(unsigned long function_id,
++			unsigned long arg0, unsigned long arg1,
++			unsigned long arg2)
++{
++	struct arm_smccc_res res;
++
++	arm_smccc_hvc(function_id, arg0, arg1, arg2, 0, 0, 0, 0, &res);
++	return res.a0;
++}
++
++static unsigned long __invoke_psci_fn_smc(unsigned long function_id,
++			unsigned long arg0, unsigned long arg1,
++			unsigned long arg2)
++{
++	struct arm_smccc_res res;
++
++	arm_smccc_smc(function_id, arg0, arg1, arg2, 0, 0, 0, 0, &res);
++	return res.a0;
++}
++
+ static int psci_to_linux_errno(int errno)
+ {
+ 	switch (errno) {
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0022-ARM-8511-1-ARM64-kernel-PSCI-move-PSCI-idle-manageme.patch b/tools/kdump/0022-ARM-8511-1-ARM64-kernel-PSCI-move-PSCI-idle-manageme.patch
new file mode 100644
index 0000000..f938f84
--- /dev/null
+++ b/tools/kdump/0022-ARM-8511-1-ARM64-kernel-PSCI-move-PSCI-idle-manageme.patch
@@ -0,0 +1,352 @@
+From 28a017c6a57db704a13e6c163115a51b1708deab Mon Sep 17 00:00:00 2001
+From: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Date: Mon, 1 Feb 2016 18:01:30 +0100
+Subject: [PATCH 022/123] ARM: 8511/1: ARM64: kernel: PSCI: move PSCI idle
+ management code to drivers/firmware
+
+ARM64 PSCI kernel interfaces that initialize idle states and implement
+the suspend API to enter them are generic and can be shared with the
+ARM architecture.
+
+To achieve that goal, this patch moves ARM64 PSCI idle management
+code to drivers/firmware, so that the interface to initialize and
+enter idle states can actually be shared by ARM and ARM64 arches
+back-ends.
+
+The ARM generic CPUidle implementation also requires the definition of
+a cpuidle_ops section entry for the kernel to initialize the CPUidle
+operations at boot based on the enable-method (ie ARM64 has the
+statically initialized cpu_ops counterparts for that purpose); therefore
+this patch also adds the required section entry on CONFIG_ARM for PSCI so
+that the kernel can initialize the PSCI CPUidle back-end when PSCI is
+the probed enable-method.
+
+On ARM64 this patch provides no functional change.
+
+Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com> [arch/arm64]
+Acked-by: Mark Rutland <mark.rutland@arm.com>
+Tested-by: Jisheng Zhang <jszhang@marvell.com>
+Cc: Will Deacon <will.deacon@arm.com>
+Cc: Sudeep Holla <sudeep.holla@arm.com>
+Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Mark Rutland <mark.rutland@arm.com>
+Cc: Jisheng Zhang <jszhang@marvell.com>
+Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>
+(cherry picked from commit 8b6f2499ac45d5a0ab2e4b6f9613ab3f60416be1)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/Kconfig         |   2 +-
+ arch/arm64/kernel/psci.c |  99 +-------------------------------------
+ drivers/firmware/psci.c  | 120 +++++++++++++++++++++++++++++++++++++++++++++++
+ include/linux/psci.h     |   3 ++
+ 4 files changed, 126 insertions(+), 98 deletions(-)
+
+diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
+index 2d5eebb..61d7eb3 100644
+--- a/arch/arm/Kconfig
++++ b/arch/arm/Kconfig
+@@ -2140,7 +2140,7 @@ config ARCH_SUSPEND_POSSIBLE
+ 	def_bool y
+ 
+ config ARM_CPU_SUSPEND
+-	def_bool PM_SLEEP || BL_SWITCHER
++	def_bool PM_SLEEP || BL_SWITCHER || ARM_PSCI_FW
+ 	depends on ARCH_SUSPEND_POSSIBLE
+ 
+ config ARCH_HIBERNATION_POSSIBLE
+diff --git a/arch/arm64/kernel/psci.c b/arch/arm64/kernel/psci.c
+index f67f35b..42816be 100644
+--- a/arch/arm64/kernel/psci.c
++++ b/arch/arm64/kernel/psci.c
+@@ -20,7 +20,6 @@
+ #include <linux/smp.h>
+ #include <linux/delay.h>
+ #include <linux/psci.h>
+-#include <linux/slab.h>
+ 
+ #include <uapi/linux/psci.h>
+ 
+@@ -28,73 +27,6 @@
+ #include <asm/cpu_ops.h>
+ #include <asm/errno.h>
+ #include <asm/smp_plat.h>
+-#include <asm/suspend.h>
+-
+-static DEFINE_PER_CPU_READ_MOSTLY(u32 *, psci_power_state);
+-
+-static int __maybe_unused cpu_psci_cpu_init_idle(unsigned int cpu)
+-{
+-	int i, ret, count = 0;
+-	u32 *psci_states;
+-	struct device_node *state_node, *cpu_node;
+-
+-	cpu_node = of_get_cpu_node(cpu, NULL);
+-	if (!cpu_node)
+-		return -ENODEV;
+-
+-	/*
+-	 * If the PSCI cpu_suspend function hook has not been initialized
+-	 * idle states must not be enabled, so bail out
+-	 */
+-	if (!psci_ops.cpu_suspend)
+-		return -EOPNOTSUPP;
+-
+-	/* Count idle states */
+-	while ((state_node = of_parse_phandle(cpu_node, "cpu-idle-states",
+-					      count))) {
+-		count++;
+-		of_node_put(state_node);
+-	}
+-
+-	if (!count)
+-		return -ENODEV;
+-
+-	psci_states = kcalloc(count, sizeof(*psci_states), GFP_KERNEL);
+-	if (!psci_states)
+-		return -ENOMEM;
+-
+-	for (i = 0; i < count; i++) {
+-		u32 state;
+-
+-		state_node = of_parse_phandle(cpu_node, "cpu-idle-states", i);
+-
+-		ret = of_property_read_u32(state_node,
+-					   "arm,psci-suspend-param",
+-					   &state);
+-		if (ret) {
+-			pr_warn(" * %s missing arm,psci-suspend-param property\n",
+-				state_node->full_name);
+-			of_node_put(state_node);
+-			goto free_mem;
+-		}
+-
+-		of_node_put(state_node);
+-		pr_debug("psci-power-state %#x index %d\n", state, i);
+-		if (!psci_power_state_is_valid(state)) {
+-			pr_warn("Invalid PSCI power state %#x\n", state);
+-			ret = -EINVAL;
+-			goto free_mem;
+-		}
+-		psci_states[i] = state;
+-	}
+-	/* Idle states parsed correctly, initialize per-cpu pointer */
+-	per_cpu(psci_power_state, cpu) = psci_states;
+-	return 0;
+-
+-free_mem:
+-	kfree(psci_states);
+-	return ret;
+-}
+ 
+ static int __init cpu_psci_cpu_init(unsigned int cpu)
+ {
+@@ -178,38 +110,11 @@ static int cpu_psci_cpu_kill(unsigned int cpu)
+ }
+ #endif
+ 
+-static int psci_suspend_finisher(unsigned long index)
+-{
+-	u32 *state = __this_cpu_read(psci_power_state);
+-
+-	return psci_ops.cpu_suspend(state[index - 1],
+-				    virt_to_phys(cpu_resume));
+-}
+-
+-static int __maybe_unused cpu_psci_cpu_suspend(unsigned long index)
+-{
+-	int ret;
+-	u32 *state = __this_cpu_read(psci_power_state);
+-	/*
+-	 * idle state index 0 corresponds to wfi, should never be called
+-	 * from the cpu_suspend operations
+-	 */
+-	if (WARN_ON_ONCE(!index))
+-		return -EINVAL;
+-
+-	if (!psci_power_state_loses_context(state[index - 1]))
+-		ret = psci_ops.cpu_suspend(state[index - 1], 0);
+-	else
+-		ret = cpu_suspend(index, psci_suspend_finisher);
+-
+-	return ret;
+-}
+-
+ const struct cpu_operations cpu_psci_ops = {
+ 	.name		= "psci",
+ #ifdef CONFIG_CPU_IDLE
+-	.cpu_init_idle	= cpu_psci_cpu_init_idle,
+-	.cpu_suspend	= cpu_psci_cpu_suspend,
++	.cpu_init_idle	= psci_cpu_init_idle,
++	.cpu_suspend	= psci_cpu_suspend_enter,
+ #endif
+ 	.cpu_init	= cpu_psci_cpu_init,
+ 	.cpu_prepare	= cpu_psci_cpu_prepare,
+diff --git a/drivers/firmware/psci.c b/drivers/firmware/psci.c
+index f25cd79..11bfee8 100644
+--- a/drivers/firmware/psci.c
++++ b/drivers/firmware/psci.c
+@@ -14,6 +14,7 @@
+ #define pr_fmt(fmt) "psci: " fmt
+ 
+ #include <linux/arm-smccc.h>
++#include <linux/cpuidle.h>
+ #include <linux/errno.h>
+ #include <linux/linkage.h>
+ #include <linux/of.h>
+@@ -21,10 +22,12 @@
+ #include <linux/printk.h>
+ #include <linux/psci.h>
+ #include <linux/reboot.h>
++#include <linux/slab.h>
+ #include <linux/suspend.h>
+ 
+ #include <uapi/linux/psci.h>
+ 
++#include <asm/cpuidle.h>
+ #include <asm/cputype.h>
+ #include <asm/system_misc.h>
+ #include <asm/smp_plat.h>
+@@ -244,6 +247,123 @@ static int __init psci_features(u32 psci_func_id)
+ 			      psci_func_id, 0, 0);
+ }
+ 
++#ifdef CONFIG_CPU_IDLE
++static DEFINE_PER_CPU_READ_MOSTLY(u32 *, psci_power_state);
++
++static int psci_dt_cpu_init_idle(struct device_node *cpu_node, int cpu)
++{
++	int i, ret, count = 0;
++	u32 *psci_states;
++	struct device_node *state_node;
++
++	/*
++	 * If the PSCI cpu_suspend function hook has not been initialized
++	 * idle states must not be enabled, so bail out
++	 */
++	if (!psci_ops.cpu_suspend)
++		return -EOPNOTSUPP;
++
++	/* Count idle states */
++	while ((state_node = of_parse_phandle(cpu_node, "cpu-idle-states",
++					      count))) {
++		count++;
++		of_node_put(state_node);
++	}
++
++	if (!count)
++		return -ENODEV;
++
++	psci_states = kcalloc(count, sizeof(*psci_states), GFP_KERNEL);
++	if (!psci_states)
++		return -ENOMEM;
++
++	for (i = 0; i < count; i++) {
++		u32 state;
++
++		state_node = of_parse_phandle(cpu_node, "cpu-idle-states", i);
++
++		ret = of_property_read_u32(state_node,
++					   "arm,psci-suspend-param",
++					   &state);
++		if (ret) {
++			pr_warn(" * %s missing arm,psci-suspend-param property\n",
++				state_node->full_name);
++			of_node_put(state_node);
++			goto free_mem;
++		}
++
++		of_node_put(state_node);
++		pr_debug("psci-power-state %#x index %d\n", state, i);
++		if (!psci_power_state_is_valid(state)) {
++			pr_warn("Invalid PSCI power state %#x\n", state);
++			ret = -EINVAL;
++			goto free_mem;
++		}
++		psci_states[i] = state;
++	}
++	/* Idle states parsed correctly, initialize per-cpu pointer */
++	per_cpu(psci_power_state, cpu) = psci_states;
++	return 0;
++
++free_mem:
++	kfree(psci_states);
++	return ret;
++}
++
++int psci_cpu_init_idle(unsigned int cpu)
++{
++	struct device_node *cpu_node;
++	int ret;
++
++	cpu_node = of_get_cpu_node(cpu, NULL);
++	if (!cpu_node)
++		return -ENODEV;
++
++	ret = psci_dt_cpu_init_idle(cpu_node, cpu);
++
++	of_node_put(cpu_node);
++
++	return ret;
++}
++
++static int psci_suspend_finisher(unsigned long index)
++{
++	u32 *state = __this_cpu_read(psci_power_state);
++
++	return psci_ops.cpu_suspend(state[index - 1],
++				    virt_to_phys(cpu_resume));
++}
++
++int psci_cpu_suspend_enter(unsigned long index)
++{
++	int ret;
++	u32 *state = __this_cpu_read(psci_power_state);
++	/*
++	 * idle state index 0 corresponds to wfi, should never be called
++	 * from the cpu_suspend operations
++	 */
++	if (WARN_ON_ONCE(!index))
++		return -EINVAL;
++
++	if (!psci_power_state_loses_context(state[index - 1]))
++		ret = psci_ops.cpu_suspend(state[index - 1], 0);
++	else
++		ret = cpu_suspend(index, psci_suspend_finisher);
++
++	return ret;
++}
++
++/* ARM specific CPU idle operations */
++#ifdef CONFIG_ARM
++static struct cpuidle_ops psci_cpuidle_ops __initdata = {
++	.suspend = psci_cpu_suspend_enter,
++	.init = psci_dt_cpu_init_idle,
++};
++
++CPUIDLE_METHOD_OF_DECLARE(psci, "arm,psci", &psci_cpuidle_ops);
++#endif
++#endif
++
+ static int psci_system_suspend(unsigned long unused)
+ {
+ 	return invoke_psci_fn(PSCI_FN_NATIVE(1_0, SYSTEM_SUSPEND),
+diff --git a/include/linux/psci.h b/include/linux/psci.h
+index 12c4865..393efe2 100644
+--- a/include/linux/psci.h
++++ b/include/linux/psci.h
+@@ -24,6 +24,9 @@ bool psci_tos_resident_on(int cpu);
+ bool psci_power_state_loses_context(u32 state);
+ bool psci_power_state_is_valid(u32 state);
+ 
++int psci_cpu_init_idle(unsigned int cpu);
++int psci_cpu_suspend_enter(unsigned long index);
++
+ struct psci_operations {
+ 	int (*cpu_suspend)(u32 state, unsigned long entry_point);
+ 	int (*cpu_off)(u32 state);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0023-arm64-mm-fold-alternatives-into-.init.patch b/tools/kdump/0023-arm64-mm-fold-alternatives-into-.init.patch
new file mode 100644
index 0000000..74fab18
--- /dev/null
+++ b/tools/kdump/0023-arm64-mm-fold-alternatives-into-.init.patch
@@ -0,0 +1,101 @@
+From d52693cafd2a56cfbb102e1d09daadfbf124aef9 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Wed, 9 Dec 2015 12:44:38 +0000
+Subject: [PATCH 023/123] arm64: mm: fold alternatives into .init
+
+Currently we treat the alternatives separately from other data that's
+only used during initialisation, using separate .altinstructions and
+.altinstr_replacement linker sections. These are freed for general
+allocation separately from .init*. This is problematic as:
+
+* We do not remove execute permissions, as we do for .init, leaving the
+  memory executable.
+
+* We pad between them, making the kernel Image bianry up to PAGE_SIZE
+  bytes larger than necessary.
+
+This patch moves the two sections into the contiguous region used for
+.init*. This saves some memory, ensures that we remove execute
+permissions, and allows us to remove some code made redundant by this
+reorganisation.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Cc: Andre Przywara <andre.przywara@arm.com>
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 9aa4ec1571da62366cfddc20f3b923609604fe63)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/alternative.h | 1 -
+ arch/arm64/kernel/alternative.c      | 6 ------
+ arch/arm64/kernel/vmlinux.lds.S      | 5 ++---
+ arch/arm64/mm/init.c                 | 1 -
+ 4 files changed, 2 insertions(+), 11 deletions(-)
+
+diff --git a/arch/arm64/include/asm/alternative.h b/arch/arm64/include/asm/alternative.h
+index d56ec07..e4962f0 100644
+--- a/arch/arm64/include/asm/alternative.h
++++ b/arch/arm64/include/asm/alternative.h
+@@ -19,7 +19,6 @@ struct alt_instr {
+ 
+ void __init apply_alternatives_all(void);
+ void apply_alternatives(void *start, size_t length);
+-void free_alternatives_memory(void);
+ 
+ #define ALTINSTR_ENTRY(feature)						      \
+ 	" .word 661b - .\n"				/* label           */ \
+diff --git a/arch/arm64/kernel/alternative.c b/arch/arm64/kernel/alternative.c
+index ab9db0e..d2ee1b2 100644
+--- a/arch/arm64/kernel/alternative.c
++++ b/arch/arm64/kernel/alternative.c
+@@ -158,9 +158,3 @@ void apply_alternatives(void *start, size_t length)
+ 
+ 	__apply_alternatives(&region);
+ }
+-
+-void free_alternatives_memory(void)
+-{
+-	free_reserved_area(__alt_instructions, __alt_instructions_end,
+-			   0, "alternatives");
+-}
+diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
+index 2fb8560..f04c476 100644
+--- a/arch/arm64/kernel/vmlinux.lds.S
++++ b/arch/arm64/kernel/vmlinux.lds.S
+@@ -145,9 +145,6 @@ SECTIONS
+ 
+ 	PERCPU_SECTION(L1_CACHE_BYTES)
+ 
+-	. = ALIGN(PAGE_SIZE);
+-	__init_end = .;
+-
+ 	. = ALIGN(4);
+ 	.altinstructions : {
+ 		__alt_instructions = .;
+@@ -159,6 +156,8 @@ SECTIONS
+ 	}
+ 
+ 	. = ALIGN(PAGE_SIZE);
++	__init_end = .;
++
+ 	_data = .;
+ 	_sdata = .;
+ 	RW_DATA_SECTION(L1_CACHE_BYTES, PAGE_SIZE, THREAD_SIZE)
+diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
+index 4cb98aa..b37e259 100644
+--- a/arch/arm64/mm/init.c
++++ b/arch/arm64/mm/init.c
+@@ -360,7 +360,6 @@ void free_initmem(void)
+ {
+ 	fixup_init();
+ 	free_initmem_default(0);
+-	free_alternatives_memory();
+ }
+ 
+ #ifdef CONFIG_BLK_DEV_INITRD
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0024-asm-generic-Fix-local-variable-shadow-in-__set_fixma.patch b/tools/kdump/0024-asm-generic-Fix-local-variable-shadow-in-__set_fixma.patch
new file mode 100644
index 0000000..a2a1d60
--- /dev/null
+++ b/tools/kdump/0024-asm-generic-Fix-local-variable-shadow-in-__set_fixma.patch
@@ -0,0 +1,55 @@
+From 316b66b091c9f0f3041f063d48ae9364f44d72da Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:44:55 +0000
+Subject: [PATCH 024/123] asm-generic: Fix local variable shadow in
+ __set_fixmap_offset
+
+Currently __set_fixmap_offset is a macro function which has a local
+variable called 'addr'. If a caller passes a 'phys' parameter which is
+derived from a variable also called 'addr', the local variable will
+shadow this, and the compiler will complain about the use of an
+uninitialized variable. To avoid the issue with namespace clashes,
+'addr' is prefixed with a liberal sprinkling of underscores.
+
+Turning __set_fixmap_offset into a static inline breaks the build for
+several architectures. Fixing this properly requires updates to a number
+of architectures to make them agree on the prototype of __set_fixmap (it
+could be done as a subsequent patch series).
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Cc: Arnd Bergmann <arnd@arndb.de>
+[catalin.marinas@arm.com: squashed the original function patch and macro fixup]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+
+(cherry picked from commit 3694bd76781b76c4f8d2ecd85018feeb1609f0e5)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ include/asm-generic/fixmap.h | 12 ++++++------
+ 1 file changed, 6 insertions(+), 6 deletions(-)
+
+diff --git a/include/asm-generic/fixmap.h b/include/asm-generic/fixmap.h
+index 1cbb833..827e4d3 100644
+--- a/include/asm-generic/fixmap.h
++++ b/include/asm-generic/fixmap.h
+@@ -70,12 +70,12 @@ static inline unsigned long virt_to_fix(const unsigned long vaddr)
+ #endif
+ 
+ /* Return a pointer with offset calculated */
+-#define __set_fixmap_offset(idx, phys, flags)		      \
+-({							      \
+-	unsigned long addr;				      \
+-	__set_fixmap(idx, phys, flags);			      \
+-	addr = fix_to_virt(idx) + ((phys) & (PAGE_SIZE - 1)); \
+-	addr;						      \
++#define __set_fixmap_offset(idx, phys, flags)				\
++({									\
++	unsigned long ________addr;					\
++	__set_fixmap(idx, phys, flags);					\
++	________addr = fix_to_virt(idx) + ((phys) & (PAGE_SIZE - 1));	\
++	________addr;							\
+ })
+ 
+ #define set_fixmap_offset(idx, phys) \
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0025-arm64-mm-remove-pointless-PAGE_MASKing.patch b/tools/kdump/0025-arm64-mm-remove-pointless-PAGE_MASKing.patch
new file mode 100644
index 0000000..2452863
--- /dev/null
+++ b/tools/kdump/0025-arm64-mm-remove-pointless-PAGE_MASKing.patch
@@ -0,0 +1,47 @@
+From 8ac80c9772b85518dd7800b9667d5fb0ab348d1a Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Wed, 9 Dec 2015 12:44:36 +0000
+Subject: [PATCH 025/123] arm64: mm: remove pointless PAGE_MASKing
+
+As pgd_offset{,_k} shift the input address by PGDIR_SHIFT, the sub-page
+bits will always be shifted out. There is no need to apply PAGE_MASK
+before this.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit e2c30ee320eb96304896c7ab84499e5bc5e5fb6e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 653735a..41b62ef 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -280,7 +280,7 @@ static void __init create_mapping(phys_addr_t phys, unsigned long virt,
+ 			&phys, virt);
+ 		return;
+ 	}
+-	__create_mapping(&init_mm, pgd_offset_k(virt & PAGE_MASK), phys, virt,
++	__create_mapping(&init_mm, pgd_offset_k(virt), phys, virt,
+ 			 size, prot, early_alloc);
+ }
+ 
+@@ -301,7 +301,7 @@ static void create_mapping_late(phys_addr_t phys, unsigned long virt,
+ 		return;
+ 	}
+ 
+-	return __create_mapping(&init_mm, pgd_offset_k(virt & PAGE_MASK),
++	return __create_mapping(&init_mm, pgd_offset_k(virt),
+ 				phys, virt, size, prot, late_alloc);
+ }
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0026-arm64-mm-specialise-pagetable-allocators.patch b/tools/kdump/0026-arm64-mm-specialise-pagetable-allocators.patch
new file mode 100644
index 0000000..55ef1e8
--- /dev/null
+++ b/tools/kdump/0026-arm64-mm-specialise-pagetable-allocators.patch
@@ -0,0 +1,221 @@
+From 8e413ed7ea623d4b5237f641a4a179ec345e1ef7 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:44:56 +0000
+Subject: [PATCH 026/123] arm64: mm: specialise pagetable allocators
+
+We pass a size parameter to early_alloc and late_alloc, but these are
+only ever used to allocate single pages. In late_alloc we always
+allocate a single page.
+
+Both allocators provide us with zeroed pages (such that all entries are
+invalid), but we have no barriers between allocating a page and adding
+that page to existing (live) tables. A concurrent page table walk may
+see stale data, leading to a number of issues.
+
+This patch specialises the two allocators for page tables. The size
+parameter is removed and the necessary dsb(ishst) is folded into each.
+To make it clear that the functions are intended for use for page table
+allocation, they are renamed to {early,late}_pgtable_alloc, with the
+related function pointed renamed to pgtable_alloc.
+
+As the dsb(ishst) is now in the allocator, the existing barrier for the
+zero page is redundant and thus is removed. The previously missing
+include of barrier.h is added.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 21ab99c289d350f4ae454bc069870009db6df20e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 52 +++++++++++++++++++++++++++-------------------------
+ 1 file changed, 27 insertions(+), 25 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 41b62ef..f823302 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -30,6 +30,7 @@
+ #include <linux/slab.h>
+ #include <linux/stop_machine.h>
+ 
++#include <asm/barrier.h>
+ #include <asm/cputype.h>
+ #include <asm/fixmap.h>
+ #include <asm/kernel-pgtable.h>
+@@ -62,15 +63,18 @@ pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
+ }
+ EXPORT_SYMBOL(phys_mem_access_prot);
+ 
+-static void __init *early_alloc(unsigned long sz)
++static void __init *early_pgtable_alloc(void)
+ {
+ 	phys_addr_t phys;
+ 	void *ptr;
+ 
+-	phys = memblock_alloc(sz, sz);
++	phys = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	BUG_ON(!phys);
+ 	ptr = __va(phys);
+-	memset(ptr, 0, sz);
++	memset(ptr, 0, PAGE_SIZE);
++
++	/* Ensure the zeroed page is visible to the page table walker */
++	dsb(ishst);
+ 	return ptr;
+ }
+ 
+@@ -95,12 +99,12 @@ static void split_pmd(pmd_t *pmd, pte_t *pte)
+ static void alloc_init_pte(pmd_t *pmd, unsigned long addr,
+ 				  unsigned long end, unsigned long pfn,
+ 				  pgprot_t prot,
+-				  void *(*alloc)(unsigned long size))
++				  void *(*pgtable_alloc)(void))
+ {
+ 	pte_t *pte;
+ 
+ 	if (pmd_none(*pmd) || pmd_sect(*pmd)) {
+-		pte = alloc(PTRS_PER_PTE * sizeof(pte_t));
++		pte = pgtable_alloc();
+ 		if (pmd_sect(*pmd))
+ 			split_pmd(pmd, pte);
+ 		__pmd_populate(pmd, __pa(pte), PMD_TYPE_TABLE);
+@@ -130,7 +134,7 @@ static void split_pud(pud_t *old_pud, pmd_t *pmd)
+ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 				  unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+-				  void *(*alloc)(unsigned long size))
++				  void *(*pgtable_alloc)(void))
+ {
+ 	pmd_t *pmd;
+ 	unsigned long next;
+@@ -139,7 +143,7 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 	 * Check for initial section mappings in the pgd/pud and remove them.
+ 	 */
+ 	if (pud_none(*pud) || pud_sect(*pud)) {
+-		pmd = alloc(PTRS_PER_PMD * sizeof(pmd_t));
++		pmd = pgtable_alloc();
+ 		if (pud_sect(*pud)) {
+ 			/*
+ 			 * need to have the 1G of mappings continue to be
+@@ -174,7 +178,7 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 			}
+ 		} else {
+ 			alloc_init_pte(pmd, addr, next, __phys_to_pfn(phys),
+-				       prot, alloc);
++				       prot, pgtable_alloc);
+ 		}
+ 		phys += next - addr;
+ 	} while (pmd++, addr = next, addr != end);
+@@ -195,13 +199,13 @@ static inline bool use_1G_block(unsigned long addr, unsigned long next,
+ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ 				  unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+-				  void *(*alloc)(unsigned long size))
++				  void *(*pgtable_alloc)(void))
+ {
+ 	pud_t *pud;
+ 	unsigned long next;
+ 
+ 	if (pgd_none(*pgd)) {
+-		pud = alloc(PTRS_PER_PUD * sizeof(pud_t));
++		pud = pgtable_alloc();
+ 		pgd_populate(mm, pgd, pud);
+ 	}
+ 	BUG_ON(pgd_bad(*pgd));
+@@ -234,7 +238,8 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ 				}
+ 			}
+ 		} else {
+-			alloc_init_pmd(mm, pud, addr, next, phys, prot, alloc);
++			alloc_init_pmd(mm, pud, addr, next, phys, prot,
++				       pgtable_alloc);
+ 		}
+ 		phys += next - addr;
+ 	} while (pud++, addr = next, addr != end);
+@@ -247,7 +252,7 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ static void  __create_mapping(struct mm_struct *mm, pgd_t *pgd,
+ 				    phys_addr_t phys, unsigned long virt,
+ 				    phys_addr_t size, pgprot_t prot,
+-				    void *(*alloc)(unsigned long size))
++				    void *(*pgtable_alloc)(void))
+ {
+ 	unsigned long addr, length, end, next;
+ 
+@@ -257,18 +262,18 @@ static void  __create_mapping(struct mm_struct *mm, pgd_t *pgd,
+ 	end = addr + length;
+ 	do {
+ 		next = pgd_addr_end(addr, end);
+-		alloc_init_pud(mm, pgd, addr, next, phys, prot, alloc);
++		alloc_init_pud(mm, pgd, addr, next, phys, prot, pgtable_alloc);
+ 		phys += next - addr;
+ 	} while (pgd++, addr = next, addr != end);
+ }
+ 
+-static void *late_alloc(unsigned long size)
++static void *late_pgtable_alloc(void)
+ {
+-	void *ptr;
+-
+-	BUG_ON(size > PAGE_SIZE);
+-	ptr = (void *)__get_free_page(PGALLOC_GFP);
++	void *ptr = (void *)__get_free_page(PGALLOC_GFP);
+ 	BUG_ON(!ptr);
++
++	/* Ensure the zeroed page is visible to the page table walker */
++	dsb(ishst);
+ 	return ptr;
+ }
+ 
+@@ -281,7 +286,7 @@ static void __init create_mapping(phys_addr_t phys, unsigned long virt,
+ 		return;
+ 	}
+ 	__create_mapping(&init_mm, pgd_offset_k(virt), phys, virt,
+-			 size, prot, early_alloc);
++			 size, prot, early_pgtable_alloc);
+ }
+ 
+ void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
+@@ -289,7 +294,7 @@ void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
+ 			       pgprot_t prot)
+ {
+ 	__create_mapping(mm, pgd_offset(mm, virt), phys, virt, size, prot,
+-				late_alloc);
++				late_pgtable_alloc);
+ }
+ 
+ static void create_mapping_late(phys_addr_t phys, unsigned long virt,
+@@ -302,7 +307,7 @@ static void create_mapping_late(phys_addr_t phys, unsigned long virt,
+ 	}
+ 
+ 	return __create_mapping(&init_mm, pgd_offset_k(virt),
+-				phys, virt, size, prot, late_alloc);
++				phys, virt, size, prot, late_pgtable_alloc);
+ }
+ 
+ #ifdef CONFIG_DEBUG_RODATA
+@@ -450,15 +455,12 @@ void __init paging_init(void)
+ 	fixup_executable();
+ 
+ 	/* allocate the zero page. */
+-	zero_page = early_alloc(PAGE_SIZE);
++	zero_page = early_pgtable_alloc();
+ 
+ 	bootmem_init();
+ 
+ 	empty_zero_page = virt_to_page(zero_page);
+ 
+-	/* Ensure the zero page is visible to the page table walker */
+-	dsb(ishst);
+-
+ 	/*
+ 	 * TTBR0 is only used for the identity mapping at this stage. Make it
+ 	 * point to zero page to avoid speculatively fetching new entries.
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0027-arm64-head.S-use-memset-to-clear-BSS.patch b/tools/kdump/0027-arm64-head.S-use-memset-to-clear-BSS.patch
new file mode 100644
index 0000000..5ffb1c6
--- /dev/null
+++ b/tools/kdump/0027-arm64-head.S-use-memset-to-clear-BSS.patch
@@ -0,0 +1,57 @@
+From d956e9f811728706e81b353289141c11eb2fab34 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Wed, 6 Jan 2016 11:05:27 +0000
+Subject: [PATCH 027/123] arm64: head.S: use memset to clear BSS
+
+Currently we use an open-coded memzero to clear the BSS. As it is a
+trivial implementation, it is sub-optimal.
+
+Our optimised memset doesn't use the stack, is position-independent, and
+for the memzero case can use of DC ZVA to clear large blocks
+efficiently. In __mmap_switched the MMU is on and there are no live
+caller-saved registers, so we can safely call an uninstrumented memset.
+
+This patch changes __mmap_switched to use memset when clearing the BSS.
+We use the __pi_memset alias so as to avoid any instrumentation in all
+kernel configurations.
+
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 2a803c4db615d85126c5c7afd5849a3cfde71422)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kernel/head.S | 15 +++++++--------
+ 1 file changed, 7 insertions(+), 8 deletions(-)
+
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index 20ceb5e..10eeeab 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -415,14 +415,13 @@ ENDPROC(__create_page_tables)
+  */
+ 	.set	initial_sp, init_thread_union + THREAD_START_SP
+ __mmap_switched:
+-	adr_l	x6, __bss_start
+-	adr_l	x7, __bss_stop
+-
+-1:	cmp	x6, x7
+-	b.hs	2f
+-	str	xzr, [x6], #8			// Clear BSS
+-	b	1b
+-2:
++	// Clear BSS
++	adr_l	x0, __bss_start
++	mov	x1, xzr
++	adr_l	x2, __bss_stop
++	sub	x2, x2, x0
++	bl	__pi_memset
++
+ 	adr_l	sp, initial_sp, x4
+ 	str_l	x21, __fdt_pointer, x5		// Save FDT pointer
+ 	str_l	x24, memstart_addr, x6		// Save PHYS_OFFSET
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0028-arm64-mm-place-empty_zero_page-in-bss.patch b/tools/kdump/0028-arm64-mm-place-empty_zero_page-in-bss.patch
new file mode 100644
index 0000000..631bb00
--- /dev/null
+++ b/tools/kdump/0028-arm64-mm-place-empty_zero_page-in-bss.patch
@@ -0,0 +1,118 @@
+From 70b6ef26fe7328bdb7d9be2cb1cf915809df7ac8 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:44:57 +0000
+Subject: [PATCH 028/123] arm64: mm: place empty_zero_page in bss
+
+Currently the zero page is set up in paging_init, and thus we cannot use
+the zero page earlier. We use the zero page as a reserved TTBR value
+from which no TLB entries may be allocated (e.g. when uninstalling the
+idmap). To enable such usage earlier (as may be required for invasive
+changes to the kernel page tables), and to minimise the time that the
+idmap is active, we need to be able to use the zero page before
+paging_init.
+
+This patch follows the example set by x86, by allocating the zero page
+at compile time, in .bss. This means that the zero page itself is
+available immediately upon entry to start_kernel (as we zero .bss before
+this), and also means that the zero page takes up no space in the raw
+Image binary. The associated struct page is allocated in bootmem_init,
+and remains unavailable until this time.
+
+Outside of arch code, the only users of empty_zero_page assume that the
+empty_zero_page symbol refers to the zeroed memory itself, and that
+ZERO_PAGE(x) must be used to acquire the associated struct page,
+following the example of x86. This patch also brings arm64 inline with
+these assumptions.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 5227cfa71f9e8574373f4d0e9e754942d76cdf67)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/mmu_context.h | 2 +-
+ arch/arm64/include/asm/pgtable.h     | 4 ++--
+ arch/arm64/kernel/head.S             | 1 +
+ arch/arm64/mm/mmu.c                  | 9 +--------
+ 4 files changed, 5 insertions(+), 11 deletions(-)
+
+diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
+index 2416578..600eacb 100644
+--- a/arch/arm64/include/asm/mmu_context.h
++++ b/arch/arm64/include/asm/mmu_context.h
+@@ -48,7 +48,7 @@ static inline void contextidr_thread_switch(struct task_struct *next)
+  */
+ static inline void cpu_set_reserved_ttbr0(void)
+ {
+-	unsigned long ttbr = page_to_phys(empty_zero_page);
++	unsigned long ttbr = virt_to_phys(empty_zero_page);
+ 
+ 	asm(
+ 	"	msr	ttbr0_el1, %0			// set TTBR0\n"
+diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
+index 67c2ad6..a11053a 100644
+--- a/arch/arm64/include/asm/pgtable.h
++++ b/arch/arm64/include/asm/pgtable.h
+@@ -123,8 +123,8 @@ extern void __pgd_error(const char *file, int line, unsigned long val);
+  * ZERO_PAGE is a global shared page that is always zero: used
+  * for zero-mapped memory areas etc..
+  */
+-extern struct page *empty_zero_page;
+-#define ZERO_PAGE(vaddr)	(empty_zero_page)
++extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
++#define ZERO_PAGE(vaddr)	virt_to_page(empty_zero_page)
+ 
+ #define pte_ERROR(pte)		__pte_error(__FILE__, __LINE__, pte_val(pte))
+ 
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index 10eeeab..8e22dfe 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -421,6 +421,7 @@ __mmap_switched:
+ 	adr_l	x2, __bss_stop
+ 	sub	x2, x2, x0
+ 	bl	__pi_memset
++	dsb	ishst				// Make zero page visible to PTW
+ 
+ 	adr_l	sp, initial_sp, x4
+ 	str_l	x21, __fdt_pointer, x5		// Save FDT pointer
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index f823302..7319529 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -49,7 +49,7 @@ u64 idmap_t0sz = TCR_T0SZ(VA_BITS);
+  * Empty_zero_page is a special page that is used for zero-initialized data
+  * and COW.
+  */
+-struct page *empty_zero_page;
++unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)] __page_aligned_bss;
+ EXPORT_SYMBOL(empty_zero_page);
+ 
+ pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
+@@ -449,18 +449,11 @@ void fixup_init(void)
+  */
+ void __init paging_init(void)
+ {
+-	void *zero_page;
+-
+ 	map_mem();
+ 	fixup_executable();
+ 
+-	/* allocate the zero page. */
+-	zero_page = early_pgtable_alloc();
+-
+ 	bootmem_init();
+ 
+-	empty_zero_page = virt_to_page(zero_page);
+-
+ 	/*
+ 	 * TTBR0 is only used for the identity mapping at this stage. Make it
+ 	 * point to zero page to avoid speculatively fetching new entries.
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0029-arm64-unify-idmap-removal.patch b/tools/kdump/0029-arm64-unify-idmap-removal.patch
new file mode 100644
index 0000000..bb40be3
--- /dev/null
+++ b/tools/kdump/0029-arm64-unify-idmap-removal.patch
@@ -0,0 +1,157 @@
+From e6ebd418b05bb22c6e27ccdbe67ec23989e20ac8 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:44:58 +0000
+Subject: [PATCH 029/123] arm64: unify idmap removal
+
+We currently open-code the removal of the idmap and restoration of the
+current task's MMU state in a few places.
+
+Before introducing yet more copies of this sequence, unify these to call
+a new helper, cpu_uninstall_idmap.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 9e8e865bbe294a69666a1996bda3e87825b258c0)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/mmu_context.h | 25 +++++++++++++++++++++++++
+ arch/arm64/kernel/setup.c            |  1 +
+ arch/arm64/kernel/smp.c              |  4 +---
+ arch/arm64/kernel/suspend.c          | 20 ++++----------------
+ arch/arm64/mm/mmu.c                  |  4 +---
+ 5 files changed, 32 insertions(+), 22 deletions(-)
+
+diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
+index 600eacb..b1b2514 100644
+--- a/arch/arm64/include/asm/mmu_context.h
++++ b/arch/arm64/include/asm/mmu_context.h
+@@ -27,6 +27,7 @@
+ #include <asm-generic/mm_hooks.h>
+ #include <asm/cputype.h>
+ #include <asm/pgtable.h>
++#include <asm/tlbflush.h>
+ 
+ #ifdef CONFIG_PID_IN_CONTEXTIDR
+ static inline void contextidr_thread_switch(struct task_struct *next)
+@@ -90,6 +91,30 @@ static inline void cpu_set_default_tcr_t0sz(void)
+ }
+ 
+ /*
++ * Remove the idmap from TTBR0_EL1 and install the pgd of the active mm.
++ *
++ * The idmap lives in the same VA range as userspace, but uses global entries
++ * and may use a different TCR_EL1.T0SZ. To avoid issues resulting from
++ * speculative TLB fetches, we must temporarily install the reserved page
++ * tables while we invalidate the TLBs and set up the correct TCR_EL1.T0SZ.
++ *
++ * If current is a not a user task, the mm covers the TTBR1_EL1 page tables,
++ * which should not be installed in TTBR0_EL1. In this case we can leave the
++ * reserved page tables in place.
++ */
++static inline void cpu_uninstall_idmap(void)
++{
++	struct mm_struct *mm = current->active_mm;
++
++	cpu_set_reserved_ttbr0();
++	local_flush_tlb_all();
++	cpu_set_default_tcr_t0sz();
++
++	if (mm != &init_mm)
++		cpu_switch_mm(mm->pgd, mm);
++}
++
++/*
+  * It would be nice to return ASIDs back to the allocator, but unfortunately
+  * that introduces a race with a generation rollover where we could erroneously
+  * free an ASID allocated in a future generation. We could workaround this by
+diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c
+index 8119479..f6621ba 100644
+--- a/arch/arm64/kernel/setup.c
++++ b/arch/arm64/kernel/setup.c
+@@ -62,6 +62,7 @@
+ #include <asm/memblock.h>
+ #include <asm/efi.h>
+ #include <asm/xen/hypervisor.h>
++#include <asm/mmu_context.h>
+ 
+ phys_addr_t __fdt_pointer __initdata;
+ 
+diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
+index f3c3d8f..3df454f 100644
+--- a/arch/arm64/kernel/smp.c
++++ b/arch/arm64/kernel/smp.c
+@@ -149,9 +149,7 @@ asmlinkage void secondary_start_kernel(void)
+ 	 * TTBR0 is only used for the identity mapping at this stage. Make it
+ 	 * point to zero page to avoid speculatively fetching new entries.
+ 	 */
+-	cpu_set_reserved_ttbr0();
+-	local_flush_tlb_all();
+-	cpu_set_default_tcr_t0sz();
++	cpu_uninstall_idmap();
+ 
+ 	preempt_disable();
+ 	trace_hardirqs_off();
+diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
+index 00c1372..20b6b9b 100644
+--- a/arch/arm64/kernel/suspend.c
++++ b/arch/arm64/kernel/suspend.c
+@@ -62,7 +62,6 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
+  */
+ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+ {
+-	struct mm_struct *mm = current->active_mm;
+ 	int ret;
+ 	unsigned long flags;
+ 
+@@ -89,22 +88,11 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+ 	ret = __cpu_suspend_enter(arg, fn);
+ 	if (ret == 0) {
+ 		/*
+-		 * We are resuming from reset with TTBR0_EL1 set to the
+-		 * idmap to enable the MMU; set the TTBR0 to the reserved
+-		 * page tables to prevent speculative TLB allocations, flush
+-		 * the local tlb and set the default tcr_el1.t0sz so that
+-		 * the TTBR0 address space set-up is properly restored.
+-		 * If the current active_mm != &init_mm we entered cpu_suspend
+-		 * with mappings in TTBR0 that must be restored, so we switch
+-		 * them back to complete the address space configuration
+-		 * restoration before returning.
++		 * We are resuming from reset with the idmap active in TTBR0_EL1.
++		 * We must uninstall the idmap and restore the expected MMU
++		 * state before we can possibly return to userspace.
+ 		 */
+-		cpu_set_reserved_ttbr0();
+-		local_flush_tlb_all();
+-		cpu_set_default_tcr_t0sz();
+-
+-		if (mm != &init_mm)
+-			cpu_switch_mm(mm->pgd, mm);
++		cpu_uninstall_idmap();
+ 
+ 		/*
+ 		 * Restore per-cpu offset before any kernel
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 7319529..74898e5 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -458,9 +458,7 @@ void __init paging_init(void)
+ 	 * TTBR0 is only used for the identity mapping at this stage. Make it
+ 	 * point to zero page to avoid speculatively fetching new entries.
+ 	 */
+-	cpu_set_reserved_ttbr0();
+-	local_flush_tlb_all();
+-	cpu_set_default_tcr_t0sz();
++	cpu_uninstall_idmap();
+ }
+ 
+ /*
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0030-arm64-unmap-idmap-earlier.patch b/tools/kdump/0030-arm64-unmap-idmap-earlier.patch
new file mode 100644
index 0000000..fbd33eb
--- /dev/null
+++ b/tools/kdump/0030-arm64-unmap-idmap-earlier.patch
@@ -0,0 +1,66 @@
+From f3351b59420be88177dd7242bb70a5c1d9b8d3a1 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:44:59 +0000
+Subject: [PATCH 030/123] arm64: unmap idmap earlier
+
+During boot we leave the idmap in place until paging_init, as we
+previously had to wait for the zero page to become allocated and
+accessible.
+
+Now that we have a statically-allocated zero page, we can uninstall the
+idmap much earlier in the boot process, making it far easier to spot
+accidental use of physical addresses. This also brings the cold boot
+path in line with the secondary boot path.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 86ccce896cb0aa800a7a6dcd29b41ffc4eeb1a75)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kernel/setup.c | 6 ++++++
+ arch/arm64/mm/mmu.c       | 6 ------
+ 2 files changed, 6 insertions(+), 6 deletions(-)
+
+diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c
+index f6621ba..cfed56f 100644
+--- a/arch/arm64/kernel/setup.c
++++ b/arch/arm64/kernel/setup.c
+@@ -314,6 +314,12 @@ void __init setup_arch(char **cmdline_p)
+ 	 */
+ 	local_async_enable();
+ 
++	/*
++	 * TTBR0 is only used for the identity mapping at this stage. Make it
++	 * point to zero page to avoid speculatively fetching new entries.
++	 */
++	cpu_uninstall_idmap();
++
+ 	efi_init();
+ 	arm64_memblock_init();
+ 
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 74898e5..1e2ae80 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -453,12 +453,6 @@ void __init paging_init(void)
+ 	fixup_executable();
+ 
+ 	bootmem_init();
+-
+-	/*
+-	 * TTBR0 is only used for the identity mapping at this stage. Make it
+-	 * point to zero page to avoid speculatively fetching new entries.
+-	 */
+-	cpu_uninstall_idmap();
+ }
+ 
+ /*
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0031-arm64-add-function-to-install-the-idmap.patch b/tools/kdump/0031-arm64-add-function-to-install-the-idmap.patch
new file mode 100644
index 0000000..4ea4352
--- /dev/null
+++ b/tools/kdump/0031-arm64-add-function-to-install-the-idmap.patch
@@ -0,0 +1,71 @@
+From 69ffd2011ab1019447200bb6e2de2da40dcd9c19 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:00 +0000
+Subject: [PATCH 031/123] arm64: add function to install the idmap
+
+In some cases (e.g. when making invasive changes to the kernel page
+tables) we will need to execute code from the idmap.
+
+Add a new helper which may be used to install the idmap, complementing
+the existing cpu_uninstall_idmap.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 609116d202a8c5fd3fe393eb85373cbee906df68)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/mmu_context.h | 16 ++++++++++++++--
+ 1 file changed, 14 insertions(+), 2 deletions(-)
+
+diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
+index b1b2514..944f273 100644
+--- a/arch/arm64/include/asm/mmu_context.h
++++ b/arch/arm64/include/asm/mmu_context.h
+@@ -74,7 +74,7 @@ static inline bool __cpu_uses_extended_idmap(void)
+ /*
+  * Set TCR.T0SZ to its default value (based on VA_BITS)
+  */
+-static inline void cpu_set_default_tcr_t0sz(void)
++static inline void __cpu_set_tcr_t0sz(unsigned long t0sz)
+ {
+ 	unsigned long tcr;
+ 
+@@ -87,9 +87,12 @@ static inline void cpu_set_default_tcr_t0sz(void)
+ 	"	msr	tcr_el1, %0	;"
+ 	"	isb"
+ 	: "=&r" (tcr)
+-	: "r"(TCR_T0SZ(VA_BITS)), "I"(TCR_T0SZ_OFFSET), "I"(TCR_TxSZ_WIDTH));
++	: "r"(t0sz), "I"(TCR_T0SZ_OFFSET), "I"(TCR_TxSZ_WIDTH));
+ }
+ 
++#define cpu_set_default_tcr_t0sz()	__cpu_set_tcr_t0sz(TCR_T0SZ(VA_BITS))
++#define cpu_set_idmap_tcr_t0sz()	__cpu_set_tcr_t0sz(idmap_t0sz)
++
+ /*
+  * Remove the idmap from TTBR0_EL1 and install the pgd of the active mm.
+  *
+@@ -114,6 +117,15 @@ static inline void cpu_uninstall_idmap(void)
+ 		cpu_switch_mm(mm->pgd, mm);
+ }
+ 
++static inline void cpu_install_idmap(void)
++{
++	cpu_set_reserved_ttbr0();
++	local_flush_tlb_all();
++	cpu_set_idmap_tcr_t0sz();
++
++	cpu_switch_mm(idmap_pg_dir, &init_mm);
++}
++
+ /*
+  * It would be nice to return ASIDs back to the allocator, but unfortunately
+  * that introduces a race with a generation rollover where we could erroneously
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0032-arm64-mm-place-__cpu_setup-in-.text.patch b/tools/kdump/0032-arm64-mm-place-__cpu_setup-in-.text.patch
new file mode 100644
index 0000000..584c860
--- /dev/null
+++ b/tools/kdump/0032-arm64-mm-place-__cpu_setup-in-.text.patch
@@ -0,0 +1,42 @@
+From 90948bbaab6bd564412bd072baf4ba62d362a4fd Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Fri, 11 Dec 2015 11:04:31 +0000
+Subject: [PATCH 032/123] arm64: mm: place __cpu_setup in .text
+
+We drop __cpu_setup in .text.init, which ends up being part of .text.
+The .text.init section was a legacy section name which has been unused
+elsewhere for a long time.
+
+The ".text.init" name is misleading if read as a synonym for
+".init.text". Any CPU may execute __cpu_setup before turning the MMU on,
+so it should simply live in .text.
+
+Remove the pointless section assignment. This will leave __cpu_setup in
+the .text section.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit f00083cae331e5d3eecade6b4fdc35d0825e73ef)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/proc.S | 2 --
+ 1 file changed, 2 deletions(-)
+
+diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
+index 18201e9..3a44efb 100644
+--- a/arch/arm64/mm/proc.S
++++ b/arch/arm64/mm/proc.S
+@@ -152,8 +152,6 @@ alternative_else
+ alternative_endif
+ ENDPROC(cpu_do_switch_mm)
+ 
+-	.section ".text.init", #alloc, #execinstr
+-
+ /*
+  *	__cpu_setup
+  *
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0033-arm64-mm-add-code-to-safely-replace-TTBR1_EL1.patch b/tools/kdump/0033-arm64-mm-add-code-to-safely-replace-TTBR1_EL1.patch
new file mode 100644
index 0000000..b9a56ad
--- /dev/null
+++ b/tools/kdump/0033-arm64-mm-add-code-to-safely-replace-TTBR1_EL1.patch
@@ -0,0 +1,118 @@
+From c1118fbb4277a30f04477af879902bbc037a0a1e Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:01 +0000
+Subject: [PATCH 033/123] arm64: mm: add code to safely replace TTBR1_EL1
+
+If page tables are modified without suitable TLB maintenance, the ARM
+architecture permits multiple TLB entries to be allocated for the same
+VA. When this occurs, it is permitted that TLB conflict aborts are
+raised in response to synchronous data/instruction accesses, and/or and
+amalgamation of the TLB entries may be used as a result of a TLB lookup.
+
+The presence of conflicting TLB entries may result in a variety of
+behaviours detrimental to the system (e.g. erroneous physical addresses
+may be used by I-cache fetches and/or page table walks). Some of these
+cases may result in unexpected changes of hardware state, and/or result
+in the (asynchronous) delivery of SError.
+
+To avoid these issues, we must avoid situations where conflicting
+entries may be allocated into TLBs. For user and module mappings we can
+follow a strict break-before-make approach, but this cannot work for
+modifications to the swapper page tables that cover the kernel text and
+data.
+
+Instead, this patch adds code which is intended to be executed from the
+idmap, which can safely unmap the swapper page tables as it only
+requires the idmap to be active. This enables us to uninstall the active
+TTBR1_EL1 entry, invalidate TLBs, then install a new TTBR1_EL1 entry
+without potentially unmapping code or data required for the sequence.
+This avoids the risk of conflict, but requires that updates are staged
+in a copy of the swapper page tables prior to being installed.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 50e1881ddde2a986c7d0d2150985239e5e3d7d96)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/mmu_context.h | 19 +++++++++++++++++++
+ arch/arm64/mm/proc.S                 | 28 ++++++++++++++++++++++++++++
+ 2 files changed, 47 insertions(+)
+
+diff --git a/arch/arm64/include/asm/mmu_context.h b/arch/arm64/include/asm/mmu_context.h
+index 944f273..a00f7cf 100644
+--- a/arch/arm64/include/asm/mmu_context.h
++++ b/arch/arm64/include/asm/mmu_context.h
+@@ -127,6 +127,25 @@ static inline void cpu_install_idmap(void)
+ }
+ 
+ /*
++ * Atomically replaces the active TTBR1_EL1 PGD with a new VA-compatible PGD,
++ * avoiding the possibility of conflicting TLB entries being allocated.
++ */
++static inline void cpu_replace_ttbr1(pgd_t *pgd)
++{
++	typedef void (ttbr_replace_func)(phys_addr_t);
++	extern ttbr_replace_func idmap_cpu_replace_ttbr1;
++	ttbr_replace_func *replace_phys;
++
++	phys_addr_t pgd_phys = virt_to_phys(pgd);
++
++	replace_phys = (void *)virt_to_phys(idmap_cpu_replace_ttbr1);
++
++	cpu_install_idmap();
++	replace_phys(pgd_phys);
++	cpu_uninstall_idmap();
++}
++
++/*
+  * It would be nice to return ASIDs back to the allocator, but unfortunately
+  * that introduces a race with a generation rollover where we could erroneously
+  * free an ASID allocated in a future generation. We could workaround this by
+diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
+index 3a44efb..a92738e 100644
+--- a/arch/arm64/mm/proc.S
++++ b/arch/arm64/mm/proc.S
+@@ -152,6 +152,34 @@ alternative_else
+ alternative_endif
+ ENDPROC(cpu_do_switch_mm)
+ 
++	.pushsection ".idmap.text", "ax"
++/*
++ * void idmap_cpu_replace_ttbr1(phys_addr_t new_pgd)
++ *
++ * This is the low-level counterpart to cpu_replace_ttbr1, and should not be
++ * called by anything else. It can only be executed from a TTBR0 mapping.
++ */
++ENTRY(idmap_cpu_replace_ttbr1)
++	mrs	x2, daif
++	msr	daifset, #0xf
++
++	adrp	x1, empty_zero_page
++	msr	ttbr1_el1, x1
++	isb
++
++	tlbi	vmalle1
++	dsb	nsh
++	isb
++
++	msr	ttbr1_el1, x0
++	isb
++
++	msr	daif, x2
++
++	ret
++ENDPROC(idmap_cpu_replace_ttbr1)
++	.popsection
++
+ /*
+  *	__cpu_setup
+  *
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0034-arm64-Use-PoU-cache-instr-for-I-D-coherency.patch b/tools/kdump/0034-arm64-Use-PoU-cache-instr-for-I-D-coherency.patch
new file mode 100644
index 0000000..7d8f104
--- /dev/null
+++ b/tools/kdump/0034-arm64-Use-PoU-cache-instr-for-I-D-coherency.patch
@@ -0,0 +1,183 @@
+From ae36c72149e94ac31a3f180df84658c4c5ef6713 Mon Sep 17 00:00:00 2001
+From: Ashok Kumar <ashoks@broadcom.com>
+Date: Thu, 17 Dec 2015 01:38:32 -0800
+Subject: [PATCH 034/123] arm64: Use PoU cache instr for I/D coherency
+
+In systems with three levels of cache(PoU at L1 and PoC at L3),
+PoC cache flush instructions flushes L2 and L3 caches which could affect
+performance.
+For cache flushes for I and D coherency, PoU should suffice.
+So changing all I and D coherency related cache flushes to PoU.
+
+Introduced a new __clean_dcache_area_pou API for dcache flush till PoU
+and provided a common macro for __flush_dcache_area and
+__clean_dcache_area_pou.
+
+Also, now in __sync_icache_dcache, icache invalidation for non-aliasing
+VIPT icache is done only for that particular page instead of the earlier
+__flush_icache_all.
+
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Ashok Kumar <ashoks@broadcom.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 0a28714c53fd4f7aea709be7577dfbe0095c8c3e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/mm/proc-macros.S
+---
+ arch/arm64/include/asm/cacheflush.h |  1 +
+ arch/arm64/mm/cache.S               | 28 +++++++++++++++++-----------
+ arch/arm64/mm/flush.c               | 33 ++++++++++++++++++---------------
+ arch/arm64/mm/proc-macros.S         | 21 +++++++++++++++++++++
+ 4 files changed, 57 insertions(+), 26 deletions(-)
+
+diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h
+index 54efeda..7fc294c 100644
+--- a/arch/arm64/include/asm/cacheflush.h
++++ b/arch/arm64/include/asm/cacheflush.h
+@@ -68,6 +68,7 @@
+ extern void flush_cache_range(struct vm_area_struct *vma, unsigned long start, unsigned long end);
+ extern void flush_icache_range(unsigned long start, unsigned long end);
+ extern void __flush_dcache_area(void *addr, size_t len);
++extern void __clean_dcache_area_pou(void *addr, size_t len);
+ extern long __flush_cache_user_range(unsigned long start, unsigned long end);
+ 
+ static inline void flush_cache_mm(struct mm_struct *mm)
+diff --git a/arch/arm64/mm/cache.S b/arch/arm64/mm/cache.S
+index cfa44a6..6df0706 100644
+--- a/arch/arm64/mm/cache.S
++++ b/arch/arm64/mm/cache.S
+@@ -81,26 +81,32 @@ ENDPROC(__flush_cache_user_range)
+ /*
+  *	__flush_dcache_area(kaddr, size)
+  *
+- *	Ensure that the data held in the page kaddr is written back to the
+- *	page in question.
++ *	Ensure that any D-cache lines for the interval [kaddr, kaddr+size)
++ *	are cleaned and invalidated to the PoC.
+  *
+  *	- kaddr   - kernel address
+  *	- size    - size in question
+  */
+ ENTRY(__flush_dcache_area)
+-	dcache_line_size x2, x3
+-	add	x1, x0, x1
+-	sub	x3, x2, #1
+-	bic	x0, x0, x3
+-1:	dc	civac, x0			// clean & invalidate D line / unified line
+-	add	x0, x0, x2
+-	cmp	x0, x1
+-	b.lo	1b
+-	dsb	sy
++	dcache_by_line_op civac, sy, x0, x1, x2, x3
+ 	ret
+ ENDPIPROC(__flush_dcache_area)
+ 
+ /*
++ *	__clean_dcache_area_pou(kaddr, size)
++ *
++ * 	Ensure that any D-cache lines for the interval [kaddr, kaddr+size)
++ * 	are cleaned to the PoU.
++ *
++ *	- kaddr   - kernel address
++ *	- size    - size in question
++ */
++ENTRY(__clean_dcache_area_pou)
++	dcache_by_line_op cvau, ish, x0, x1, x2, x3
++	ret
++ENDPROC(__clean_dcache_area_pou)
++
++/*
+  *	__inval_cache_range(start, end)
+  *	- start   - start address of region
+  *	- end     - end address of region
+diff --git a/arch/arm64/mm/flush.c b/arch/arm64/mm/flush.c
+index c26b804..46649d6 100644
+--- a/arch/arm64/mm/flush.c
++++ b/arch/arm64/mm/flush.c
+@@ -34,19 +34,24 @@ void flush_cache_range(struct vm_area_struct *vma, unsigned long start,
+ 		__flush_icache_all();
+ }
+ 
++static void sync_icache_aliases(void *kaddr, unsigned long len)
++{
++	unsigned long addr = (unsigned long)kaddr;
++
++	if (icache_is_aliasing()) {
++		__clean_dcache_area_pou(kaddr, len);
++		__flush_icache_all();
++	} else {
++		flush_icache_range(addr, addr + len);
++	}
++}
++
+ static void flush_ptrace_access(struct vm_area_struct *vma, struct page *page,
+ 				unsigned long uaddr, void *kaddr,
+ 				unsigned long len)
+ {
+-	if (vma->vm_flags & VM_EXEC) {
+-		unsigned long addr = (unsigned long)kaddr;
+-		if (icache_is_aliasing()) {
+-			__flush_dcache_area(kaddr, len);
+-			__flush_icache_all();
+-		} else {
+-			flush_icache_range(addr, addr + len);
+-		}
+-	}
++	if (vma->vm_flags & VM_EXEC)
++		sync_icache_aliases(kaddr, len);
+ }
+ 
+ /*
+@@ -74,13 +79,11 @@ void __sync_icache_dcache(pte_t pte, unsigned long addr)
+ 	if (!page_mapping(page))
+ 		return;
+ 
+-	if (!test_and_set_bit(PG_dcache_clean, &page->flags)) {
+-		__flush_dcache_area(page_address(page),
+-				PAGE_SIZE << compound_order(page));
++	if (!test_and_set_bit(PG_dcache_clean, &page->flags))
++		sync_icache_aliases(page_address(page),
++				    PAGE_SIZE << compound_order(page));
++	else if (icache_is_aivivt())
+ 		__flush_icache_all();
+-	} else if (icache_is_aivivt()) {
+-		__flush_icache_all();
+-	}
+ }
+ 
+ /*
+diff --git a/arch/arm64/mm/proc-macros.S b/arch/arm64/mm/proc-macros.S
+index d69dfff..2bce364 100644
+--- a/arch/arm64/mm/proc-macros.S
++++ b/arch/arm64/mm/proc-macros.S
+@@ -74,3 +74,24 @@
+ 	msr	pmuserenr_el0, xzr		// Disable PMU access from EL0
+ 9000:
+ 	.endm
++/*
++ * Macro to perform a data cache maintenance for the interval
++ * [kaddr, kaddr + size)
++ *
++ * 	op:		operation passed to dc instruction
++ * 	domain:		domain used in dsb instruciton
++ * 	kaddr:		starting virtual address of the region
++ * 	size:		size of the region
++ * 	Corrupts: 	kaddr, size, tmp1, tmp2
++ */
++	.macro dcache_by_line_op op, domain, kaddr, size, tmp1, tmp2
++	dcache_line_size \tmp1, \tmp2
++	add	\size, \kaddr, \size
++	sub	\tmp2, \tmp1, #1
++	bic	\kaddr, \kaddr, \tmp2
++9998:	dc	\op, \kaddr
++	add	\kaddr, \kaddr, \tmp1
++	cmp	\kaddr, \size
++	b.lo	9998b
++	dsb	\domain
++	.endm
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0035-arm64-Add-macros-to-read-write-system-registers.patch b/tools/kdump/0035-arm64-Add-macros-to-read-write-system-registers.patch
new file mode 100644
index 0000000..435b239
--- /dev/null
+++ b/tools/kdump/0035-arm64-Add-macros-to-read-write-system-registers.patch
@@ -0,0 +1,69 @@
+From 72d7c52bd169b276cd20a23a34e3ae9473efd35c Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Thu, 5 Nov 2015 15:09:17 +0000
+Subject: [PATCH 035/123] arm64: Add macros to read/write system registers
+
+Rather than crafting custom macros for reading/writing each system
+register provide generics accessors, read_sysreg and write_sysreg, for
+this purpose.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Suzuki Poulose <suzuki.poulose@arm.com>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 3600c2fdc09a43a30909743569e35a29121602ed)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/sysreg.h | 21 +++++++++++++++++++++
+ 1 file changed, 21 insertions(+)
+
+diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
+index d48ab5b..4aeebec 100644
+--- a/arch/arm64/include/asm/sysreg.h
++++ b/arch/arm64/include/asm/sysreg.h
+@@ -20,6 +20,8 @@
+ #ifndef __ASM_SYSREG_H
+ #define __ASM_SYSREG_H
+ 
++#include <linux/stringify.h>
++
+ #include <asm/opcodes.h>
+ 
+ /*
+@@ -208,6 +210,8 @@
+ 
+ #else
+ 
++#include <linux/types.h>
++
+ asm(
+ "	.irp	num,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n"
+ "	.equ	__reg_num_x\\num, \\num\n"
+@@ -232,6 +236,23 @@ static inline void config_sctlr_el1(u32 clear, u32 set)
+ 	val |= set;
+ 	asm volatile("msr sctlr_el1, %0" : : "r" (val));
+ }
++
++/*
++ * Unlike read_cpuid, calls to read_sysreg are never expected to be
++ * optimized away or replaced with synthetic values.
++ */
++#define read_sysreg(r) ({					\
++	u64 __val;						\
++	asm volatile("mrs %0, " __stringify(r) : "=r" (__val));	\
++	__val;							\
++})
++
++#define write_sysreg(v, r) do {					\
++	u64 __val = (u64)v;					\
++	asm volatile("msr " __stringify(r) ", %0"		\
++		     : : "r" (__val));				\
++} while (0)
++
+ #endif
+ 
+ #endif	/* __ASM_SYSREG_H */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0036-KVM-arm-arm64-vgic-v3-Make-the-LR-indexing-macro-pub.patch b/tools/kdump/0036-KVM-arm-arm64-vgic-v3-Make-the-LR-indexing-macro-pub.patch
new file mode 100644
index 0000000..ae0232d
--- /dev/null
+++ b/tools/kdump/0036-KVM-arm-arm64-vgic-v3-Make-the-LR-indexing-macro-pub.patch
@@ -0,0 +1,73 @@
+From 078d9a778dad82dc0a3002a015aaa7b5ce77083e Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Tue, 1 Dec 2015 13:48:56 +0000
+Subject: [PATCH 036/123] KVM: arm/arm64: vgic-v3: Make the LR indexing macro
+ public
+
+We store GICv3 LRs in reverse order so that the CPU can save/restore
+them in rever order as well (don't ask why, the design is crazy),
+and yet generate memory traffic that doesn't completely suck.
+
+We need this macro to be available to the C version of save/restore.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 3c13b8f435acb452eac62d966148a8b6fa92151f)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ include/kvm/arm_vgic.h |  6 ++++++
+ virt/kvm/arm/vgic-v3.c | 10 ++--------
+ 2 files changed, 8 insertions(+), 8 deletions(-)
+
+diff --git a/include/kvm/arm_vgic.h b/include/kvm/arm_vgic.h
+index d2f4147..13a3d53 100644
+--- a/include/kvm/arm_vgic.h
++++ b/include/kvm/arm_vgic.h
+@@ -279,6 +279,12 @@ struct vgic_v2_cpu_if {
+ 	u32		vgic_lr[VGIC_V2_MAX_LRS];
+ };
+ 
++/*
++ * LRs are stored in reverse order in memory. make sure we index them
++ * correctly.
++ */
++#define VGIC_V3_LR_INDEX(lr)		(VGIC_V3_MAX_LRS - 1 - lr)
++
+ struct vgic_v3_cpu_if {
+ #ifdef CONFIG_KVM_ARM_VGIC_V3
+ 	u32		vgic_hcr;
+diff --git a/virt/kvm/arm/vgic-v3.c b/virt/kvm/arm/vgic-v3.c
+index 487d635..3813d23 100644
+--- a/virt/kvm/arm/vgic-v3.c
++++ b/virt/kvm/arm/vgic-v3.c
+@@ -36,18 +36,12 @@
+ #define GICH_LR_PHYSID_CPUID		(7UL << GICH_LR_PHYSID_CPUID_SHIFT)
+ #define ICH_LR_VIRTUALID_MASK		(BIT_ULL(32) - 1)
+ 
+-/*
+- * LRs are stored in reverse order in memory. make sure we index them
+- * correctly.
+- */
+-#define LR_INDEX(lr)			(VGIC_V3_MAX_LRS - 1 - lr)
+-
+ static u32 ich_vtr_el2;
+ 
+ static struct vgic_lr vgic_v3_get_lr(const struct kvm_vcpu *vcpu, int lr)
+ {
+ 	struct vgic_lr lr_desc;
+-	u64 val = vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[LR_INDEX(lr)];
++	u64 val = vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[VGIC_V3_LR_INDEX(lr)];
+ 
+ 	if (vcpu->kvm->arch.vgic.vgic_model == KVM_DEV_TYPE_ARM_VGIC_V3)
+ 		lr_desc.irq = val & ICH_LR_VIRTUALID_MASK;
+@@ -111,7 +105,7 @@ static void vgic_v3_set_lr(struct kvm_vcpu *vcpu, int lr,
+ 		lr_val |= ((u64)lr_desc.hwirq) << ICH_LR_PHYS_ID_SHIFT;
+ 	}
+ 
+-	vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[LR_INDEX(lr)] = lr_val;
++	vcpu->arch.vgic_cpu.vgic_v3.vgic_lr[VGIC_V3_LR_INDEX(lr)] = lr_val;
+ 
+ 	if (!(lr_desc.state & LR_STATE_MASK))
+ 		vcpu->arch.vgic_cpu.vgic_v3.vgic_elrsr |= (1U << lr);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0037-arm64-KVM-Add-a-HYP-specific-header-file.patch b/tools/kdump/0037-arm64-KVM-Add-a-HYP-specific-header-file.patch
new file mode 100644
index 0000000..e665955
--- /dev/null
+++ b/tools/kdump/0037-arm64-KVM-Add-a-HYP-specific-header-file.patch
@@ -0,0 +1,61 @@
+From 8ac03662a0c347eb1fe07ec59e10f8a7b632a905 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Wed, 21 Oct 2015 10:09:49 +0100
+Subject: [PATCH 037/123] arm64: KVM: Add a HYP-specific header file
+
+In order to expose the various EL2 services that are private to
+the hypervisor, add a new hyp.h file.
+
+So far, it only contains mundane things such as section annotation
+and VA manipulation.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit c76a0a6695c61088c8d2e731e25305502666bf7d)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/hyp.h | 33 +++++++++++++++++++++++++++++++++
+ 1 file changed, 33 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/hyp.h
+
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+new file mode 100644
+index 0000000..057f483
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -0,0 +1,33 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#ifndef __ARM64_KVM_HYP_H__
++#define __ARM64_KVM_HYP_H__
++
++#include <linux/compiler.h>
++#include <linux/kvm_host.h>
++#include <asm/kvm_mmu.h>
++#include <asm/sysreg.h>
++
++#define __hyp_text __section(.hyp.text) notrace
++
++#define kern_hyp_va(v) (typeof(v))((unsigned long)(v) & HYP_PAGE_OFFSET_MASK)
++#define hyp_kern_va(v) (typeof(v))((unsigned long)(v) - HYP_PAGE_OFFSET \
++						      + PAGE_OFFSET)
++
++#endif /* __ARM64_KVM_HYP_H__ */
++
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0038-arm64-KVM-Implement-vgic-v2-save-restore.patch b/tools/kdump/0038-arm64-KVM-Implement-vgic-v2-save-restore.patch
new file mode 100644
index 0000000..63cfcfb
--- /dev/null
+++ b/tools/kdump/0038-arm64-KVM-Implement-vgic-v2-save-restore.patch
@@ -0,0 +1,149 @@
+From d412c4dd32551179d2f401e564d5bf2ef7d783e5 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 19 Oct 2015 15:50:37 +0100
+Subject: [PATCH 038/123] arm64: KVM: Implement vgic-v2 save/restore
+
+Implement the vgic-v2 save restore (mostly) as a direct translation
+of the assembly code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 06282fd2c2bf61619649a2b13e4a08556598a64c)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/Makefile         |  1 +
+ arch/arm64/kvm/hyp/Makefile     |  5 +++
+ arch/arm64/kvm/hyp/hyp.h        |  3 ++
+ arch/arm64/kvm/hyp/vgic-v2-sr.c | 84 +++++++++++++++++++++++++++++++++++++++++
+ 4 files changed, 93 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/Makefile
+ create mode 100644 arch/arm64/kvm/hyp/vgic-v2-sr.c
+
+diff --git a/arch/arm64/kvm/Makefile b/arch/arm64/kvm/Makefile
+index 1949fe5..d31e4e5 100644
+--- a/arch/arm64/kvm/Makefile
++++ b/arch/arm64/kvm/Makefile
+@@ -10,6 +10,7 @@ KVM=../../../virt/kvm
+ ARM=../../../arch/arm/kvm
+ 
+ obj-$(CONFIG_KVM_ARM_HOST) += kvm.o
++obj-$(CONFIG_KVM_ARM_HOST) += hyp/
+ 
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/kvm_main.o $(KVM)/coalesced_mmio.o $(KVM)/eventfd.o $(KVM)/vfio.o
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(ARM)/arm.o $(ARM)/mmu.o $(ARM)/mmio.o
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+new file mode 100644
+index 0000000..d8d5968
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -0,0 +1,5 @@
++#
++# Makefile for Kernel-based Virtual Machine module, HYP part
++#
++
++obj-$(CONFIG_KVM_ARM_HOST) += vgic-v2-sr.o
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index 057f483..ac63553 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -29,5 +29,8 @@
+ #define hyp_kern_va(v) (typeof(v))((unsigned long)(v) - HYP_PAGE_OFFSET \
+ 						      + PAGE_OFFSET)
+ 
++void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
++void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
++
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+diff --git a/arch/arm64/kvm/hyp/vgic-v2-sr.c b/arch/arm64/kvm/hyp/vgic-v2-sr.c
+new file mode 100644
+index 0000000..e717612
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/vgic-v2-sr.c
+@@ -0,0 +1,84 @@
++/*
++ * Copyright (C) 2012-2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/compiler.h>
++#include <linux/irqchip/arm-gic.h>
++#include <linux/kvm_host.h>
++
++#include <asm/kvm_mmu.h>
++
++#include "hyp.h"
++
++/* vcpu is already in the HYP VA space */
++void __hyp_text __vgic_v2_save_state(struct kvm_vcpu *vcpu)
++{
++	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
++	struct vgic_v2_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v2;
++	struct vgic_dist *vgic = &kvm->arch.vgic;
++	void __iomem *base = kern_hyp_va(vgic->vctrl_base);
++	u32 eisr0, eisr1, elrsr0, elrsr1;
++	int i, nr_lr;
++
++	if (!base)
++		return;
++
++	nr_lr = vcpu->arch.vgic_cpu.nr_lr;
++	cpu_if->vgic_vmcr = readl_relaxed(base + GICH_VMCR);
++	cpu_if->vgic_misr = readl_relaxed(base + GICH_MISR);
++	eisr0  = readl_relaxed(base + GICH_EISR0);
++	elrsr0 = readl_relaxed(base + GICH_ELRSR0);
++	if (unlikely(nr_lr > 32)) {
++		eisr1  = readl_relaxed(base + GICH_EISR1);
++		elrsr1 = readl_relaxed(base + GICH_ELRSR1);
++	} else {
++		eisr1 = elrsr1 = 0;
++	}
++#ifdef CONFIG_CPU_BIG_ENDIAN
++	cpu_if->vgic_eisr  = ((u64)eisr0 << 32) | eisr1;
++	cpu_if->vgic_elrsr = ((u64)elrsr0 << 32) | elrsr1;
++#else
++	cpu_if->vgic_eisr  = ((u64)eisr1 << 32) | eisr0;
++	cpu_if->vgic_elrsr = ((u64)elrsr1 << 32) | elrsr0;
++#endif
++	cpu_if->vgic_apr    = readl_relaxed(base + GICH_APR);
++
++	writel_relaxed(0, base + GICH_HCR);
++
++	for (i = 0; i < nr_lr; i++)
++		cpu_if->vgic_lr[i] = readl_relaxed(base + GICH_LR0 + (i * 4));
++}
++
++/* vcpu is already in the HYP VA space */
++void __hyp_text __vgic_v2_restore_state(struct kvm_vcpu *vcpu)
++{
++	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
++	struct vgic_v2_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v2;
++	struct vgic_dist *vgic = &kvm->arch.vgic;
++	void __iomem *base = kern_hyp_va(vgic->vctrl_base);
++	int i, nr_lr;
++
++	if (!base)
++		return;
++
++	writel_relaxed(cpu_if->vgic_hcr, base + GICH_HCR);
++	writel_relaxed(cpu_if->vgic_vmcr, base + GICH_VMCR);
++	writel_relaxed(cpu_if->vgic_apr, base + GICH_APR);
++
++	nr_lr = vcpu->arch.vgic_cpu.nr_lr;
++	for (i = 0; i < nr_lr; i++)
++		writel_relaxed(cpu_if->vgic_lr[i], base + GICH_LR0 + (i * 4));
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0039-arm64-KVM-Implement-timer-save-restore.patch b/tools/kdump/0039-arm64-KVM-Implement-timer-save-restore.patch
new file mode 100644
index 0000000..08ccbe6
--- /dev/null
+++ b/tools/kdump/0039-arm64-KVM-Implement-timer-save-restore.patch
@@ -0,0 +1,146 @@
+From b502c12c9eecc12a354c856480255bf47f74e1cd Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 19 Oct 2015 16:32:20 +0100
+Subject: [PATCH 039/123] arm64: KVM: Implement timer save/restore
+
+Implement the timer save restore as a direct translation of
+the assembly code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 1431af367e52b08038e78d346822966d968f1694)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kvm/hyp/Makefile
+	arch/arm64/kvm/hyp/hyp.h
+---
+ arch/arm64/kvm/hyp/Makefile          |  2 +
+ arch/arm64/kvm/hyp/hyp.h             |  6 +++
+ arch/arm64/kvm/hyp/timer-sr.c        | 71 ++++++++++++++++++++++++++++++++++++
+ include/clocksource/arm_arch_timer.h |  6 +++
+ 4 files changed, 85 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/timer-sr.c
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index d8d5968..455dc0a 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -3,3 +3,5 @@
+ #
+ 
+ obj-$(CONFIG_KVM_ARM_HOST) += vgic-v2-sr.o
++obj-$(CONFIG_KVM_ARM_HOST) += vgic-v3-sr.o
++obj-$(CONFIG_KVM_ARM_HOST) += timer-sr.o
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index ac63553..f213e46 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -32,5 +32,11 @@
+ void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
+ void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
+ 
++void __vgic_v3_save_state(struct kvm_vcpu *vcpu);
++void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
++
++void __timer_save_state(struct kvm_vcpu *vcpu);
++void __timer_restore_state(struct kvm_vcpu *vcpu);
++
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+diff --git a/arch/arm64/kvm/hyp/timer-sr.c b/arch/arm64/kvm/hyp/timer-sr.c
+new file mode 100644
+index 0000000..1051e5d
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/timer-sr.c
+@@ -0,0 +1,71 @@
++/*
++ * Copyright (C) 2012-2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <clocksource/arm_arch_timer.h>
++#include <linux/compiler.h>
++#include <linux/kvm_host.h>
++
++#include <asm/kvm_mmu.h>
++
++#include "hyp.h"
++
++/* vcpu is already in the HYP VA space */
++void __hyp_text __timer_save_state(struct kvm_vcpu *vcpu)
++{
++	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
++	struct arch_timer_cpu *timer = &vcpu->arch.timer_cpu;
++	u64 val;
++
++	if (kvm->arch.timer.enabled) {
++		timer->cntv_ctl = read_sysreg(cntv_ctl_el0);
++		timer->cntv_cval = read_sysreg(cntv_cval_el0);
++	}
++
++	/* Disable the virtual timer */
++	write_sysreg(0, cntv_ctl_el0);
++
++	/* Allow physical timer/counter access for the host */
++	val = read_sysreg(cnthctl_el2);
++	val |= CNTHCTL_EL1PCTEN | CNTHCTL_EL1PCEN;
++	write_sysreg(val, cnthctl_el2);
++
++	/* Clear cntvoff for the host */
++	write_sysreg(0, cntvoff_el2);
++}
++
++void __hyp_text __timer_restore_state(struct kvm_vcpu *vcpu)
++{
++	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
++	struct arch_timer_cpu *timer = &vcpu->arch.timer_cpu;
++	u64 val;
++
++	/*
++	 * Disallow physical timer access for the guest
++	 * Physical counter access is allowed
++	 */
++	val = read_sysreg(cnthctl_el2);
++	val &= ~CNTHCTL_EL1PCEN;
++	val |= CNTHCTL_EL1PCTEN;
++	write_sysreg(val, cnthctl_el2);
++
++	if (kvm->arch.timer.enabled) {
++		write_sysreg(kvm->arch.timer.cntvoff, cntvoff_el2);
++		write_sysreg(timer->cntv_cval, cntv_cval_el0);
++		isb();
++		write_sysreg(timer->cntv_ctl, cntv_ctl_el0);
++	}
++}
+diff --git a/include/clocksource/arm_arch_timer.h b/include/clocksource/arm_arch_timer.h
+index 9916d0e..25d0914 100644
+--- a/include/clocksource/arm_arch_timer.h
++++ b/include/clocksource/arm_arch_timer.h
+@@ -23,6 +23,12 @@
+ #define ARCH_TIMER_CTRL_IT_MASK		(1 << 1)
+ #define ARCH_TIMER_CTRL_IT_STAT		(1 << 2)
+ 
++#define CNTHCTL_EL1PCTEN		(1 << 0)
++#define CNTHCTL_EL1PCEN			(1 << 1)
++#define CNTHCTL_EVNTEN			(1 << 2)
++#define CNTHCTL_EVNTDIR			(1 << 3)
++#define CNTHCTL_EVNTI			(0xF << 4)
++
+ enum arch_timer_reg {
+ 	ARCH_TIMER_REG_CTRL,
+ 	ARCH_TIMER_REG_TVAL,
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0040-arm64-KVM-Implement-system-register-save-restore.patch b/tools/kdump/0040-arm64-KVM-Implement-system-register-save-restore.patch
new file mode 100644
index 0000000..b700921
--- /dev/null
+++ b/tools/kdump/0040-arm64-KVM-Implement-system-register-save-restore.patch
@@ -0,0 +1,140 @@
+From d62c690efd4bcf0d66cc5dc3af0dd1f931c45a1f Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 19 Oct 2015 18:02:48 +0100
+Subject: [PATCH 040/123] arm64: KVM: Implement system register save/restore
+
+Implement the system register save/restore as a direct translation of
+the assembly code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 6d6ec20fcf2830ca10c1b7c8efd7e2592c40e3d6)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile    |  1 +
+ arch/arm64/kvm/hyp/hyp.h       |  3 ++
+ arch/arm64/kvm/hyp/sysreg-sr.c | 90 ++++++++++++++++++++++++++++++++++++++++++
+ 3 files changed, 94 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/sysreg-sr.c
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index 455dc0a..ec94200 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -5,3 +5,4 @@
+ obj-$(CONFIG_KVM_ARM_HOST) += vgic-v2-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += vgic-v3-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += timer-sr.o
++obj-$(CONFIG_KVM_ARM_HOST) += sysreg-sr.o
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index f213e46..778d56d 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -38,5 +38,8 @@ void __vgic_v3_restore_state(struct kvm_vcpu *vcpu);
+ void __timer_save_state(struct kvm_vcpu *vcpu);
+ void __timer_restore_state(struct kvm_vcpu *vcpu);
+ 
++void __sysreg_save_state(struct kvm_cpu_context *ctxt);
++void __sysreg_restore_state(struct kvm_cpu_context *ctxt);
++
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+diff --git a/arch/arm64/kvm/hyp/sysreg-sr.c b/arch/arm64/kvm/hyp/sysreg-sr.c
+new file mode 100644
+index 0000000..add8fcb
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/sysreg-sr.c
+@@ -0,0 +1,90 @@
++/*
++ * Copyright (C) 2012-2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/compiler.h>
++#include <linux/kvm_host.h>
++
++#include <asm/kvm_mmu.h>
++
++#include "hyp.h"
++
++/* ctxt is already in the HYP VA space */
++void __hyp_text __sysreg_save_state(struct kvm_cpu_context *ctxt)
++{
++	ctxt->sys_regs[MPIDR_EL1]	= read_sysreg(vmpidr_el2);
++	ctxt->sys_regs[CSSELR_EL1]	= read_sysreg(csselr_el1);
++	ctxt->sys_regs[SCTLR_EL1]	= read_sysreg(sctlr_el1);
++	ctxt->sys_regs[ACTLR_EL1]	= read_sysreg(actlr_el1);
++	ctxt->sys_regs[CPACR_EL1]	= read_sysreg(cpacr_el1);
++	ctxt->sys_regs[TTBR0_EL1]	= read_sysreg(ttbr0_el1);
++	ctxt->sys_regs[TTBR1_EL1]	= read_sysreg(ttbr1_el1);
++	ctxt->sys_regs[TCR_EL1]		= read_sysreg(tcr_el1);
++	ctxt->sys_regs[ESR_EL1]		= read_sysreg(esr_el1);
++	ctxt->sys_regs[AFSR0_EL1]	= read_sysreg(afsr0_el1);
++	ctxt->sys_regs[AFSR1_EL1]	= read_sysreg(afsr1_el1);
++	ctxt->sys_regs[FAR_EL1]		= read_sysreg(far_el1);
++	ctxt->sys_regs[MAIR_EL1]	= read_sysreg(mair_el1);
++	ctxt->sys_regs[VBAR_EL1]	= read_sysreg(vbar_el1);
++	ctxt->sys_regs[CONTEXTIDR_EL1]	= read_sysreg(contextidr_el1);
++	ctxt->sys_regs[TPIDR_EL0]	= read_sysreg(tpidr_el0);
++	ctxt->sys_regs[TPIDRRO_EL0]	= read_sysreg(tpidrro_el0);
++	ctxt->sys_regs[TPIDR_EL1]	= read_sysreg(tpidr_el1);
++	ctxt->sys_regs[AMAIR_EL1]	= read_sysreg(amair_el1);
++	ctxt->sys_regs[CNTKCTL_EL1]	= read_sysreg(cntkctl_el1);
++	ctxt->sys_regs[PAR_EL1]		= read_sysreg(par_el1);
++	ctxt->sys_regs[MDSCR_EL1]	= read_sysreg(mdscr_el1);
++
++	ctxt->gp_regs.regs.sp		= read_sysreg(sp_el0);
++	ctxt->gp_regs.regs.pc		= read_sysreg(elr_el2);
++	ctxt->gp_regs.regs.pstate	= read_sysreg(spsr_el2);
++	ctxt->gp_regs.sp_el1		= read_sysreg(sp_el1);
++	ctxt->gp_regs.elr_el1		= read_sysreg(elr_el1);
++	ctxt->gp_regs.spsr[KVM_SPSR_EL1]= read_sysreg(spsr_el1);
++}
++
++void __hyp_text __sysreg_restore_state(struct kvm_cpu_context *ctxt)
++{
++	write_sysreg(ctxt->sys_regs[MPIDR_EL1],	  vmpidr_el2);
++	write_sysreg(ctxt->sys_regs[CSSELR_EL1],  csselr_el1);
++	write_sysreg(ctxt->sys_regs[SCTLR_EL1],	  sctlr_el1);
++	write_sysreg(ctxt->sys_regs[ACTLR_EL1],	  actlr_el1);
++	write_sysreg(ctxt->sys_regs[CPACR_EL1],	  cpacr_el1);
++	write_sysreg(ctxt->sys_regs[TTBR0_EL1],	  ttbr0_el1);
++	write_sysreg(ctxt->sys_regs[TTBR1_EL1],	  ttbr1_el1);
++	write_sysreg(ctxt->sys_regs[TCR_EL1],	  tcr_el1);
++	write_sysreg(ctxt->sys_regs[ESR_EL1],	  esr_el1);
++	write_sysreg(ctxt->sys_regs[AFSR0_EL1],	  afsr0_el1);
++	write_sysreg(ctxt->sys_regs[AFSR1_EL1],	  afsr1_el1);
++	write_sysreg(ctxt->sys_regs[FAR_EL1],	  far_el1);
++	write_sysreg(ctxt->sys_regs[MAIR_EL1],	  mair_el1);
++	write_sysreg(ctxt->sys_regs[VBAR_EL1],	  vbar_el1);
++	write_sysreg(ctxt->sys_regs[CONTEXTIDR_EL1], contextidr_el1);
++	write_sysreg(ctxt->sys_regs[TPIDR_EL0],	  tpidr_el0);
++	write_sysreg(ctxt->sys_regs[TPIDRRO_EL0], tpidrro_el0);
++	write_sysreg(ctxt->sys_regs[TPIDR_EL1],	  tpidr_el1);
++	write_sysreg(ctxt->sys_regs[AMAIR_EL1],	  amair_el1);
++	write_sysreg(ctxt->sys_regs[CNTKCTL_EL1], cntkctl_el1);
++	write_sysreg(ctxt->sys_regs[PAR_EL1],	  par_el1);
++	write_sysreg(ctxt->sys_regs[MDSCR_EL1],	  mdscr_el1);
++
++	write_sysreg(ctxt->gp_regs.regs.sp,	sp_el0);
++	write_sysreg(ctxt->gp_regs.regs.pc,	elr_el2);
++	write_sysreg(ctxt->gp_regs.regs.pstate,	spsr_el2);
++	write_sysreg(ctxt->gp_regs.sp_el1,	sp_el1);
++	write_sysreg(ctxt->gp_regs.elr_el1,	elr_el1);
++	write_sysreg(ctxt->gp_regs.spsr[KVM_SPSR_EL1], spsr_el1);
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0041-arm64-KVM-Implement-32bit-system-register-save-resto.patch b/tools/kdump/0041-arm64-KVM-Implement-32bit-system-register-save-resto.patch
new file mode 100644
index 0000000..19f31ac
--- /dev/null
+++ b/tools/kdump/0041-arm64-KVM-Implement-32bit-system-register-save-resto.patch
@@ -0,0 +1,89 @@
+From bfdb1ec8271693fe0af56de91be48cdb9dde4fea Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 19 Oct 2015 19:28:29 +0100
+Subject: [PATCH 041/123] arm64: KVM: Implement 32bit system register
+ save/restore
+
+Implement the 32bit system register save/restore as a direct
+translation of the assembly code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit c209ec85a2a7d2fd38bca0a44b7e70abd079c178)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/hyp.h       |  2 ++
+ arch/arm64/kvm/hyp/sysreg-sr.c | 47 ++++++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 49 insertions(+)
+
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index 778d56d..bffd308 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -40,6 +40,8 @@ void __timer_restore_state(struct kvm_vcpu *vcpu);
+ 
+ void __sysreg_save_state(struct kvm_cpu_context *ctxt);
+ void __sysreg_restore_state(struct kvm_cpu_context *ctxt);
++void __sysreg32_save_state(struct kvm_vcpu *vcpu);
++void __sysreg32_restore_state(struct kvm_vcpu *vcpu);
+ 
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+diff --git a/arch/arm64/kvm/hyp/sysreg-sr.c b/arch/arm64/kvm/hyp/sysreg-sr.c
+index add8fcb..eb05afb 100644
+--- a/arch/arm64/kvm/hyp/sysreg-sr.c
++++ b/arch/arm64/kvm/hyp/sysreg-sr.c
+@@ -88,3 +88,50 @@ void __hyp_text __sysreg_restore_state(struct kvm_cpu_context *ctxt)
+ 	write_sysreg(ctxt->gp_regs.elr_el1,	elr_el1);
+ 	write_sysreg(ctxt->gp_regs.spsr[KVM_SPSR_EL1], spsr_el1);
+ }
++
++void __hyp_text __sysreg32_save_state(struct kvm_vcpu *vcpu)
++{
++	u64 *spsr, *sysreg;
++
++	if (read_sysreg(hcr_el2) & HCR_RW)
++		return;
++
++	spsr = vcpu->arch.ctxt.gp_regs.spsr;
++	sysreg = vcpu->arch.ctxt.sys_regs;
++
++	spsr[KVM_SPSR_ABT] = read_sysreg(spsr_abt);
++	spsr[KVM_SPSR_UND] = read_sysreg(spsr_und);
++	spsr[KVM_SPSR_IRQ] = read_sysreg(spsr_irq);
++	spsr[KVM_SPSR_FIQ] = read_sysreg(spsr_fiq);
++
++	sysreg[DACR32_EL2] = read_sysreg(dacr32_el2);
++	sysreg[IFSR32_EL2] = read_sysreg(ifsr32_el2);
++
++	if (!(read_sysreg(cptr_el2) & CPTR_EL2_TFP))
++		sysreg[FPEXC32_EL2] = read_sysreg(fpexc32_el2);
++
++	if (vcpu->arch.debug_flags & KVM_ARM64_DEBUG_DIRTY)
++		sysreg[DBGVCR32_EL2] = read_sysreg(dbgvcr32_el2);
++}
++
++void __hyp_text __sysreg32_restore_state(struct kvm_vcpu *vcpu)
++{
++	u64 *spsr, *sysreg;
++
++	if (read_sysreg(hcr_el2) & HCR_RW)
++		return;
++
++	spsr = vcpu->arch.ctxt.gp_regs.spsr;
++	sysreg = vcpu->arch.ctxt.sys_regs;
++
++	write_sysreg(spsr[KVM_SPSR_ABT], spsr_abt);
++	write_sysreg(spsr[KVM_SPSR_UND], spsr_und);
++	write_sysreg(spsr[KVM_SPSR_IRQ], spsr_irq);
++	write_sysreg(spsr[KVM_SPSR_FIQ], spsr_fiq);
++
++	write_sysreg(sysreg[DACR32_EL2], dacr32_el2);
++	write_sysreg(sysreg[IFSR32_EL2], ifsr32_el2);
++
++	if (vcpu->arch.debug_flags & KVM_ARM64_DEBUG_DIRTY)
++		write_sysreg(sysreg[DBGVCR32_EL2], dbgvcr32_el2);
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0042-arm64-KVM-Implement-debug-save-restore.patch b/tools/kdump/0042-arm64-KVM-Implement-debug-save-restore.patch
new file mode 100644
index 0000000..69309d2
--- /dev/null
+++ b/tools/kdump/0042-arm64-KVM-Implement-debug-save-restore.patch
@@ -0,0 +1,198 @@
+From d88dd8ad75071ab79266f9c234cff541a5c497ea Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 19 Oct 2015 21:02:46 +0100
+Subject: [PATCH 042/123] arm64: KVM: Implement debug save/restore
+MIME-Version: 1.0
+Content-Type: text/plain; charset=UTF-8
+Content-Transfer-Encoding: 8bit
+
+Implement the debug save restore as a direct translation of
+the assembly code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Tested-by: Alex Benne <alex.bennee@linaro.org>
+Reviewed-by: Alex Benne <alex.bennee@linaro.org>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 8eb992674c9e69d57af199f36b6455dbc00ac9f9)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile   |   1 +
+ arch/arm64/kvm/hyp/debug-sr.c | 137 ++++++++++++++++++++++++++++++++++++++++++
+ arch/arm64/kvm/hyp/hyp.h      |   9 +++
+ 3 files changed, 147 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/debug-sr.c
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index ec94200..ec14cac 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -6,3 +6,4 @@ obj-$(CONFIG_KVM_ARM_HOST) += vgic-v2-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += vgic-v3-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += timer-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += sysreg-sr.o
++obj-$(CONFIG_KVM_ARM_HOST) += debug-sr.o
+diff --git a/arch/arm64/kvm/hyp/debug-sr.c b/arch/arm64/kvm/hyp/debug-sr.c
+new file mode 100644
+index 0000000..7848322
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/debug-sr.c
+@@ -0,0 +1,137 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/compiler.h>
++#include <linux/kvm_host.h>
++
++#include <asm/kvm_mmu.h>
++
++#include "hyp.h"
++
++#define read_debug(r,n)		read_sysreg(r##n##_el1)
++#define write_debug(v,r,n)	write_sysreg(v, r##n##_el1)
++
++#define save_debug(ptr,reg,nr)						\
++	switch (nr) {							\
++	case 15:	ptr[15] = read_debug(reg, 15);			\
++	case 14:	ptr[14] = read_debug(reg, 14);			\
++	case 13:	ptr[13] = read_debug(reg, 13);			\
++	case 12:	ptr[12] = read_debug(reg, 12);			\
++	case 11:	ptr[11] = read_debug(reg, 11);			\
++	case 10:	ptr[10] = read_debug(reg, 10);			\
++	case 9:		ptr[9] = read_debug(reg, 9);			\
++	case 8:		ptr[8] = read_debug(reg, 8);			\
++	case 7:		ptr[7] = read_debug(reg, 7);			\
++	case 6:		ptr[6] = read_debug(reg, 6);			\
++	case 5:		ptr[5] = read_debug(reg, 5);			\
++	case 4:		ptr[4] = read_debug(reg, 4);			\
++	case 3:		ptr[3] = read_debug(reg, 3);			\
++	case 2:		ptr[2] = read_debug(reg, 2);			\
++	case 1:		ptr[1] = read_debug(reg, 1);			\
++	default:	ptr[0] = read_debug(reg, 0);			\
++	}
++
++#define restore_debug(ptr,reg,nr)					\
++	switch (nr) {							\
++	case 15:	write_debug(ptr[15], reg, 15);			\
++	case 14:	write_debug(ptr[14], reg, 14);			\
++	case 13:	write_debug(ptr[13], reg, 13);			\
++	case 12:	write_debug(ptr[12], reg, 12);			\
++	case 11:	write_debug(ptr[11], reg, 11);			\
++	case 10:	write_debug(ptr[10], reg, 10);			\
++	case 9:		write_debug(ptr[9], reg, 9);			\
++	case 8:		write_debug(ptr[8], reg, 8);			\
++	case 7:		write_debug(ptr[7], reg, 7);			\
++	case 6:		write_debug(ptr[6], reg, 6);			\
++	case 5:		write_debug(ptr[5], reg, 5);			\
++	case 4:		write_debug(ptr[4], reg, 4);			\
++	case 3:		write_debug(ptr[3], reg, 3);			\
++	case 2:		write_debug(ptr[2], reg, 2);			\
++	case 1:		write_debug(ptr[1], reg, 1);			\
++	default:	write_debug(ptr[0], reg, 0);			\
++	}
++
++void __hyp_text __debug_save_state(struct kvm_vcpu *vcpu,
++				   struct kvm_guest_debug_arch *dbg,
++				   struct kvm_cpu_context *ctxt)
++{
++	u64 aa64dfr0;
++	int brps, wrps;
++
++	if (!(vcpu->arch.debug_flags & KVM_ARM64_DEBUG_DIRTY))
++		return;
++
++	aa64dfr0 = read_sysreg(id_aa64dfr0_el1);
++	brps = (aa64dfr0 >> 12) & 0xf;
++	wrps = (aa64dfr0 >> 20) & 0xf;
++
++	save_debug(dbg->dbg_bcr, dbgbcr, brps);
++	save_debug(dbg->dbg_bvr, dbgbvr, brps);
++	save_debug(dbg->dbg_wcr, dbgwcr, wrps);
++	save_debug(dbg->dbg_wvr, dbgwvr, wrps);
++
++	ctxt->sys_regs[MDCCINT_EL1] = read_sysreg(mdccint_el1);
++}
++
++void __hyp_text __debug_restore_state(struct kvm_vcpu *vcpu,
++				      struct kvm_guest_debug_arch *dbg,
++				      struct kvm_cpu_context *ctxt)
++{
++	u64 aa64dfr0;
++	int brps, wrps;
++
++	if (!(vcpu->arch.debug_flags & KVM_ARM64_DEBUG_DIRTY))
++		return;
++
++	aa64dfr0 = read_sysreg(id_aa64dfr0_el1);
++
++	brps = (aa64dfr0 >> 12) & 0xf;
++	wrps = (aa64dfr0 >> 20) & 0xf;
++
++	restore_debug(dbg->dbg_bcr, dbgbcr, brps);
++	restore_debug(dbg->dbg_bvr, dbgbvr, brps);
++	restore_debug(dbg->dbg_wcr, dbgwcr, wrps);
++	restore_debug(dbg->dbg_wvr, dbgwvr, wrps);
++
++	write_sysreg(ctxt->sys_regs[MDCCINT_EL1], mdccint_el1);
++}
++
++void __hyp_text __debug_cond_save_host_state(struct kvm_vcpu *vcpu)
++{
++	/* If any of KDE, MDE or KVM_ARM64_DEBUG_DIRTY is set, perform
++	 * a full save/restore cycle. */
++	if ((vcpu->arch.ctxt.sys_regs[MDSCR_EL1] & DBG_MDSCR_KDE) ||
++	    (vcpu->arch.ctxt.sys_regs[MDSCR_EL1] & DBG_MDSCR_MDE))
++		vcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;
++
++	__debug_save_state(vcpu, &vcpu->arch.host_debug_state,
++			   kern_hyp_va(vcpu->arch.host_cpu_context));
++}
++
++void __hyp_text __debug_cond_restore_host_state(struct kvm_vcpu *vcpu)
++{
++	__debug_restore_state(vcpu, &vcpu->arch.host_debug_state,
++			      kern_hyp_va(vcpu->arch.host_cpu_context));
++
++	if (vcpu->arch.debug_flags & KVM_ARM64_DEBUG_DIRTY)
++		vcpu->arch.debug_flags &= ~KVM_ARM64_DEBUG_DIRTY;
++}
++
++u32 __hyp_text __debug_read_mdcr_el2(void)
++{
++	return read_sysreg(mdcr_el2);
++}
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index bffd308..454e46f 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -43,5 +43,14 @@ void __sysreg_restore_state(struct kvm_cpu_context *ctxt);
+ void __sysreg32_save_state(struct kvm_vcpu *vcpu);
+ void __sysreg32_restore_state(struct kvm_vcpu *vcpu);
+ 
++void __debug_save_state(struct kvm_vcpu *vcpu,
++			struct kvm_guest_debug_arch *dbg,
++			struct kvm_cpu_context *ctxt);
++void __debug_restore_state(struct kvm_vcpu *vcpu,
++			   struct kvm_guest_debug_arch *dbg,
++			   struct kvm_cpu_context *ctxt);
++void __debug_cond_save_host_state(struct kvm_vcpu *vcpu);
++void __debug_cond_restore_host_state(struct kvm_vcpu *vcpu);
++
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0043-arm64-KVM-Implement-guest-entry.patch b/tools/kdump/0043-arm64-KVM-Implement-guest-entry.patch
new file mode 100644
index 0000000..1e4862a
--- /dev/null
+++ b/tools/kdump/0043-arm64-KVM-Implement-guest-entry.patch
@@ -0,0 +1,180 @@
+From 9bcb00c7b6d000cf18753192b34f17b52c97850a Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Thu, 22 Oct 2015 08:32:18 +0100
+Subject: [PATCH 043/123] arm64: KVM: Implement guest entry
+
+Contrary to the previous patch, the guest entry is fairly different
+from its assembly counterpart, mostly because it is only concerned
+with saving/restoring the GP registers, and nothing else.
+
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit b97b66c14b96ab562e4fd516d804c5cd05c0529e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile |   1 +
+ arch/arm64/kvm/hyp/entry.S  | 130 ++++++++++++++++++++++++++++++++++++++++++++
+ arch/arm64/kvm/hyp/hyp.h    |   2 +
+ 3 files changed, 133 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/entry.S
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index ec14cac..1e1ff06 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -7,3 +7,4 @@ obj-$(CONFIG_KVM_ARM_HOST) += vgic-v3-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += timer-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += sysreg-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += debug-sr.o
++obj-$(CONFIG_KVM_ARM_HOST) += entry.o
+diff --git a/arch/arm64/kvm/hyp/entry.S b/arch/arm64/kvm/hyp/entry.S
+new file mode 100644
+index 0000000..ff19695
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/entry.S
+@@ -0,0 +1,130 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/linkage.h>
++
++#include <asm/asm-offsets.h>
++#include <asm/assembler.h>
++#include <asm/fpsimdmacros.h>
++#include <asm/kvm.h>
++#include <asm/kvm_arm.h>
++#include <asm/kvm_asm.h>
++#include <asm/kvm_mmu.h>
++
++#define CPU_GP_REG_OFFSET(x)	(CPU_GP_REGS + x)
++#define CPU_XREG_OFFSET(x)	CPU_GP_REG_OFFSET(CPU_USER_PT_REGS + 8*x)
++
++	.text
++	.pushsection	.hyp.text, "ax"
++
++.macro save_callee_saved_regs ctxt
++	stp	x19, x20, [\ctxt, #CPU_XREG_OFFSET(19)]
++	stp	x21, x22, [\ctxt, #CPU_XREG_OFFSET(21)]
++	stp	x23, x24, [\ctxt, #CPU_XREG_OFFSET(23)]
++	stp	x25, x26, [\ctxt, #CPU_XREG_OFFSET(25)]
++	stp	x27, x28, [\ctxt, #CPU_XREG_OFFSET(27)]
++	stp	x29, lr,  [\ctxt, #CPU_XREG_OFFSET(29)]
++.endm
++
++.macro restore_callee_saved_regs ctxt
++	ldp	x19, x20, [\ctxt, #CPU_XREG_OFFSET(19)]
++	ldp	x21, x22, [\ctxt, #CPU_XREG_OFFSET(21)]
++	ldp	x23, x24, [\ctxt, #CPU_XREG_OFFSET(23)]
++	ldp	x25, x26, [\ctxt, #CPU_XREG_OFFSET(25)]
++	ldp	x27, x28, [\ctxt, #CPU_XREG_OFFSET(27)]
++	ldp	x29, lr,  [\ctxt, #CPU_XREG_OFFSET(29)]
++.endm
++
++/*
++ * u64 __guest_enter(struct kvm_vcpu *vcpu,
++ *		     struct kvm_cpu_context *host_ctxt);
++ */
++ENTRY(__guest_enter)
++	// x0: vcpu
++	// x1: host/guest context
++	// x2-x18: clobbered by macros
++
++	// Store the host regs
++	save_callee_saved_regs x1
++
++	// Preserve vcpu & host_ctxt for use at exit time
++	stp	x0, x1, [sp, #-16]!
++
++	add	x1, x0, #VCPU_CONTEXT
++
++	// Prepare x0-x1 for later restore by pushing them onto the stack
++	ldp	x2, x3, [x1, #CPU_XREG_OFFSET(0)]
++	stp	x2, x3, [sp, #-16]!
++
++	// x2-x18
++	ldp	x2, x3,   [x1, #CPU_XREG_OFFSET(2)]
++	ldp	x4, x5,   [x1, #CPU_XREG_OFFSET(4)]
++	ldp	x6, x7,   [x1, #CPU_XREG_OFFSET(6)]
++	ldp	x8, x9,   [x1, #CPU_XREG_OFFSET(8)]
++	ldp	x10, x11, [x1, #CPU_XREG_OFFSET(10)]
++	ldp	x12, x13, [x1, #CPU_XREG_OFFSET(12)]
++	ldp	x14, x15, [x1, #CPU_XREG_OFFSET(14)]
++	ldp	x16, x17, [x1, #CPU_XREG_OFFSET(16)]
++	ldr	x18,      [x1, #CPU_XREG_OFFSET(18)]
++
++	// x19-x29, lr
++	restore_callee_saved_regs x1
++
++	// Last bits of the 64bit state
++	ldp	x0, x1, [sp], #16
++
++	// Do not touch any register after this!
++	eret
++ENDPROC(__guest_enter)
++
++ENTRY(__guest_exit)
++	// x0: vcpu
++	// x1: return code
++	// x2-x3: free
++	// x4-x29,lr: vcpu regs
++	// vcpu x0-x3 on the stack
++
++	add	x2, x0, #VCPU_CONTEXT
++
++	stp	x4, x5,   [x2, #CPU_XREG_OFFSET(4)]
++	stp	x6, x7,   [x2, #CPU_XREG_OFFSET(6)]
++	stp	x8, x9,   [x2, #CPU_XREG_OFFSET(8)]
++	stp	x10, x11, [x2, #CPU_XREG_OFFSET(10)]
++	stp	x12, x13, [x2, #CPU_XREG_OFFSET(12)]
++	stp	x14, x15, [x2, #CPU_XREG_OFFSET(14)]
++	stp	x16, x17, [x2, #CPU_XREG_OFFSET(16)]
++	str	x18,      [x2, #CPU_XREG_OFFSET(18)]
++
++	ldp	x6, x7, [sp], #16	// x2, x3
++	ldp	x4, x5, [sp], #16	// x0, x1
++
++	stp	x4, x5, [x2, #CPU_XREG_OFFSET(0)]
++	stp	x6, x7, [x2, #CPU_XREG_OFFSET(2)]
++
++	save_callee_saved_regs x2
++
++	// Restore vcpu & host_ctxt from the stack
++	// (preserving return code in x1)
++	ldp	x0, x2, [sp], #16
++	// Now restore the host regs
++	restore_callee_saved_regs x2
++
++	mov	x0, x1
++	ret
++ENDPROC(__guest_exit)
++
++	/* Insert fault handling here */
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index 454e46f..0809653 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -52,5 +52,7 @@ void __debug_restore_state(struct kvm_vcpu *vcpu,
+ void __debug_cond_save_host_state(struct kvm_vcpu *vcpu);
+ void __debug_cond_restore_host_state(struct kvm_vcpu *vcpu);
+ 
++u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
++
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0044-arm64-KVM-Add-patchable-function-selector.patch b/tools/kdump/0044-arm64-KVM-Add-patchable-function-selector.patch
new file mode 100644
index 0000000..6319087
--- /dev/null
+++ b/tools/kdump/0044-arm64-KVM-Add-patchable-function-selector.patch
@@ -0,0 +1,59 @@
+From 1e9d78cf5842586f0dbeec9bb16fa235a564e7a6 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Wed, 28 Oct 2015 08:45:37 +0000
+Subject: [PATCH 044/123] arm64: KVM: Add patchable function selector
+
+KVM so far relies on code patching, and is likely to use it more
+in the future. The main issue is that our alternative system works
+at the instruction level, while we'd like to have alternatives at
+the function level.
+
+In order to cope with this, add the "hyp_alternate_select" macro that
+outputs a brief sequence of code that in turn can be patched, allowing
+an alternative function to be selected.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit c1bf6e18e97e7ead77371d4251f8ef1567455584)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/hyp.h | 24 ++++++++++++++++++++++++
+ 1 file changed, 24 insertions(+)
+
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index 0809653..73419a7 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -29,6 +29,30 @@
+ #define hyp_kern_va(v) (typeof(v))((unsigned long)(v) - HYP_PAGE_OFFSET \
+ 						      + PAGE_OFFSET)
+ 
++/**
++ * hyp_alternate_select - Generates patchable code sequences that are
++ * used to switch between two implementations of a function, depending
++ * on the availability of a feature.
++ *
++ * @fname: a symbol name that will be defined as a function returning a
++ * function pointer whose type will match @orig and @alt
++ * @orig: A pointer to the default function, as returned by @fname when
++ * @cond doesn't hold
++ * @alt: A pointer to the alternate function, as returned by @fname
++ * when @cond holds
++ * @cond: a CPU feature (as described in asm/cpufeature.h)
++ */
++#define hyp_alternate_select(fname, orig, alt, cond)			\
++typeof(orig) * __hyp_text fname(void)					\
++{									\
++	typeof(alt) *val = orig;					\
++	asm volatile(ALTERNATIVE("nop		\n",			\
++				 "mov	%0, %1	\n",			\
++				 cond)					\
++		     : "+r" (val) : "r" (alt));				\
++	return val;							\
++}
++
+ void __vgic_v2_save_state(struct kvm_vcpu *vcpu);
+ void __vgic_v2_restore_state(struct kvm_vcpu *vcpu);
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0045-arm64-KVM-Implement-the-core-world-switch.patch b/tools/kdump/0045-arm64-KVM-Implement-the-core-world-switch.patch
new file mode 100644
index 0000000..75b9597
--- /dev/null
+++ b/tools/kdump/0045-arm64-KVM-Implement-the-core-world-switch.patch
@@ -0,0 +1,173 @@
+From 954b943b04e4f282883a23fb4ca4e63dcba1a174 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Wed, 21 Oct 2015 09:57:10 +0100
+Subject: [PATCH 045/123] arm64: KVM: Implement the core world switch
+
+Implement the core of the world switch in C. Not everything is there
+yet, and there is nothing to re-enter the world switch either.
+
+But this already outlines the code structure well enough.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit be901e9b15cd2c8e48dc089b4655ea4a076e66fd)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile |   1 +
+ arch/arm64/kvm/hyp/switch.c | 135 ++++++++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 136 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/switch.c
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index 1e1ff06..9c11b0f 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -8,3 +8,4 @@ obj-$(CONFIG_KVM_ARM_HOST) += timer-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += sysreg-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += debug-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += entry.o
++obj-$(CONFIG_KVM_ARM_HOST) += switch.o
+diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c
+new file mode 100644
+index 0000000..79f59c9
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/switch.c
+@@ -0,0 +1,135 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include "hyp.h"
++
++static void __hyp_text __activate_traps(struct kvm_vcpu *vcpu)
++{
++	u64 val;
++
++	/*
++	 * We are about to set CPTR_EL2.TFP to trap all floating point
++	 * register accesses to EL2, however, the ARM ARM clearly states that
++	 * traps are only taken to EL2 if the operation would not otherwise
++	 * trap to EL1.  Therefore, always make sure that for 32-bit guests,
++	 * we set FPEXC.EN to prevent traps to EL1, when setting the TFP bit.
++	 */
++	val = vcpu->arch.hcr_el2;
++	if (!(val & HCR_RW)) {
++		write_sysreg(1 << 30, fpexc32_el2);
++		isb();
++	}
++	write_sysreg(val, hcr_el2);
++	/* Trap on AArch32 cp15 c15 accesses (EL1 or EL0) */
++	write_sysreg(1 << 15, hstr_el2);
++	write_sysreg(CPTR_EL2_TTA | CPTR_EL2_TFP, cptr_el2);
++	write_sysreg(vcpu->arch.mdcr_el2, mdcr_el2);
++}
++
++static void __hyp_text __deactivate_traps(struct kvm_vcpu *vcpu)
++{
++	write_sysreg(HCR_RW, hcr_el2);
++	write_sysreg(0, hstr_el2);
++	write_sysreg(read_sysreg(mdcr_el2) & MDCR_EL2_HPMN_MASK, mdcr_el2);
++	write_sysreg(0, cptr_el2);
++}
++
++static void __hyp_text __activate_vm(struct kvm_vcpu *vcpu)
++{
++	struct kvm *kvm = kern_hyp_va(vcpu->kvm);
++	write_sysreg(kvm->arch.vttbr, vttbr_el2);
++}
++
++static void __hyp_text __deactivate_vm(struct kvm_vcpu *vcpu)
++{
++	write_sysreg(0, vttbr_el2);
++}
++
++static hyp_alternate_select(__vgic_call_save_state,
++			    __vgic_v2_save_state, __vgic_v3_save_state,
++			    ARM64_HAS_SYSREG_GIC_CPUIF);
++
++static hyp_alternate_select(__vgic_call_restore_state,
++			    __vgic_v2_restore_state, __vgic_v3_restore_state,
++			    ARM64_HAS_SYSREG_GIC_CPUIF);
++
++static void __hyp_text __vgic_save_state(struct kvm_vcpu *vcpu)
++{
++	__vgic_call_save_state()(vcpu);
++	write_sysreg(read_sysreg(hcr_el2) & ~HCR_INT_OVERRIDE, hcr_el2);
++}
++
++static void __hyp_text __vgic_restore_state(struct kvm_vcpu *vcpu)
++{
++	u64 val;
++
++	val = read_sysreg(hcr_el2);
++	val |= 	HCR_INT_OVERRIDE;
++	val |= vcpu->arch.irq_lines;
++	write_sysreg(val, hcr_el2);
++
++	__vgic_call_restore_state()(vcpu);
++}
++
++int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
++{
++	struct kvm_cpu_context *host_ctxt;
++	struct kvm_cpu_context *guest_ctxt;
++	u64 exit_code;
++
++	vcpu = kern_hyp_va(vcpu);
++	write_sysreg(vcpu, tpidr_el2);
++
++	host_ctxt = kern_hyp_va(vcpu->arch.host_cpu_context);
++	guest_ctxt = &vcpu->arch.ctxt;
++
++	__sysreg_save_state(host_ctxt);
++	__debug_cond_save_host_state(vcpu);
++
++	__activate_traps(vcpu);
++	__activate_vm(vcpu);
++
++	__vgic_restore_state(vcpu);
++	__timer_restore_state(vcpu);
++
++	/*
++	 * We must restore the 32-bit state before the sysregs, thanks
++	 * to Cortex-A57 erratum #852523.
++	 */
++	__sysreg32_restore_state(vcpu);
++	__sysreg_restore_state(guest_ctxt);
++	__debug_restore_state(vcpu, kern_hyp_va(vcpu->arch.debug_ptr), guest_ctxt);
++
++	/* Jump in the fire! */
++	exit_code = __guest_enter(vcpu, host_ctxt);
++	/* And we're baaack! */
++
++	__sysreg_save_state(guest_ctxt);
++	__sysreg32_save_state(vcpu);
++	__timer_save_state(vcpu);
++	__vgic_save_state(vcpu);
++
++	__deactivate_traps(vcpu);
++	__deactivate_vm(vcpu);
++
++	__sysreg_restore_state(host_ctxt);
++
++	__debug_save_state(vcpu, kern_hyp_va(vcpu->arch.debug_ptr), guest_ctxt);
++	__debug_cond_restore_host_state(vcpu);
++
++	return exit_code;
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0046-arm64-KVM-Implement-fpsimd-save-restore.patch b/tools/kdump/0046-arm64-KVM-Implement-fpsimd-save-restore.patch
new file mode 100644
index 0000000..490df6d
--- /dev/null
+++ b/tools/kdump/0046-arm64-KVM-Implement-fpsimd-save-restore.patch
@@ -0,0 +1,184 @@
+From 8e0275b4cd8e21f39a82b8688f1adb6acf56e036 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 26 Oct 2015 08:34:09 +0000
+Subject: [PATCH 046/123] arm64: KVM: Implement fpsimd save/restore
+
+Implement the fpsimd save restore, keeping the lazy part in
+assembler (as returning to C would be overkill).
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit c13d1683df16db16c91372177ca10c31677b5ed5)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile    |  1 +
+ arch/arm64/kvm/hyp/entry.S     | 32 +++++++++++++++++++++++++++++++-
+ arch/arm64/kvm/hyp/fpsimd.S    | 33 +++++++++++++++++++++++++++++++++
+ arch/arm64/kvm/hyp/hyp.h       |  7 +++++++
+ arch/arm64/kvm/hyp/switch.c    |  8 ++++++++
+ arch/arm64/kvm/hyp/sysreg-sr.c |  2 +-
+ 6 files changed, 81 insertions(+), 2 deletions(-)
+ create mode 100644 arch/arm64/kvm/hyp/fpsimd.S
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index 9c11b0f..56238d0 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -9,3 +9,4 @@ obj-$(CONFIG_KVM_ARM_HOST) += sysreg-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += debug-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += entry.o
+ obj-$(CONFIG_KVM_ARM_HOST) += switch.o
++obj-$(CONFIG_KVM_ARM_HOST) += fpsimd.o
+diff --git a/arch/arm64/kvm/hyp/entry.S b/arch/arm64/kvm/hyp/entry.S
+index ff19695..90cbf0f 100644
+--- a/arch/arm64/kvm/hyp/entry.S
++++ b/arch/arm64/kvm/hyp/entry.S
+@@ -27,6 +27,7 @@
+ 
+ #define CPU_GP_REG_OFFSET(x)	(CPU_GP_REGS + x)
+ #define CPU_XREG_OFFSET(x)	CPU_GP_REG_OFFSET(CPU_USER_PT_REGS + 8*x)
++#define CPU_SYSREG_OFFSET(x)	(CPU_SYSREGS + 8*x)
+ 
+ 	.text
+ 	.pushsection	.hyp.text, "ax"
+@@ -127,4 +128,33 @@ ENTRY(__guest_exit)
+ 	ret
+ ENDPROC(__guest_exit)
+ 
+-	/* Insert fault handling here */
++ENTRY(__fpsimd_guest_restore)
++	stp	x4, lr, [sp, #-16]!
++
++	mrs	x2, cptr_el2
++	bic	x2, x2, #CPTR_EL2_TFP
++	msr	cptr_el2, x2
++	isb
++
++	mrs	x3, tpidr_el2
++
++	ldr	x0, [x3, #VCPU_HOST_CONTEXT]
++	kern_hyp_va x0
++	add	x0, x0, #CPU_GP_REG_OFFSET(CPU_FP_REGS)
++	bl	__fpsimd_save_state
++
++	add	x2, x3, #VCPU_CONTEXT
++	add	x0, x2, #CPU_GP_REG_OFFSET(CPU_FP_REGS)
++	bl	__fpsimd_restore_state
++
++	mrs	x1, hcr_el2
++	tbnz	x1, #HCR_RW_SHIFT, 1f
++	ldr	x4, [x2, #CPU_SYSREG_OFFSET(FPEXC32_EL2)]
++	msr	fpexc32_el2, x4
++1:
++	ldp	x4, lr, [sp], #16
++	ldp	x2, x3, [sp], #16
++	ldp	x0, x1, [sp], #16
++
++	eret
++ENDPROC(__fpsimd_guest_restore)
+diff --git a/arch/arm64/kvm/hyp/fpsimd.S b/arch/arm64/kvm/hyp/fpsimd.S
+new file mode 100644
+index 0000000..da3f22c
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/fpsimd.S
+@@ -0,0 +1,33 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/linkage.h>
++
++#include <asm/fpsimdmacros.h>
++
++	.text
++	.pushsection	.hyp.text, "ax"
++
++ENTRY(__fpsimd_save_state)
++	fpsimd_save	x0, 1
++	ret
++ENDPROC(__fpsimd_save_state)
++
++ENTRY(__fpsimd_restore_state)
++	fpsimd_restore	x0, 1
++	ret
++ENDPROC(__fpsimd_restore_state)
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index 73419a7..70d4f69 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -76,6 +76,13 @@ void __debug_restore_state(struct kvm_vcpu *vcpu,
+ void __debug_cond_save_host_state(struct kvm_vcpu *vcpu);
+ void __debug_cond_restore_host_state(struct kvm_vcpu *vcpu);
+ 
++void __fpsimd_save_state(struct user_fpsimd_state *fp_regs);
++void __fpsimd_restore_state(struct user_fpsimd_state *fp_regs);
++static inline bool __fpsimd_enabled(void)
++{
++	return !(read_sysreg(cptr_el2) & CPTR_EL2_TFP);
++}
++
+ u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
+ 
+ #endif /* __ARM64_KVM_HYP_H__ */
+diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c
+index 79f59c9..608155f 100644
+--- a/arch/arm64/kvm/hyp/switch.c
++++ b/arch/arm64/kvm/hyp/switch.c
+@@ -89,6 +89,7 @@ int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpu_context *host_ctxt;
+ 	struct kvm_cpu_context *guest_ctxt;
++	bool fp_enabled;
+ 	u64 exit_code;
+ 
+ 	vcpu = kern_hyp_va(vcpu);
+@@ -118,6 +119,8 @@ int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ 	exit_code = __guest_enter(vcpu, host_ctxt);
+ 	/* And we're baaack! */
+ 
++	fp_enabled = __fpsimd_enabled();
++
+ 	__sysreg_save_state(guest_ctxt);
+ 	__sysreg32_save_state(vcpu);
+ 	__timer_save_state(vcpu);
+@@ -128,6 +131,11 @@ int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ 
+ 	__sysreg_restore_state(host_ctxt);
+ 
++	if (fp_enabled) {
++		__fpsimd_save_state(&guest_ctxt->gp_regs.fp_regs);
++		__fpsimd_restore_state(&host_ctxt->gp_regs.fp_regs);
++	}
++
+ 	__debug_save_state(vcpu, kern_hyp_va(vcpu->arch.debug_ptr), guest_ctxt);
+ 	__debug_cond_restore_host_state(vcpu);
+ 
+diff --git a/arch/arm64/kvm/hyp/sysreg-sr.c b/arch/arm64/kvm/hyp/sysreg-sr.c
+index eb05afb..3603541 100644
+--- a/arch/arm64/kvm/hyp/sysreg-sr.c
++++ b/arch/arm64/kvm/hyp/sysreg-sr.c
+@@ -107,7 +107,7 @@ void __hyp_text __sysreg32_save_state(struct kvm_vcpu *vcpu)
+ 	sysreg[DACR32_EL2] = read_sysreg(dacr32_el2);
+ 	sysreg[IFSR32_EL2] = read_sysreg(ifsr32_el2);
+ 
+-	if (!(read_sysreg(cptr_el2) & CPTR_EL2_TFP))
++	if (__fpsimd_enabled())
+ 		sysreg[FPEXC32_EL2] = read_sysreg(fpexc32_el2);
+ 
+ 	if (vcpu->arch.debug_flags & KVM_ARM64_DEBUG_DIRTY)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0047-arm64-KVM-Implement-TLB-handling.patch b/tools/kdump/0047-arm64-KVM-Implement-TLB-handling.patch
new file mode 100644
index 0000000..2edc525
--- /dev/null
+++ b/tools/kdump/0047-arm64-KVM-Implement-TLB-handling.patch
@@ -0,0 +1,122 @@
+From 81c4a84f91cdbf7dd6a147c7fce044d4d77ce45d Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Fri, 23 Oct 2015 08:26:37 +0100
+Subject: [PATCH 047/123] arm64: KVM: Implement TLB handling
+
+Implement the TLB handling as a direct translation of the assembly
+code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 5eec0a91e32a2862e86265532ae773820e0afd77)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile |  1 +
+ arch/arm64/kvm/hyp/entry.S  |  1 +
+ arch/arm64/kvm/hyp/tlb.c    | 73 +++++++++++++++++++++++++++++++++++++++++++++
+ 3 files changed, 75 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/tlb.c
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index 56238d0..1a529f5 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -10,3 +10,4 @@ obj-$(CONFIG_KVM_ARM_HOST) += debug-sr.o
+ obj-$(CONFIG_KVM_ARM_HOST) += entry.o
+ obj-$(CONFIG_KVM_ARM_HOST) += switch.o
+ obj-$(CONFIG_KVM_ARM_HOST) += fpsimd.o
++obj-$(CONFIG_KVM_ARM_HOST) += tlb.o
+diff --git a/arch/arm64/kvm/hyp/entry.S b/arch/arm64/kvm/hyp/entry.S
+index 90cbf0f..1050b2b 100644
+--- a/arch/arm64/kvm/hyp/entry.S
++++ b/arch/arm64/kvm/hyp/entry.S
+@@ -147,6 +147,7 @@ ENTRY(__fpsimd_guest_restore)
+ 	add	x0, x2, #CPU_GP_REG_OFFSET(CPU_FP_REGS)
+ 	bl	__fpsimd_restore_state
+ 
++	// Skip restoring fpexc32 for AArch64 guests
+ 	mrs	x1, hcr_el2
+ 	tbnz	x1, #HCR_RW_SHIFT, 1f
+ 	ldr	x4, [x2, #CPU_SYSREG_OFFSET(FPEXC32_EL2)]
+diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c
+new file mode 100644
+index 0000000..6fcb93a
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/tlb.c
+@@ -0,0 +1,73 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include "hyp.h"
++
++void __hyp_text __tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
++{
++	dsb(ishst);
++
++	/* Switch to requested VMID */
++	kvm = kern_hyp_va(kvm);
++	write_sysreg(kvm->arch.vttbr, vttbr_el2);
++	isb();
++
++	/*
++	 * We could do so much better if we had the VA as well.
++	 * Instead, we invalidate Stage-2 for this IPA, and the
++	 * whole of Stage-1. Weep...
++	 */
++	ipa >>= 12;
++	asm volatile("tlbi ipas2e1is, %0" : : "r" (ipa));
++
++	/*
++	 * We have to ensure completion of the invalidation at Stage-2,
++	 * since a table walk on another CPU could refill a TLB with a
++	 * complete (S1 + S2) walk based on the old Stage-2 mapping if
++	 * the Stage-1 invalidation happened first.
++	 */
++	dsb(ish);
++	asm volatile("tlbi vmalle1is" : : );
++	dsb(ish);
++	isb();
++
++	write_sysreg(0, vttbr_el2);
++}
++
++void __hyp_text __tlb_flush_vmid(struct kvm *kvm)
++{
++	dsb(ishst);
++
++	/* Switch to requested VMID */
++	kvm = kern_hyp_va(kvm);
++	write_sysreg(kvm->arch.vttbr, vttbr_el2);
++	isb();
++
++	asm volatile("tlbi vmalls12e1is" : : );
++	dsb(ish);
++	isb();
++
++	write_sysreg(0, vttbr_el2);
++}
++
++void __hyp_text __tlb_flush_vm_context(void)
++{
++	dsb(ishst);
++	asm volatile("tlbi alle1is	\n"
++		     "ic ialluis	  ": : );
++	dsb(ish);
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0048-arm64-KVM-HYP-mode-entry-points.patch b/tools/kdump/0048-arm64-KVM-HYP-mode-entry-points.patch
new file mode 100644
index 0000000..b4a7634
--- /dev/null
+++ b/tools/kdump/0048-arm64-KVM-HYP-mode-entry-points.patch
@@ -0,0 +1,239 @@
+From 8e9f85528e99239d91f78deb981b1b99b1ec3ed8 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Sun, 25 Oct 2015 08:01:56 +0000
+Subject: [PATCH 048/123] arm64: KVM: HYP mode entry points
+
+Add the entry points for HYP mode (both for hypercalls and
+exception handling).
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 2b28162cf65a6fe1c93d172675e4f2792792f17e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/Makefile    |   1 +
+ arch/arm64/kvm/hyp/hyp-entry.S | 203 +++++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 204 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/hyp-entry.S
+
+diff --git a/arch/arm64/kvm/hyp/Makefile b/arch/arm64/kvm/hyp/Makefile
+index 1a529f5..826032b 100644
+--- a/arch/arm64/kvm/hyp/Makefile
++++ b/arch/arm64/kvm/hyp/Makefile
+@@ -11,3 +11,4 @@ obj-$(CONFIG_KVM_ARM_HOST) += entry.o
+ obj-$(CONFIG_KVM_ARM_HOST) += switch.o
+ obj-$(CONFIG_KVM_ARM_HOST) += fpsimd.o
+ obj-$(CONFIG_KVM_ARM_HOST) += tlb.o
++obj-$(CONFIG_KVM_ARM_HOST) += hyp-entry.o
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+new file mode 100644
+index 0000000..818731a
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -0,0 +1,203 @@
++/*
++ * Copyright (C) 2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/linkage.h>
++
++#include <asm/alternative.h>
++#include <asm/assembler.h>
++#include <asm/asm-offsets.h>
++#include <asm/cpufeature.h>
++#include <asm/kvm_arm.h>
++#include <asm/kvm_asm.h>
++#include <asm/kvm_mmu.h>
++
++	.text
++	.pushsection	.hyp.text, "ax"
++
++.macro	save_x0_to_x3
++	stp	x0, x1, [sp, #-16]!
++	stp	x2, x3, [sp, #-16]!
++.endm
++
++.macro	restore_x0_to_x3
++	ldp	x2, x3, [sp], #16
++	ldp	x0, x1, [sp], #16
++.endm
++
++el1_sync:				// Guest trapped into EL2
++	save_x0_to_x3
++
++	mrs	x1, esr_el2
++	lsr	x2, x1, #ESR_ELx_EC_SHIFT
++
++	cmp	x2, #ESR_ELx_EC_HVC64
++	b.ne	el1_trap
++
++	mrs	x3, vttbr_el2		// If vttbr is valid, the 64bit guest
++	cbnz	x3, el1_trap		// called HVC
++
++	/* Here, we're pretty sure the host called HVC. */
++	restore_x0_to_x3
++
++	/* Check for __hyp_get_vectors */
++	cbnz	x0, 1f
++	mrs	x0, vbar_el2
++	b	2f
++
++1:	stp	lr, xzr, [sp, #-16]!
++
++	/*
++	 * Compute the function address in EL2, and shuffle the parameters.
++	 */
++	kern_hyp_va	x0
++	mov	lr, x0
++	mov	x0, x1
++	mov	x1, x2
++	mov	x2, x3
++	blr	lr
++
++	ldp	lr, xzr, [sp], #16
++2:	eret
++
++el1_trap:
++	/*
++	 * x1: ESR
++	 * x2: ESR_EC
++	 */
++
++	/* Guest accessed VFP/SIMD registers, save host, restore Guest */
++	cmp	x2, #ESR_ELx_EC_FP_ASIMD
++	b.eq	__fpsimd_guest_restore
++
++	cmp	x2, #ESR_ELx_EC_DABT_LOW
++	mov	x0, #ESR_ELx_EC_IABT_LOW
++	ccmp	x2, x0, #4, ne
++	b.ne	1f		// Not an abort we care about
++
++	/* This is an abort. Check for permission fault */
++alternative_if_not ARM64_WORKAROUND_834220
++	and	x2, x1, #ESR_ELx_FSC_TYPE
++	cmp	x2, #FSC_PERM
++	b.ne	1f		// Not a permission fault
++alternative_else
++	nop			// Use the permission fault path to
++	nop			// check for a valid S1 translation,
++	nop			// regardless of the ESR value.
++alternative_endif
++
++	/*
++	 * Check for Stage-1 page table walk, which is guaranteed
++	 * to give a valid HPFAR_EL2.
++	 */
++	tbnz	x1, #7, 1f	// S1PTW is set
++
++	/* Preserve PAR_EL1 */
++	mrs	x3, par_el1
++	stp	x3, xzr, [sp, #-16]!
++
++	/*
++	 * Permission fault, HPFAR_EL2 is invalid.
++	 * Resolve the IPA the hard way using the guest VA.
++	 * Stage-1 translation already validated the memory access rights.
++	 * As such, we can use the EL1 translation regime, and don't have
++	 * to distinguish between EL0 and EL1 access.
++	 */
++	mrs	x2, far_el2
++	at	s1e1r, x2
++	isb
++
++	/* Read result */
++	mrs	x3, par_el1
++	ldp	x0, xzr, [sp], #16	// Restore PAR_EL1 from the stack
++	msr	par_el1, x0
++	tbnz	x3, #0, 3f		// Bail out if we failed the translation
++	ubfx	x3, x3, #12, #36	// Extract IPA
++	lsl	x3, x3, #4		// and present it like HPFAR
++	b	2f
++
++1:	mrs	x3, hpfar_el2
++	mrs	x2, far_el2
++
++2:	mrs	x0, tpidr_el2
++	str	w1, [x0, #VCPU_ESR_EL2]
++	str	x2, [x0, #VCPU_FAR_EL2]
++	str	x3, [x0, #VCPU_HPFAR_EL2]
++
++	mov	x1, #ARM_EXCEPTION_TRAP
++	b	__guest_exit
++
++	/*
++	 * Translation failed. Just return to the guest and
++	 * let it fault again. Another CPU is probably playing
++	 * behind our back.
++	 */
++3:	restore_x0_to_x3
++
++	eret
++
++el1_irq:
++	save_x0_to_x3
++	mrs	x0, tpidr_el2
++	mov	x1, #ARM_EXCEPTION_IRQ
++	b	__guest_exit
++
++.macro invalid_vector	label, target = __kvm_hyp_panic
++	.align	2
++\label:
++	b \target
++ENDPROC(\label)
++.endm
++
++	/* None of these should ever happen */
++	invalid_vector	el2t_sync_invalid
++	invalid_vector	el2t_irq_invalid
++	invalid_vector	el2t_fiq_invalid
++	invalid_vector	el2t_error_invalid
++	invalid_vector	el2h_sync_invalid
++	invalid_vector	el2h_irq_invalid
++	invalid_vector	el2h_fiq_invalid
++	invalid_vector	el2h_error_invalid
++	invalid_vector	el1_sync_invalid
++	invalid_vector	el1_irq_invalid
++	invalid_vector	el1_fiq_invalid
++	invalid_vector	el1_error_invalid
++
++	.ltorg
++
++	.align 11
++
++ENTRY(__hyp_vector)
++	ventry	el2t_sync_invalid		// Synchronous EL2t
++	ventry	el2t_irq_invalid		// IRQ EL2t
++	ventry	el2t_fiq_invalid		// FIQ EL2t
++	ventry	el2t_error_invalid		// Error EL2t
++
++	ventry	el2h_sync_invalid		// Synchronous EL2h
++	ventry	el2h_irq_invalid		// IRQ EL2h
++	ventry	el2h_fiq_invalid		// FIQ EL2h
++	ventry	el2h_error_invalid		// Error EL2h
++
++	ventry	el1_sync			// Synchronous 64-bit EL1
++	ventry	el1_irq				// IRQ 64-bit EL1
++	ventry	el1_fiq_invalid			// FIQ 64-bit EL1
++	ventry	el1_error_invalid		// Error 64-bit EL1
++
++	ventry	el1_sync			// Synchronous 32-bit EL1
++	ventry	el1_irq				// IRQ 32-bit EL1
++	ventry	el1_fiq_invalid			// FIQ 32-bit EL1
++	ventry	el1_error_invalid		// Error 32-bit EL1
++ENDPROC(__hyp_vector)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0049-arm64-KVM-Add-panic-handling.patch b/tools/kdump/0049-arm64-KVM-Add-panic-handling.patch
new file mode 100644
index 0000000..ad04492
--- /dev/null
+++ b/tools/kdump/0049-arm64-KVM-Add-panic-handling.patch
@@ -0,0 +1,93 @@
+From 2366bf77323ff0c576652d0913a703054e255e5d Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Sun, 25 Oct 2015 15:21:52 +0000
+Subject: [PATCH 049/123] arm64: KVM: Add panic handling
+
+Add the panic handler, together with the small bits of assembly
+code to call the kernel's panic implementation.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 53fd5b6487e4438049a5da5e36dfb8edcf1fd789)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/hyp-entry.S | 11 ++++++++++-
+ arch/arm64/kvm/hyp/hyp.h       |  1 +
+ arch/arm64/kvm/hyp/switch.c    | 30 ++++++++++++++++++++++++++++++
+ 3 files changed, 41 insertions(+), 1 deletion(-)
+
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+index 818731a..8e58a3b 100644
+--- a/arch/arm64/kvm/hyp/hyp-entry.S
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -155,7 +155,16 @@ el1_irq:
+ 	mov	x1, #ARM_EXCEPTION_IRQ
+ 	b	__guest_exit
+ 
+-.macro invalid_vector	label, target = __kvm_hyp_panic
++ENTRY(__hyp_do_panic)
++	mov	lr, #(PSR_F_BIT | PSR_I_BIT | PSR_A_BIT | PSR_D_BIT |\
++		      PSR_MODE_EL1h)
++	msr	spsr_el2, lr
++	ldr	lr, =panic
++	msr	elr_el2, lr
++	eret
++ENDPROC(__hyp_do_panic)
++
++.macro invalid_vector	label, target = __hyp_panic
+ 	.align	2
+ \label:
+ 	b \target
+diff --git a/arch/arm64/kvm/hyp/hyp.h b/arch/arm64/kvm/hyp/hyp.h
+index 70d4f69..fb27517 100644
+--- a/arch/arm64/kvm/hyp/hyp.h
++++ b/arch/arm64/kvm/hyp/hyp.h
+@@ -84,6 +84,7 @@ static inline bool __fpsimd_enabled(void)
+ }
+ 
+ u64 __guest_enter(struct kvm_vcpu *vcpu, struct kvm_cpu_context *host_ctxt);
++void __noreturn __hyp_do_panic(unsigned long, ...);
+ 
+ #endif /* __ARM64_KVM_HYP_H__ */
+ 
+diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c
+index 608155f..b012870 100644
+--- a/arch/arm64/kvm/hyp/switch.c
++++ b/arch/arm64/kvm/hyp/switch.c
+@@ -141,3 +141,33 @@ int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ 
+ 	return exit_code;
+ }
++
++static const char __hyp_panic_string[] = "HYP panic:\nPS:%08llx PC:%016llx ESR:%08llx\nFAR:%016llx HPFAR:%016llx PAR:%016llx\nVCPU:%p\n";
++
++void __hyp_text __noreturn __hyp_panic(void)
++{
++	unsigned long str_va = (unsigned long)__hyp_panic_string;
++	u64 spsr = read_sysreg(spsr_el2);
++	u64 elr = read_sysreg(elr_el2);
++	u64 par = read_sysreg(par_el1);
++
++	if (read_sysreg(vttbr_el2)) {
++		struct kvm_vcpu *vcpu;
++		struct kvm_cpu_context *host_ctxt;
++
++		vcpu = (struct kvm_vcpu *)read_sysreg(tpidr_el2);
++		host_ctxt = kern_hyp_va(vcpu->arch.host_cpu_context);
++		__deactivate_traps(vcpu);
++		__deactivate_vm(vcpu);
++		__sysreg_restore_state(host_ctxt);
++	}
++
++	/* Call panic for real */
++	__hyp_do_panic(hyp_kern_va(str_va),
++		       spsr,  elr,
++		       read_sysreg(esr_el2),   read_sysreg(far_el2),
++		       read_sysreg(hpfar_el2), par,
++		       (void *)read_sysreg(tpidr_el2));
++
++	unreachable();
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0050-arm64-KVM-Implement-vgic-v3-save-restore.patch b/tools/kdump/0050-arm64-KVM-Implement-vgic-v3-save-restore.patch
new file mode 100644
index 0000000..e766826
--- /dev/null
+++ b/tools/kdump/0050-arm64-KVM-Implement-vgic-v3-save-restore.patch
@@ -0,0 +1,255 @@
+From 0a2b3edd18b042b76f6e133f34ef30c2470554d1 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 19 Oct 2015 15:50:58 +0100
+Subject: [PATCH 050/123] arm64: KVM: Implement vgic-v3 save/restore
+
+Implement the vgic-v3 save restore as a direct translation of
+the assembly code version.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit f68d2b1b73cc3d8f6eb189c11ce79a472ed27c42)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kvm/hyp/Makefile
+	arch/arm64/kvm/hyp/hyp.h
+---
+ arch/arm64/kvm/hyp/vgic-v3-sr.c | 226 ++++++++++++++++++++++++++++++++++++++++
+ 1 file changed, 226 insertions(+)
+ create mode 100644 arch/arm64/kvm/hyp/vgic-v3-sr.c
+
+diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
+new file mode 100644
+index 0000000..78d05f3
+--- /dev/null
++++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
+@@ -0,0 +1,226 @@
++/*
++ * Copyright (C) 2012-2015 - ARM Ltd
++ * Author: Marc Zyngier <marc.zyngier@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++
++#include <linux/compiler.h>
++#include <linux/irqchip/arm-gic-v3.h>
++#include <linux/kvm_host.h>
++
++#include <asm/kvm_mmu.h>
++
++#include "hyp.h"
++
++#define vtr_to_max_lr_idx(v)		((v) & 0xf)
++#define vtr_to_nr_pri_bits(v)		(((u32)(v) >> 29) + 1)
++
++#define read_gicreg(r)							\
++	({								\
++		u64 reg;						\
++		asm volatile("mrs_s %0, " __stringify(r) : "=r" (reg));	\
++		reg;							\
++	})
++
++#define write_gicreg(v,r)						\
++	do {								\
++		u64 __val = (v);					\
++		asm volatile("msr_s " __stringify(r) ", %0" : : "r" (__val));\
++	} while (0)
++
++/* vcpu is already in the HYP VA space */
++void __hyp_text __vgic_v3_save_state(struct kvm_vcpu *vcpu)
++{
++	struct vgic_v3_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v3;
++	u64 val;
++	u32 max_lr_idx, nr_pri_bits;
++
++	/*
++	 * Make sure stores to the GIC via the memory mapped interface
++	 * are now visible to the system register interface.
++	 */
++	dsb(st);
++
++	cpu_if->vgic_vmcr  = read_gicreg(ICH_VMCR_EL2);
++	cpu_if->vgic_misr  = read_gicreg(ICH_MISR_EL2);
++	cpu_if->vgic_eisr  = read_gicreg(ICH_EISR_EL2);
++	cpu_if->vgic_elrsr = read_gicreg(ICH_ELSR_EL2);
++
++	write_gicreg(0, ICH_HCR_EL2);
++	val = read_gicreg(ICH_VTR_EL2);
++	max_lr_idx = vtr_to_max_lr_idx(val);
++	nr_pri_bits = vtr_to_nr_pri_bits(val);
++
++	switch (max_lr_idx) {
++	case 15:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(15)] = read_gicreg(ICH_LR15_EL2);
++	case 14:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(14)] = read_gicreg(ICH_LR14_EL2);
++	case 13:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(13)] = read_gicreg(ICH_LR13_EL2);
++	case 12:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(12)] = read_gicreg(ICH_LR12_EL2);
++	case 11:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(11)] = read_gicreg(ICH_LR11_EL2);
++	case 10:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(10)] = read_gicreg(ICH_LR10_EL2);
++	case 9:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(9)] = read_gicreg(ICH_LR9_EL2);
++	case 8:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(8)] = read_gicreg(ICH_LR8_EL2);
++	case 7:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(7)] = read_gicreg(ICH_LR7_EL2);
++	case 6:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(6)] = read_gicreg(ICH_LR6_EL2);
++	case 5:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(5)] = read_gicreg(ICH_LR5_EL2);
++	case 4:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(4)] = read_gicreg(ICH_LR4_EL2);
++	case 3:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(3)] = read_gicreg(ICH_LR3_EL2);
++	case 2:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(2)] = read_gicreg(ICH_LR2_EL2);
++	case 1:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(1)] = read_gicreg(ICH_LR1_EL2);
++	case 0:
++		cpu_if->vgic_lr[VGIC_V3_LR_INDEX(0)] = read_gicreg(ICH_LR0_EL2);
++	}
++
++	switch (nr_pri_bits) {
++	case 7:
++		cpu_if->vgic_ap0r[3] = read_gicreg(ICH_AP0R3_EL2);
++		cpu_if->vgic_ap0r[2] = read_gicreg(ICH_AP0R2_EL2);
++	case 6:
++		cpu_if->vgic_ap0r[1] = read_gicreg(ICH_AP0R1_EL2);
++	default:
++		cpu_if->vgic_ap0r[0] = read_gicreg(ICH_AP0R0_EL2);
++	}
++
++	switch (nr_pri_bits) {
++	case 7:
++		cpu_if->vgic_ap1r[3] = read_gicreg(ICH_AP1R3_EL2);
++		cpu_if->vgic_ap1r[2] = read_gicreg(ICH_AP1R2_EL2);
++	case 6:
++		cpu_if->vgic_ap1r[1] = read_gicreg(ICH_AP1R1_EL2);
++	default:
++		cpu_if->vgic_ap1r[0] = read_gicreg(ICH_AP1R0_EL2);
++	}
++
++	val = read_gicreg(ICC_SRE_EL2);
++	write_gicreg(val | ICC_SRE_EL2_ENABLE, ICC_SRE_EL2);
++	isb(); /* Make sure ENABLE is set at EL2 before setting SRE at EL1 */
++	write_gicreg(1, ICC_SRE_EL1);
++}
++
++void __hyp_text __vgic_v3_restore_state(struct kvm_vcpu *vcpu)
++{
++	struct vgic_v3_cpu_if *cpu_if = &vcpu->arch.vgic_cpu.vgic_v3;
++	u64 val;
++	u32 max_lr_idx, nr_pri_bits;
++
++	/*
++	 * VFIQEn is RES1 if ICC_SRE_EL1.SRE is 1. This causes a
++	 * Group0 interrupt (as generated in GICv2 mode) to be
++	 * delivered as a FIQ to the guest, with potentially fatal
++	 * consequences. So we must make sure that ICC_SRE_EL1 has
++	 * been actually programmed with the value we want before
++	 * starting to mess with the rest of the GIC.
++	 */
++	write_gicreg(cpu_if->vgic_sre, ICC_SRE_EL1);
++	isb();
++
++	write_gicreg(cpu_if->vgic_hcr, ICH_HCR_EL2);
++	write_gicreg(cpu_if->vgic_vmcr, ICH_VMCR_EL2);
++
++	val = read_gicreg(ICH_VTR_EL2);
++	max_lr_idx = vtr_to_max_lr_idx(val);
++	nr_pri_bits = vtr_to_nr_pri_bits(val);
++
++	switch (nr_pri_bits) {
++	case 7:
++		 write_gicreg(cpu_if->vgic_ap1r[3], ICH_AP1R3_EL2);
++		 write_gicreg(cpu_if->vgic_ap1r[2], ICH_AP1R2_EL2);
++	case 6:
++		 write_gicreg(cpu_if->vgic_ap1r[1], ICH_AP1R1_EL2);
++	default:
++		 write_gicreg(cpu_if->vgic_ap1r[0], ICH_AP1R0_EL2);
++	}	 	                           
++		 	                           
++	switch (nr_pri_bits) {
++	case 7:
++		 write_gicreg(cpu_if->vgic_ap0r[3], ICH_AP0R3_EL2);
++		 write_gicreg(cpu_if->vgic_ap0r[2], ICH_AP0R2_EL2);
++	case 6:
++		 write_gicreg(cpu_if->vgic_ap0r[1], ICH_AP0R1_EL2);
++	default:
++		 write_gicreg(cpu_if->vgic_ap0r[0], ICH_AP0R0_EL2);
++	}
++
++	switch (max_lr_idx) {
++	case 15:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(15)], ICH_LR15_EL2);
++	case 14:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(14)], ICH_LR14_EL2);
++	case 13:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(13)], ICH_LR13_EL2);
++	case 12:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(12)], ICH_LR12_EL2);
++	case 11:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(11)], ICH_LR11_EL2);
++	case 10:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(10)], ICH_LR10_EL2);
++	case 9:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(9)], ICH_LR9_EL2);
++	case 8:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(8)], ICH_LR8_EL2);
++	case 7:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(7)], ICH_LR7_EL2);
++	case 6:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(6)], ICH_LR6_EL2);
++	case 5:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(5)], ICH_LR5_EL2);
++	case 4:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(4)], ICH_LR4_EL2);
++	case 3:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(3)], ICH_LR3_EL2);
++	case 2:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(2)], ICH_LR2_EL2);
++	case 1:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(1)], ICH_LR1_EL2);
++	case 0:
++		write_gicreg(cpu_if->vgic_lr[VGIC_V3_LR_INDEX(0)], ICH_LR0_EL2);
++	}
++
++	/*
++	 * Ensures that the above will have reached the
++	 * (re)distributors. This ensure the guest will read the
++	 * correct values from the memory-mapped interface.
++	 */
++	isb();
++	dsb(sy);
++
++	/*
++	 * Prevent the guest from touching the GIC system registers if
++	 * SRE isn't enabled for GICv3 emulation.
++	 */
++	if (!cpu_if->vgic_sre) {
++		write_gicreg(read_gicreg(ICC_SRE_EL2) & ~ICC_SRE_EL2_ENABLE,
++			     ICC_SRE_EL2);
++	}
++}
++
++u64 __hyp_text __vgic_v3_read_ich_vtr_el2(void)
++{
++	return read_gicreg(ICH_VTR_EL2);
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0051-arm64-KVM-Add-compatibility-aliases.patch b/tools/kdump/0051-arm64-KVM-Add-compatibility-aliases.patch
new file mode 100644
index 0000000..a6d5bb2
--- /dev/null
+++ b/tools/kdump/0051-arm64-KVM-Add-compatibility-aliases.patch
@@ -0,0 +1,118 @@
+From 27c5f825e14eacca6292b5638300e52841f179e4 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Sun, 25 Oct 2015 13:58:00 +0000
+Subject: [PATCH 051/123] arm64: KVM: Add compatibility aliases
+
+So far, we've implemented the new world switch with a completely
+different namespace, so that we could have both implementation
+compiled in.
+
+Let's take things one step further by adding weak aliases that
+have the same names as the original implementation. The weak
+attributes allows the new implementation to be overriden by the
+old one, and everything still work.
+
+At a later point, we'll be able to simply drop the old code, and
+everything will hopefully keep working, thanks to the aliases we
+have just added. This also saves us repainting all the callers.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 044ac37d1281fc7b59d5dce4fe979a99369e95f2)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/debug-sr.c   | 3 +++
+ arch/arm64/kvm/hyp/hyp-entry.S  | 3 +++
+ arch/arm64/kvm/hyp/switch.c     | 3 +++
+ arch/arm64/kvm/hyp/tlb.c        | 9 +++++++++
+ arch/arm64/kvm/hyp/vgic-v3-sr.c | 3 +++
+ 5 files changed, 21 insertions(+)
+
+diff --git a/arch/arm64/kvm/hyp/debug-sr.c b/arch/arm64/kvm/hyp/debug-sr.c
+index 7848322..d071f45 100644
+--- a/arch/arm64/kvm/hyp/debug-sr.c
++++ b/arch/arm64/kvm/hyp/debug-sr.c
+@@ -135,3 +135,6 @@ u32 __hyp_text __debug_read_mdcr_el2(void)
+ {
+ 	return read_sysreg(mdcr_el2);
+ }
++
++__alias(__debug_read_mdcr_el2)
++u32 __weak __kvm_get_mdcr_el2(void);
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+index 8e58a3b..10d6d2a 100644
+--- a/arch/arm64/kvm/hyp/hyp-entry.S
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -189,6 +189,8 @@ ENDPROC(\label)
+ 
+ 	.align 11
+ 
++	.weak	__kvm_hyp_vector
++ENTRY(__kvm_hyp_vector)
+ ENTRY(__hyp_vector)
+ 	ventry	el2t_sync_invalid		// Synchronous EL2t
+ 	ventry	el2t_irq_invalid		// IRQ EL2t
+@@ -210,3 +212,4 @@ ENTRY(__hyp_vector)
+ 	ventry	el1_fiq_invalid			// FIQ 32-bit EL1
+ 	ventry	el1_error_invalid		// Error 32-bit EL1
+ ENDPROC(__hyp_vector)
++ENDPROC(__kvm_hyp_vector)
+diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c
+index b012870..7457ae4 100644
+--- a/arch/arm64/kvm/hyp/switch.c
++++ b/arch/arm64/kvm/hyp/switch.c
+@@ -142,6 +142,9 @@ int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ 	return exit_code;
+ }
+ 
++__alias(__guest_run)
++int __weak __kvm_vcpu_run(struct kvm_vcpu *vcpu);
++
+ static const char __hyp_panic_string[] = "HYP panic:\nPS:%08llx PC:%016llx ESR:%08llx\nFAR:%016llx HPFAR:%016llx PAR:%016llx\nVCPU:%p\n";
+ 
+ void __hyp_text __noreturn __hyp_panic(void)
+diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c
+index 6fcb93a..5f815cf 100644
+--- a/arch/arm64/kvm/hyp/tlb.c
++++ b/arch/arm64/kvm/hyp/tlb.c
+@@ -48,6 +48,9 @@ void __hyp_text __tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
+ 	write_sysreg(0, vttbr_el2);
+ }
+ 
++__alias(__tlb_flush_vmid_ipa)
++void __weak __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
++
+ void __hyp_text __tlb_flush_vmid(struct kvm *kvm)
+ {
+ 	dsb(ishst);
+@@ -64,6 +67,9 @@ void __hyp_text __tlb_flush_vmid(struct kvm *kvm)
+ 	write_sysreg(0, vttbr_el2);
+ }
+ 
++__alias(__tlb_flush_vmid)
++void __weak __kvm_tlb_flush_vmid(struct kvm *kvm);
++
+ void __hyp_text __tlb_flush_vm_context(void)
+ {
+ 	dsb(ishst);
+@@ -71,3 +77,6 @@ void __hyp_text __tlb_flush_vm_context(void)
+ 		     "ic ialluis	  ": : );
+ 	dsb(ish);
+ }
++
++__alias(__tlb_flush_vm_context)
++void __weak __kvm_flush_vm_context(void);
+diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
+index 78d05f3..a769458 100644
+--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
++++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
+@@ -224,3 +224,6 @@ u64 __hyp_text __vgic_v3_read_ich_vtr_el2(void)
+ {
+ 	return read_gicreg(ICH_VTR_EL2);
+ }
++
++__alias(__vgic_v3_read_ich_vtr_el2)
++u64 __weak __vgic_v3_get_ich_vtr_el2(void);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0052-arm64-KVM-Map-the-kernel-RO-section-into-HYP.patch b/tools/kdump/0052-arm64-KVM-Map-the-kernel-RO-section-into-HYP.patch
new file mode 100644
index 0000000..a3f9c85
--- /dev/null
+++ b/tools/kdump/0052-arm64-KVM-Map-the-kernel-RO-section-into-HYP.patch
@@ -0,0 +1,44 @@
+From e0bbb13819e101b291c0cf9f933ada646727a5af Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Tue, 27 Oct 2015 12:18:48 +0000
+Subject: [PATCH 052/123] arm64: KVM: Map the kernel RO section into HYP
+
+In order to run C code in HYP, we must make sure that the kernel's
+RO section is mapped into HYP (otherwise things break badly).
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 910917bb7db070cc67557a6b3c8fcceaa5c398a7)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/kvm/arm.c | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index d7bef21..449e312 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -44,6 +44,7 @@
+ #include <asm/kvm_emulate.h>
+ #include <asm/kvm_coproc.h>
+ #include <asm/kvm_psci.h>
++#include <asm/sections.h>
+ 
+ #ifdef REQUIRES_VIRT
+ __asm__(".arch_extension	virt");
+@@ -1065,6 +1066,12 @@ static int init_hyp_mode(void)
+ 		goto out_free_mappings;
+ 	}
+ 
++	err = create_hyp_mappings(__start_rodata, __end_rodata);
++	if (err) {
++		kvm_err("Cannot map rodata section\n");
++		goto out_free_mappings;
++	}
++
+ 	/*
+ 	 * Map the Hyp stack pages
+ 	 */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0053-arm64-KVM-Move-away-from-the-assembly-version-of-the.patch b/tools/kdump/0053-arm64-KVM-Move-away-from-the-assembly-version-of-the.patch
new file mode 100644
index 0000000..9dbd9d4
--- /dev/null
+++ b/tools/kdump/0053-arm64-KVM-Move-away-from-the-assembly-version-of-the.patch
@@ -0,0 +1,1556 @@
+From 77fccc6a5550c59087797162d79ac2de2c855747 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Sun, 25 Oct 2015 15:51:41 +0000
+Subject: [PATCH 053/123] arm64: KVM: Move away from the assembly version of
+ the world switch
+
+This is it. We remove all of the code that has now been rewritten.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 1ea66d27e7b01086669ff2abdc3ac89dc90eae51)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/Makefile         |    2 -
+ arch/arm64/kvm/hyp.S            | 1081 +--------------------------------------
+ arch/arm64/kvm/vgic-v2-switch.S |  134 -----
+ arch/arm64/kvm/vgic-v3-switch.S |  269 ----------
+ 4 files changed, 1 insertion(+), 1485 deletions(-)
+ delete mode 100644 arch/arm64/kvm/vgic-v2-switch.S
+ delete mode 100644 arch/arm64/kvm/vgic-v3-switch.S
+
+diff --git a/arch/arm64/kvm/Makefile b/arch/arm64/kvm/Makefile
+index d31e4e5..caee9ee 100644
+--- a/arch/arm64/kvm/Makefile
++++ b/arch/arm64/kvm/Makefile
+@@ -23,8 +23,6 @@ kvm-$(CONFIG_KVM_ARM_HOST) += guest.o debug.o reset.o sys_regs.o sys_regs_generi
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/arm/vgic.o
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/arm/vgic-v2.o
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/arm/vgic-v2-emul.o
+-kvm-$(CONFIG_KVM_ARM_HOST) += vgic-v2-switch.o
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/arm/vgic-v3.o
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/arm/vgic-v3-emul.o
+-kvm-$(CONFIG_KVM_ARM_HOST) += vgic-v3-switch.o
+ kvm-$(CONFIG_KVM_ARM_HOST) += $(KVM)/arm/arch_timer.o
+diff --git a/arch/arm64/kvm/hyp.S b/arch/arm64/kvm/hyp.S
+index 86c2898..0ccdcbb 100644
+--- a/arch/arm64/kvm/hyp.S
++++ b/arch/arm64/kvm/hyp.S
+@@ -17,910 +17,7 @@
+ 
+ #include <linux/linkage.h>
+ 
+-#include <asm/alternative.h>
+-#include <asm/asm-offsets.h>
+ #include <asm/assembler.h>
+-#include <asm/cpufeature.h>
+-#include <asm/debug-monitors.h>
+-#include <asm/esr.h>
+-#include <asm/fpsimdmacros.h>
+-#include <asm/kvm.h>
+-#include <asm/kvm_arm.h>
+-#include <asm/kvm_asm.h>
+-#include <asm/kvm_mmu.h>
+-#include <asm/memory.h>
+-
+-#define CPU_GP_REG_OFFSET(x)	(CPU_GP_REGS + x)
+-#define CPU_XREG_OFFSET(x)	CPU_GP_REG_OFFSET(CPU_USER_PT_REGS + 8*x)
+-#define CPU_SPSR_OFFSET(x)	CPU_GP_REG_OFFSET(CPU_SPSR + 8*x)
+-#define CPU_SYSREG_OFFSET(x)	(CPU_SYSREGS + 8*x)
+-
+-	.text
+-	.pushsection	.hyp.text, "ax"
+-	.align	PAGE_SHIFT
+-
+-.macro save_common_regs
+-	// x2: base address for cpu context
+-	// x3: tmp register
+-
+-	add	x3, x2, #CPU_XREG_OFFSET(19)
+-	stp	x19, x20, [x3]
+-	stp	x21, x22, [x3, #16]
+-	stp	x23, x24, [x3, #32]
+-	stp	x25, x26, [x3, #48]
+-	stp	x27, x28, [x3, #64]
+-	stp	x29, lr, [x3, #80]
+-
+-	mrs	x19, sp_el0
+-	mrs	x20, elr_el2		// pc before entering el2
+-	mrs	x21, spsr_el2		// pstate before entering el2
+-
+-	stp	x19, x20, [x3, #96]
+-	str	x21, [x3, #112]
+-
+-	mrs	x22, sp_el1
+-	mrs	x23, elr_el1
+-	mrs	x24, spsr_el1
+-
+-	str	x22, [x2, #CPU_GP_REG_OFFSET(CPU_SP_EL1)]
+-	str	x23, [x2, #CPU_GP_REG_OFFSET(CPU_ELR_EL1)]
+-	str	x24, [x2, #CPU_SPSR_OFFSET(KVM_SPSR_EL1)]
+-.endm
+-
+-.macro restore_common_regs
+-	// x2: base address for cpu context
+-	// x3: tmp register
+-
+-	ldr	x22, [x2, #CPU_GP_REG_OFFSET(CPU_SP_EL1)]
+-	ldr	x23, [x2, #CPU_GP_REG_OFFSET(CPU_ELR_EL1)]
+-	ldr	x24, [x2, #CPU_SPSR_OFFSET(KVM_SPSR_EL1)]
+-
+-	msr	sp_el1, x22
+-	msr	elr_el1, x23
+-	msr	spsr_el1, x24
+-
+-	add	x3, x2, #CPU_XREG_OFFSET(31)    // SP_EL0
+-	ldp	x19, x20, [x3]
+-	ldr	x21, [x3, #16]
+-
+-	msr	sp_el0, x19
+-	msr	elr_el2, x20 		// pc on return from el2
+-	msr	spsr_el2, x21 		// pstate on return from el2
+-
+-	add	x3, x2, #CPU_XREG_OFFSET(19)
+-	ldp	x19, x20, [x3]
+-	ldp	x21, x22, [x3, #16]
+-	ldp	x23, x24, [x3, #32]
+-	ldp	x25, x26, [x3, #48]
+-	ldp	x27, x28, [x3, #64]
+-	ldp	x29, lr, [x3, #80]
+-.endm
+-
+-.macro save_host_regs
+-	save_common_regs
+-.endm
+-
+-.macro restore_host_regs
+-	restore_common_regs
+-.endm
+-
+-.macro save_fpsimd
+-	// x2: cpu context address
+-	// x3, x4: tmp regs
+-	add	x3, x2, #CPU_GP_REG_OFFSET(CPU_FP_REGS)
+-	fpsimd_save x3, 4
+-.endm
+-
+-.macro restore_fpsimd
+-	// x2: cpu context address
+-	// x3, x4: tmp regs
+-	add	x3, x2, #CPU_GP_REG_OFFSET(CPU_FP_REGS)
+-	fpsimd_restore x3, 4
+-.endm
+-
+-.macro save_guest_regs
+-	// x0 is the vcpu address
+-	// x1 is the return code, do not corrupt!
+-	// x2 is the cpu context
+-	// x3 is a tmp register
+-	// Guest's x0-x3 are on the stack
+-
+-	// Compute base to save registers
+-	add	x3, x2, #CPU_XREG_OFFSET(4)
+-	stp	x4, x5, [x3]
+-	stp	x6, x7, [x3, #16]
+-	stp	x8, x9, [x3, #32]
+-	stp	x10, x11, [x3, #48]
+-	stp	x12, x13, [x3, #64]
+-	stp	x14, x15, [x3, #80]
+-	stp	x16, x17, [x3, #96]
+-	str	x18, [x3, #112]
+-
+-	pop	x6, x7			// x2, x3
+-	pop	x4, x5			// x0, x1
+-
+-	add	x3, x2, #CPU_XREG_OFFSET(0)
+-	stp	x4, x5, [x3]
+-	stp	x6, x7, [x3, #16]
+-
+-	save_common_regs
+-.endm
+-
+-.macro restore_guest_regs
+-	// x0 is the vcpu address.
+-	// x2 is the cpu context
+-	// x3 is a tmp register
+-
+-	// Prepare x0-x3 for later restore
+-	add	x3, x2, #CPU_XREG_OFFSET(0)
+-	ldp	x4, x5, [x3]
+-	ldp	x6, x7, [x3, #16]
+-	push	x4, x5		// Push x0-x3 on the stack
+-	push	x6, x7
+-
+-	// x4-x18
+-	ldp	x4, x5, [x3, #32]
+-	ldp	x6, x7, [x3, #48]
+-	ldp	x8, x9, [x3, #64]
+-	ldp	x10, x11, [x3, #80]
+-	ldp	x12, x13, [x3, #96]
+-	ldp	x14, x15, [x3, #112]
+-	ldp	x16, x17, [x3, #128]
+-	ldr	x18, [x3, #144]
+-
+-	// x19-x29, lr, sp*, elr*, spsr*
+-	restore_common_regs
+-
+-	// Last bits of the 64bit state
+-	pop	x2, x3
+-	pop	x0, x1
+-
+-	// Do not touch any register after this!
+-.endm
+-
+-/*
+- * Macros to perform system register save/restore.
+- *
+- * Ordering here is absolutely critical, and must be kept consistent
+- * in {save,restore}_sysregs, {save,restore}_guest_32bit_state,
+- * and in kvm_asm.h.
+- *
+- * In other words, don't touch any of these unless you know what
+- * you are doing.
+- */
+-.macro save_sysregs
+-	// x2: base address for cpu context
+-	// x3: tmp register
+-
+-	add	x3, x2, #CPU_SYSREG_OFFSET(MPIDR_EL1)
+-
+-	mrs	x4,	vmpidr_el2
+-	mrs	x5,	csselr_el1
+-	mrs	x6,	sctlr_el1
+-	mrs	x7,	actlr_el1
+-	mrs	x8,	cpacr_el1
+-	mrs	x9,	ttbr0_el1
+-	mrs	x10,	ttbr1_el1
+-	mrs	x11,	tcr_el1
+-	mrs	x12,	esr_el1
+-	mrs	x13, 	afsr0_el1
+-	mrs	x14,	afsr1_el1
+-	mrs	x15,	far_el1
+-	mrs	x16,	mair_el1
+-	mrs	x17,	vbar_el1
+-	mrs	x18,	contextidr_el1
+-	mrs	x19,	tpidr_el0
+-	mrs	x20,	tpidrro_el0
+-	mrs	x21,	tpidr_el1
+-	mrs	x22, 	amair_el1
+-	mrs	x23, 	cntkctl_el1
+-	mrs	x24,	par_el1
+-	mrs	x25,	mdscr_el1
+-
+-	stp	x4, x5, [x3]
+-	stp	x6, x7, [x3, #16]
+-	stp	x8, x9, [x3, #32]
+-	stp	x10, x11, [x3, #48]
+-	stp	x12, x13, [x3, #64]
+-	stp	x14, x15, [x3, #80]
+-	stp	x16, x17, [x3, #96]
+-	stp	x18, x19, [x3, #112]
+-	stp	x20, x21, [x3, #128]
+-	stp	x22, x23, [x3, #144]
+-	stp	x24, x25, [x3, #160]
+-.endm
+-
+-.macro save_debug type
+-	// x4: pointer to register set
+-	// x5: number of registers to skip
+-	// x6..x22 trashed
+-
+-	adr	x22, 1f
+-	add	x22, x22, x5, lsl #2
+-	br	x22
+-1:
+-	mrs	x21, \type\()15_el1
+-	mrs	x20, \type\()14_el1
+-	mrs	x19, \type\()13_el1
+-	mrs	x18, \type\()12_el1
+-	mrs	x17, \type\()11_el1
+-	mrs	x16, \type\()10_el1
+-	mrs	x15, \type\()9_el1
+-	mrs	x14, \type\()8_el1
+-	mrs	x13, \type\()7_el1
+-	mrs	x12, \type\()6_el1
+-	mrs	x11, \type\()5_el1
+-	mrs	x10, \type\()4_el1
+-	mrs	x9, \type\()3_el1
+-	mrs	x8, \type\()2_el1
+-	mrs	x7, \type\()1_el1
+-	mrs	x6, \type\()0_el1
+-
+-	adr	x22, 1f
+-	add	x22, x22, x5, lsl #2
+-	br	x22
+-1:
+-	str	x21, [x4, #(15 * 8)]
+-	str	x20, [x4, #(14 * 8)]
+-	str	x19, [x4, #(13 * 8)]
+-	str	x18, [x4, #(12 * 8)]
+-	str	x17, [x4, #(11 * 8)]
+-	str	x16, [x4, #(10 * 8)]
+-	str	x15, [x4, #(9 * 8)]
+-	str	x14, [x4, #(8 * 8)]
+-	str	x13, [x4, #(7 * 8)]
+-	str	x12, [x4, #(6 * 8)]
+-	str	x11, [x4, #(5 * 8)]
+-	str	x10, [x4, #(4 * 8)]
+-	str	x9, [x4, #(3 * 8)]
+-	str	x8, [x4, #(2 * 8)]
+-	str	x7, [x4, #(1 * 8)]
+-	str	x6, [x4, #(0 * 8)]
+-.endm
+-
+-.macro restore_sysregs
+-	// x2: base address for cpu context
+-	// x3: tmp register
+-
+-	add	x3, x2, #CPU_SYSREG_OFFSET(MPIDR_EL1)
+-
+-	ldp	x4, x5, [x3]
+-	ldp	x6, x7, [x3, #16]
+-	ldp	x8, x9, [x3, #32]
+-	ldp	x10, x11, [x3, #48]
+-	ldp	x12, x13, [x3, #64]
+-	ldp	x14, x15, [x3, #80]
+-	ldp	x16, x17, [x3, #96]
+-	ldp	x18, x19, [x3, #112]
+-	ldp	x20, x21, [x3, #128]
+-	ldp	x22, x23, [x3, #144]
+-	ldp	x24, x25, [x3, #160]
+-
+-	msr	vmpidr_el2,	x4
+-	msr	csselr_el1,	x5
+-	msr	sctlr_el1,	x6
+-	msr	actlr_el1,	x7
+-	msr	cpacr_el1,	x8
+-	msr	ttbr0_el1,	x9
+-	msr	ttbr1_el1,	x10
+-	msr	tcr_el1,	x11
+-	msr	esr_el1,	x12
+-	msr	afsr0_el1,	x13
+-	msr	afsr1_el1,	x14
+-	msr	far_el1,	x15
+-	msr	mair_el1,	x16
+-	msr	vbar_el1,	x17
+-	msr	contextidr_el1,	x18
+-	msr	tpidr_el0,	x19
+-	msr	tpidrro_el0,	x20
+-	msr	tpidr_el1,	x21
+-	msr	amair_el1,	x22
+-	msr	cntkctl_el1,	x23
+-	msr	par_el1,	x24
+-	msr	mdscr_el1,	x25
+-.endm
+-
+-.macro restore_debug type
+-	// x4: pointer to register set
+-	// x5: number of registers to skip
+-	// x6..x22 trashed
+-
+-	adr	x22, 1f
+-	add	x22, x22, x5, lsl #2
+-	br	x22
+-1:
+-	ldr	x21, [x4, #(15 * 8)]
+-	ldr	x20, [x4, #(14 * 8)]
+-	ldr	x19, [x4, #(13 * 8)]
+-	ldr	x18, [x4, #(12 * 8)]
+-	ldr	x17, [x4, #(11 * 8)]
+-	ldr	x16, [x4, #(10 * 8)]
+-	ldr	x15, [x4, #(9 * 8)]
+-	ldr	x14, [x4, #(8 * 8)]
+-	ldr	x13, [x4, #(7 * 8)]
+-	ldr	x12, [x4, #(6 * 8)]
+-	ldr	x11, [x4, #(5 * 8)]
+-	ldr	x10, [x4, #(4 * 8)]
+-	ldr	x9, [x4, #(3 * 8)]
+-	ldr	x8, [x4, #(2 * 8)]
+-	ldr	x7, [x4, #(1 * 8)]
+-	ldr	x6, [x4, #(0 * 8)]
+-
+-	adr	x22, 1f
+-	add	x22, x22, x5, lsl #2
+-	br	x22
+-1:
+-	msr	\type\()15_el1, x21
+-	msr	\type\()14_el1, x20
+-	msr	\type\()13_el1, x19
+-	msr	\type\()12_el1, x18
+-	msr	\type\()11_el1, x17
+-	msr	\type\()10_el1, x16
+-	msr	\type\()9_el1, x15
+-	msr	\type\()8_el1, x14
+-	msr	\type\()7_el1, x13
+-	msr	\type\()6_el1, x12
+-	msr	\type\()5_el1, x11
+-	msr	\type\()4_el1, x10
+-	msr	\type\()3_el1, x9
+-	msr	\type\()2_el1, x8
+-	msr	\type\()1_el1, x7
+-	msr	\type\()0_el1, x6
+-.endm
+-
+-.macro skip_32bit_state tmp, target
+-	// Skip 32bit state if not needed
+-	mrs	\tmp, hcr_el2
+-	tbnz	\tmp, #HCR_RW_SHIFT, \target
+-.endm
+-
+-.macro skip_tee_state tmp, target
+-	// Skip ThumbEE state if not needed
+-	mrs	\tmp, id_pfr0_el1
+-	tbz	\tmp, #12, \target
+-.endm
+-
+-.macro skip_debug_state tmp, target
+-	ldr	\tmp, [x0, #VCPU_DEBUG_FLAGS]
+-	tbz	\tmp, #KVM_ARM64_DEBUG_DIRTY_SHIFT, \target
+-.endm
+-
+-/*
+- * Branch to target if CPTR_EL2.TFP bit is set (VFP/SIMD trapping enabled)
+- */
+-.macro skip_fpsimd_state tmp, target
+-	mrs	\tmp, cptr_el2
+-	tbnz	\tmp, #CPTR_EL2_TFP_SHIFT, \target
+-.endm
+-
+-.macro compute_debug_state target
+-	// Compute debug state: If any of KDE, MDE or KVM_ARM64_DEBUG_DIRTY
+-	// is set, we do a full save/restore cycle and disable trapping.
+-	add	x25, x0, #VCPU_CONTEXT
+-
+-	// Check the state of MDSCR_EL1
+-	ldr	x25, [x25, #CPU_SYSREG_OFFSET(MDSCR_EL1)]
+-	and	x26, x25, #DBG_MDSCR_KDE
+-	and	x25, x25, #DBG_MDSCR_MDE
+-	adds	xzr, x25, x26
+-	b.eq	9998f		// Nothing to see there
+-
+-	// If any interesting bits was set, we must set the flag
+-	mov	x26, #KVM_ARM64_DEBUG_DIRTY
+-	str	x26, [x0, #VCPU_DEBUG_FLAGS]
+-	b	9999f		// Don't skip restore
+-
+-9998:
+-	// Otherwise load the flags from memory in case we recently
+-	// trapped
+-	skip_debug_state x25, \target
+-9999:
+-.endm
+-
+-.macro save_guest_32bit_state
+-	skip_32bit_state x3, 1f
+-
+-	add	x3, x2, #CPU_SPSR_OFFSET(KVM_SPSR_ABT)
+-	mrs	x4, spsr_abt
+-	mrs	x5, spsr_und
+-	mrs	x6, spsr_irq
+-	mrs	x7, spsr_fiq
+-	stp	x4, x5, [x3]
+-	stp	x6, x7, [x3, #16]
+-
+-	add	x3, x2, #CPU_SYSREG_OFFSET(DACR32_EL2)
+-	mrs	x4, dacr32_el2
+-	mrs	x5, ifsr32_el2
+-	stp	x4, x5, [x3]
+-
+-	skip_fpsimd_state x8, 2f
+-	mrs	x6, fpexc32_el2
+-	str	x6, [x3, #16]
+-2:
+-	skip_debug_state x8, 1f
+-	mrs	x7, dbgvcr32_el2
+-	str	x7, [x3, #24]
+-1:
+-.endm
+-
+-.macro restore_guest_32bit_state
+-	skip_32bit_state x3, 1f
+-
+-	add	x3, x2, #CPU_SPSR_OFFSET(KVM_SPSR_ABT)
+-	ldp	x4, x5, [x3]
+-	ldp	x6, x7, [x3, #16]
+-	msr	spsr_abt, x4
+-	msr	spsr_und, x5
+-	msr	spsr_irq, x6
+-	msr	spsr_fiq, x7
+-
+-	add	x3, x2, #CPU_SYSREG_OFFSET(DACR32_EL2)
+-	ldp	x4, x5, [x3]
+-	msr	dacr32_el2, x4
+-	msr	ifsr32_el2, x5
+-
+-	skip_debug_state x8, 1f
+-	ldr	x7, [x3, #24]
+-	msr	dbgvcr32_el2, x7
+-1:
+-.endm
+-
+-.macro activate_traps
+-	ldr     x2, [x0, #VCPU_HCR_EL2]
+-
+-	/*
+-	 * We are about to set CPTR_EL2.TFP to trap all floating point
+-	 * register accesses to EL2, however, the ARM ARM clearly states that
+-	 * traps are only taken to EL2 if the operation would not otherwise
+-	 * trap to EL1.  Therefore, always make sure that for 32-bit guests,
+-	 * we set FPEXC.EN to prevent traps to EL1, when setting the TFP bit.
+-	 */
+-	tbnz	x2, #HCR_RW_SHIFT, 99f // open code skip_32bit_state
+-	mov	x3, #(1 << 30)
+-	msr	fpexc32_el2, x3
+-	isb
+-99:
+-	msr     hcr_el2, x2
+-	mov	x2, #CPTR_EL2_TTA
+-	orr     x2, x2, #CPTR_EL2_TFP
+-	msr	cptr_el2, x2
+-
+-	mov	x2, #(1 << 15)	// Trap CP15 Cr=15
+-	msr	hstr_el2, x2
+-
+-	// Monitor Debug Config - see kvm_arm_setup_debug()
+-	ldr	x2, [x0, #VCPU_MDCR_EL2]
+-	msr	mdcr_el2, x2
+-.endm
+-
+-.macro deactivate_traps
+-	mov	x2, #HCR_RW
+-	msr	hcr_el2, x2
+-	msr	hstr_el2, xzr
+-
+-	mrs	x2, mdcr_el2
+-	and	x2, x2, #MDCR_EL2_HPMN_MASK
+-	msr	mdcr_el2, x2
+-.endm
+-
+-.macro activate_vm
+-	ldr	x1, [x0, #VCPU_KVM]
+-	kern_hyp_va	x1
+-	ldr	x2, [x1, #KVM_VTTBR]
+-	msr	vttbr_el2, x2
+-.endm
+-
+-.macro deactivate_vm
+-	msr	vttbr_el2, xzr
+-.endm
+-
+-/*
+- * Call into the vgic backend for state saving
+- */
+-.macro save_vgic_state
+-alternative_if_not ARM64_HAS_SYSREG_GIC_CPUIF
+-	bl	__save_vgic_v2_state
+-alternative_else
+-	bl	__save_vgic_v3_state
+-alternative_endif
+-	mrs	x24, hcr_el2
+-	mov	x25, #HCR_INT_OVERRIDE
+-	neg	x25, x25
+-	and	x24, x24, x25
+-	msr	hcr_el2, x24
+-.endm
+-
+-/*
+- * Call into the vgic backend for state restoring
+- */
+-.macro restore_vgic_state
+-	mrs	x24, hcr_el2
+-	ldr	x25, [x0, #VCPU_IRQ_LINES]
+-	orr	x24, x24, #HCR_INT_OVERRIDE
+-	orr	x24, x24, x25
+-	msr	hcr_el2, x24
+-alternative_if_not ARM64_HAS_SYSREG_GIC_CPUIF
+-	bl	__restore_vgic_v2_state
+-alternative_else
+-	bl	__restore_vgic_v3_state
+-alternative_endif
+-.endm
+-
+-.macro save_timer_state
+-	// x0: vcpu pointer
+-	ldr	x2, [x0, #VCPU_KVM]
+-	kern_hyp_va x2
+-	ldr	w3, [x2, #KVM_TIMER_ENABLED]
+-	cbz	w3, 1f
+-
+-	mrs	x3, cntv_ctl_el0
+-	and	x3, x3, #3
+-	str	w3, [x0, #VCPU_TIMER_CNTV_CTL]
+-
+-	isb
+-
+-	mrs	x3, cntv_cval_el0
+-	str	x3, [x0, #VCPU_TIMER_CNTV_CVAL]
+-
+-1:
+-	// Disable the virtual timer
+-	msr	cntv_ctl_el0, xzr
+-
+-	// Allow physical timer/counter access for the host
+-	mrs	x2, cnthctl_el2
+-	orr	x2, x2, #3
+-	msr	cnthctl_el2, x2
+-
+-	// Clear cntvoff for the host
+-	msr	cntvoff_el2, xzr
+-.endm
+-
+-.macro restore_timer_state
+-	// x0: vcpu pointer
+-	// Disallow physical timer access for the guest
+-	// Physical counter access is allowed
+-	mrs	x2, cnthctl_el2
+-	orr	x2, x2, #1
+-	bic	x2, x2, #2
+-	msr	cnthctl_el2, x2
+-
+-	ldr	x2, [x0, #VCPU_KVM]
+-	kern_hyp_va x2
+-	ldr	w3, [x2, #KVM_TIMER_ENABLED]
+-	cbz	w3, 1f
+-
+-	ldr	x3, [x2, #KVM_TIMER_CNTVOFF]
+-	msr	cntvoff_el2, x3
+-	ldr	x2, [x0, #VCPU_TIMER_CNTV_CVAL]
+-	msr	cntv_cval_el0, x2
+-	isb
+-
+-	ldr	w2, [x0, #VCPU_TIMER_CNTV_CTL]
+-	and	x2, x2, #3
+-	msr	cntv_ctl_el0, x2
+-1:
+-.endm
+-
+-__save_sysregs:
+-	save_sysregs
+-	ret
+-
+-__restore_sysregs:
+-	restore_sysregs
+-	ret
+-
+-/* Save debug state */
+-__save_debug:
+-	// x2: ptr to CPU context
+-	// x3: ptr to debug reg struct
+-	// x4/x5/x6-22/x24-26: trashed
+-
+-	mrs	x26, id_aa64dfr0_el1
+-	ubfx	x24, x26, #12, #4	// Extract BRPs
+-	ubfx	x25, x26, #20, #4	// Extract WRPs
+-	mov	w26, #15
+-	sub	w24, w26, w24		// How many BPs to skip
+-	sub	w25, w26, w25		// How many WPs to skip
+-
+-	mov	x5, x24
+-	add	x4, x3, #DEBUG_BCR
+-	save_debug dbgbcr
+-	add	x4, x3, #DEBUG_BVR
+-	save_debug dbgbvr
+-
+-	mov	x5, x25
+-	add	x4, x3, #DEBUG_WCR
+-	save_debug dbgwcr
+-	add	x4, x3, #DEBUG_WVR
+-	save_debug dbgwvr
+-
+-	mrs	x21, mdccint_el1
+-	str	x21, [x2, #CPU_SYSREG_OFFSET(MDCCINT_EL1)]
+-	ret
+-
+-/* Restore debug state */
+-__restore_debug:
+-	// x2: ptr to CPU context
+-	// x3: ptr to debug reg struct
+-	// x4/x5/x6-22/x24-26: trashed
+-
+-	mrs	x26, id_aa64dfr0_el1
+-	ubfx	x24, x26, #12, #4	// Extract BRPs
+-	ubfx	x25, x26, #20, #4	// Extract WRPs
+-	mov	w26, #15
+-	sub	w24, w26, w24		// How many BPs to skip
+-	sub	w25, w26, w25		// How many WPs to skip
+-
+-	mov	x5, x24
+-	add	x4, x3, #DEBUG_BCR
+-	restore_debug dbgbcr
+-	add	x4, x3, #DEBUG_BVR
+-	restore_debug dbgbvr
+-
+-	mov	x5, x25
+-	add	x4, x3, #DEBUG_WCR
+-	restore_debug dbgwcr
+-	add	x4, x3, #DEBUG_WVR
+-	restore_debug dbgwvr
+-
+-	ldr	x21, [x2, #CPU_SYSREG_OFFSET(MDCCINT_EL1)]
+-	msr	mdccint_el1, x21
+-
+-	ret
+-
+-__save_fpsimd:
+-	skip_fpsimd_state x3, 1f
+-	save_fpsimd
+-1:	ret
+-
+-__restore_fpsimd:
+-	skip_fpsimd_state x3, 1f
+-	restore_fpsimd
+-1:	ret
+-
+-switch_to_guest_fpsimd:
+-	push	x4, lr
+-
+-	mrs	x2, cptr_el2
+-	bic	x2, x2, #CPTR_EL2_TFP
+-	msr	cptr_el2, x2
+-	isb
+-
+-	mrs	x0, tpidr_el2
+-
+-	ldr	x2, [x0, #VCPU_HOST_CONTEXT]
+-	kern_hyp_va x2
+-	bl __save_fpsimd
+-
+-	add	x2, x0, #VCPU_CONTEXT
+-	bl __restore_fpsimd
+-
+-	skip_32bit_state x3, 1f
+-	ldr	x4, [x2, #CPU_SYSREG_OFFSET(FPEXC32_EL2)]
+-	msr	fpexc32_el2, x4
+-1:
+-	pop	x4, lr
+-	pop	x2, x3
+-	pop	x0, x1
+-
+-	eret
+-
+-/*
+- * u64 __kvm_vcpu_run(struct kvm_vcpu *vcpu);
+- *
+- * This is the world switch. The first half of the function
+- * deals with entering the guest, and anything from __kvm_vcpu_return
+- * to the end of the function deals with reentering the host.
+- * On the enter path, only x0 (vcpu pointer) must be preserved until
+- * the last moment. On the exit path, x0 (vcpu pointer) and x1 (exception
+- * code) must both be preserved until the epilogue.
+- * In both cases, x2 points to the CPU context we're saving/restoring from/to.
+- */
+-ENTRY(__kvm_vcpu_run)
+-	kern_hyp_va	x0
+-	msr	tpidr_el2, x0	// Save the vcpu register
+-
+-	// Host context
+-	ldr	x2, [x0, #VCPU_HOST_CONTEXT]
+-	kern_hyp_va x2
+-
+-	save_host_regs
+-	bl __save_sysregs
+-
+-	compute_debug_state 1f
+-	add	x3, x0, #VCPU_HOST_DEBUG_STATE
+-	bl	__save_debug
+-1:
+-	activate_traps
+-	activate_vm
+-
+-	restore_vgic_state
+-	restore_timer_state
+-
+-	// Guest context
+-	add	x2, x0, #VCPU_CONTEXT
+-
+-	// We must restore the 32-bit state before the sysregs, thanks
+-	// to Cortex-A57 erratum #852523.
+-	restore_guest_32bit_state
+-	bl __restore_sysregs
+-
+-	skip_debug_state x3, 1f
+-	ldr	x3, [x0, #VCPU_DEBUG_PTR]
+-	kern_hyp_va x3
+-	bl	__restore_debug
+-1:
+-	restore_guest_regs
+-
+-	// That's it, no more messing around.
+-	eret
+-
+-__kvm_vcpu_return:
+-	// Assume x0 is the vcpu pointer, x1 the return code
+-	// Guest's x0-x3 are on the stack
+-
+-	// Guest context
+-	add	x2, x0, #VCPU_CONTEXT
+-
+-	save_guest_regs
+-	bl __save_fpsimd
+-	bl __save_sysregs
+-
+-	skip_debug_state x3, 1f
+-	ldr	x3, [x0, #VCPU_DEBUG_PTR]
+-	kern_hyp_va x3
+-	bl	__save_debug
+-1:
+-	save_guest_32bit_state
+-
+-	save_timer_state
+-	save_vgic_state
+-
+-	deactivate_traps
+-	deactivate_vm
+-
+-	// Host context
+-	ldr	x2, [x0, #VCPU_HOST_CONTEXT]
+-	kern_hyp_va x2
+-
+-	bl __restore_sysregs
+-	bl __restore_fpsimd
+-	/* Clear FPSIMD and Trace trapping */
+-	msr     cptr_el2, xzr
+-
+-	skip_debug_state x3, 1f
+-	// Clear the dirty flag for the next run, as all the state has
+-	// already been saved. Note that we nuke the whole 64bit word.
+-	// If we ever add more flags, we'll have to be more careful...
+-	str	xzr, [x0, #VCPU_DEBUG_FLAGS]
+-	add	x3, x0, #VCPU_HOST_DEBUG_STATE
+-	bl	__restore_debug
+-1:
+-	restore_host_regs
+-
+-	mov	x0, x1
+-	ret
+-END(__kvm_vcpu_run)
+-
+-// void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
+-ENTRY(__kvm_tlb_flush_vmid_ipa)
+-	dsb	ishst
+-
+-	kern_hyp_va	x0
+-	ldr	x2, [x0, #KVM_VTTBR]
+-	msr	vttbr_el2, x2
+-	isb
+-
+-	/*
+-	 * We could do so much better if we had the VA as well.
+-	 * Instead, we invalidate Stage-2 for this IPA, and the
+-	 * whole of Stage-1. Weep...
+-	 */
+-	lsr	x1, x1, #12
+-	tlbi	ipas2e1is, x1
+-	/*
+-	 * We have to ensure completion of the invalidation at Stage-2,
+-	 * since a table walk on another CPU could refill a TLB with a
+-	 * complete (S1 + S2) walk based on the old Stage-2 mapping if
+-	 * the Stage-1 invalidation happened first.
+-	 */
+-	dsb	ish
+-	tlbi	vmalle1is
+-	dsb	ish
+-	isb
+-
+-	msr	vttbr_el2, xzr
+-	ret
+-ENDPROC(__kvm_tlb_flush_vmid_ipa)
+-
+-/**
+- * void __kvm_tlb_flush_vmid(struct kvm *kvm) - Flush per-VMID TLBs
+- * @struct kvm *kvm - pointer to kvm structure
+- *
+- * Invalidates all Stage 1 and 2 TLB entries for current VMID.
+- */
+-ENTRY(__kvm_tlb_flush_vmid)
+-	dsb     ishst
+-
+-	kern_hyp_va     x0
+-	ldr     x2, [x0, #KVM_VTTBR]
+-	msr     vttbr_el2, x2
+-	isb
+-
+-	tlbi    vmalls12e1is
+-	dsb     ish
+-	isb
+-
+-	msr     vttbr_el2, xzr
+-	ret
+-ENDPROC(__kvm_tlb_flush_vmid)
+-
+-ENTRY(__kvm_flush_vm_context)
+-	dsb	ishst
+-	tlbi	alle1is
+-	ic	ialluis
+-	dsb	ish
+-	ret
+-ENDPROC(__kvm_flush_vm_context)
+-
+-__kvm_hyp_panic:
+-	// Stash PAR_EL1 before corrupting it in __restore_sysregs
+-	mrs	x0, par_el1
+-	push	x0, xzr
+-
+-	// Guess the context by looking at VTTBR:
+-	// If zero, then we're already a host.
+-	// Otherwise restore a minimal host context before panicing.
+-	mrs	x0, vttbr_el2
+-	cbz	x0, 1f
+-
+-	mrs	x0, tpidr_el2
+-
+-	deactivate_traps
+-	deactivate_vm
+-
+-	ldr	x2, [x0, #VCPU_HOST_CONTEXT]
+-	kern_hyp_va x2
+-
+-	bl __restore_sysregs
+-
+-	/*
+-	 * Make sure we have a valid host stack, and don't leave junk in the
+-	 * frame pointer that will give us a misleading host stack unwinding.
+-	 */
+-	ldr	x22, [x2, #CPU_GP_REG_OFFSET(CPU_SP_EL1)]
+-	msr	sp_el1, x22
+-	mov	x29, xzr
+-
+-1:	adr	x0, __hyp_panic_str
+-	adr	x1, 2f
+-	ldp	x2, x3, [x1]
+-	sub	x0, x0, x2
+-	add	x0, x0, x3
+-	mrs	x1, spsr_el2
+-	mrs	x2, elr_el2
+-	mrs	x3, esr_el2
+-	mrs	x4, far_el2
+-	mrs	x5, hpfar_el2
+-	pop	x6, xzr		// active context PAR_EL1
+-	mrs	x7, tpidr_el2
+-
+-	mov	lr, #(PSR_F_BIT | PSR_I_BIT | PSR_A_BIT | PSR_D_BIT |\
+-		      PSR_MODE_EL1h)
+-	msr	spsr_el2, lr
+-	ldr	lr, =panic
+-	msr	elr_el2, lr
+-	eret
+-
+-	.align	3
+-2:	.quad	HYP_PAGE_OFFSET
+-	.quad	PAGE_OFFSET
+-ENDPROC(__kvm_hyp_panic)
+-
+-__hyp_panic_str:
+-	.ascii	"HYP panic:\nPS:%08x PC:%016x ESR:%08x\nFAR:%016x HPFAR:%016x PAR:%016x\nVCPU:%p\n\0"
+-
+-	.align	2
+ 
+ /*
+  * u64 kvm_call_hyp(void *hypfn, ...);
+@@ -934,7 +31,7 @@ __hyp_panic_str:
+  * passed as x0, x1, and x2 (a maximum of 3 arguments in addition to the
+  * function pointer can be passed).  The function being called must be mapped
+  * in Hyp mode (see init_hyp_mode in arch/arm/kvm/arm.c).  Return values are
+- * passed in r0 and r1.
++ * passed in x0.
+  *
+  * A function pointer with a value of 0 has a special meaning, and is
+  * used to implement __hyp_get_vectors in the same way as in
+@@ -944,179 +41,3 @@ ENTRY(kvm_call_hyp)
+ 	hvc	#0
+ 	ret
+ ENDPROC(kvm_call_hyp)
+-
+-.macro invalid_vector	label, target
+-	.align	2
+-\label:
+-	b \target
+-ENDPROC(\label)
+-.endm
+-
+-	/* None of these should ever happen */
+-	invalid_vector	el2t_sync_invalid, __kvm_hyp_panic
+-	invalid_vector	el2t_irq_invalid, __kvm_hyp_panic
+-	invalid_vector	el2t_fiq_invalid, __kvm_hyp_panic
+-	invalid_vector	el2t_error_invalid, __kvm_hyp_panic
+-	invalid_vector	el2h_sync_invalid, __kvm_hyp_panic
+-	invalid_vector	el2h_irq_invalid, __kvm_hyp_panic
+-	invalid_vector	el2h_fiq_invalid, __kvm_hyp_panic
+-	invalid_vector	el2h_error_invalid, __kvm_hyp_panic
+-	invalid_vector	el1_sync_invalid, __kvm_hyp_panic
+-	invalid_vector	el1_irq_invalid, __kvm_hyp_panic
+-	invalid_vector	el1_fiq_invalid, __kvm_hyp_panic
+-	invalid_vector	el1_error_invalid, __kvm_hyp_panic
+-
+-el1_sync:					// Guest trapped into EL2
+-	push	x0, x1
+-	push	x2, x3
+-
+-	mrs	x1, esr_el2
+-	lsr	x2, x1, #ESR_ELx_EC_SHIFT
+-
+-	cmp	x2, #ESR_ELx_EC_HVC64
+-	b.ne	el1_trap
+-
+-	mrs	x3, vttbr_el2			// If vttbr is valid, the 64bit guest
+-	cbnz	x3, el1_trap			// called HVC
+-
+-	/* Here, we're pretty sure the host called HVC. */
+-	pop	x2, x3
+-	pop	x0, x1
+-
+-	/* Check for __hyp_get_vectors */
+-	cbnz	x0, 1f
+-	mrs	x0, vbar_el2
+-	b	2f
+-
+-1:	push	lr, xzr
+-
+-	/*
+-	 * Compute the function address in EL2, and shuffle the parameters.
+-	 */
+-	kern_hyp_va	x0
+-	mov	lr, x0
+-	mov	x0, x1
+-	mov	x1, x2
+-	mov	x2, x3
+-	blr	lr
+-
+-	pop	lr, xzr
+-2:	eret
+-
+-el1_trap:
+-	/*
+-	 * x1: ESR
+-	 * x2: ESR_EC
+-	 */
+-
+-	/* Guest accessed VFP/SIMD registers, save host, restore Guest */
+-	cmp	x2, #ESR_ELx_EC_FP_ASIMD
+-	b.eq	switch_to_guest_fpsimd
+-
+-	cmp	x2, #ESR_ELx_EC_DABT_LOW
+-	mov	x0, #ESR_ELx_EC_IABT_LOW
+-	ccmp	x2, x0, #4, ne
+-	b.ne	1f		// Not an abort we care about
+-
+-	/* This is an abort. Check for permission fault */
+-alternative_if_not ARM64_WORKAROUND_834220
+-	and	x2, x1, #ESR_ELx_FSC_TYPE
+-	cmp	x2, #FSC_PERM
+-	b.ne	1f		// Not a permission fault
+-alternative_else
+-	nop			// Use the permission fault path to
+-	nop			// check for a valid S1 translation,
+-	nop			// regardless of the ESR value.
+-alternative_endif
+-
+-	/*
+-	 * Check for Stage-1 page table walk, which is guaranteed
+-	 * to give a valid HPFAR_EL2.
+-	 */
+-	tbnz	x1, #7, 1f	// S1PTW is set
+-
+-	/* Preserve PAR_EL1 */
+-	mrs	x3, par_el1
+-	push	x3, xzr
+-
+-	/*
+-	 * Permission fault, HPFAR_EL2 is invalid.
+-	 * Resolve the IPA the hard way using the guest VA.
+-	 * Stage-1 translation already validated the memory access rights.
+-	 * As such, we can use the EL1 translation regime, and don't have
+-	 * to distinguish between EL0 and EL1 access.
+-	 */
+-	mrs	x2, far_el2
+-	at	s1e1r, x2
+-	isb
+-
+-	/* Read result */
+-	mrs	x3, par_el1
+-	pop	x0, xzr			// Restore PAR_EL1 from the stack
+-	msr	par_el1, x0
+-	tbnz	x3, #0, 3f		// Bail out if we failed the translation
+-	ubfx	x3, x3, #12, #36	// Extract IPA
+-	lsl	x3, x3, #4		// and present it like HPFAR
+-	b	2f
+-
+-1:	mrs	x3, hpfar_el2
+-	mrs	x2, far_el2
+-
+-2:	mrs	x0, tpidr_el2
+-	str	w1, [x0, #VCPU_ESR_EL2]
+-	str	x2, [x0, #VCPU_FAR_EL2]
+-	str	x3, [x0, #VCPU_HPFAR_EL2]
+-
+-	mov	x1, #ARM_EXCEPTION_TRAP
+-	b	__kvm_vcpu_return
+-
+-	/*
+-	 * Translation failed. Just return to the guest and
+-	 * let it fault again. Another CPU is probably playing
+-	 * behind our back.
+-	 */
+-3:	pop	x2, x3
+-	pop	x0, x1
+-
+-	eret
+-
+-el1_irq:
+-	push	x0, x1
+-	push	x2, x3
+-	mrs	x0, tpidr_el2
+-	mov	x1, #ARM_EXCEPTION_IRQ
+-	b	__kvm_vcpu_return
+-
+-	.ltorg
+-
+-	.align 11
+-
+-ENTRY(__kvm_hyp_vector)
+-	ventry	el2t_sync_invalid		// Synchronous EL2t
+-	ventry	el2t_irq_invalid		// IRQ EL2t
+-	ventry	el2t_fiq_invalid		// FIQ EL2t
+-	ventry	el2t_error_invalid		// Error EL2t
+-
+-	ventry	el2h_sync_invalid		// Synchronous EL2h
+-	ventry	el2h_irq_invalid		// IRQ EL2h
+-	ventry	el2h_fiq_invalid		// FIQ EL2h
+-	ventry	el2h_error_invalid		// Error EL2h
+-
+-	ventry	el1_sync			// Synchronous 64-bit EL1
+-	ventry	el1_irq				// IRQ 64-bit EL1
+-	ventry	el1_fiq_invalid			// FIQ 64-bit EL1
+-	ventry	el1_error_invalid		// Error 64-bit EL1
+-
+-	ventry	el1_sync			// Synchronous 32-bit EL1
+-	ventry	el1_irq				// IRQ 32-bit EL1
+-	ventry	el1_fiq_invalid			// FIQ 32-bit EL1
+-	ventry	el1_error_invalid		// Error 32-bit EL1
+-ENDPROC(__kvm_hyp_vector)
+-
+-
+-ENTRY(__kvm_get_mdcr_el2)
+-	mrs	x0, mdcr_el2
+-	ret
+-ENDPROC(__kvm_get_mdcr_el2)
+-
+-	.popsection
+diff --git a/arch/arm64/kvm/vgic-v2-switch.S b/arch/arm64/kvm/vgic-v2-switch.S
+deleted file mode 100644
+index 3f00071..0000000
+--- a/arch/arm64/kvm/vgic-v2-switch.S
++++ /dev/null
+@@ -1,134 +0,0 @@
+-/*
+- * Copyright (C) 2012,2013 - ARM Ltd
+- * Author: Marc Zyngier <marc.zyngier@arm.com>
+- *
+- * This program is free software; you can redistribute it and/or modify
+- * it under the terms of the GNU General Public License version 2 as
+- * published by the Free Software Foundation.
+- *
+- * This program is distributed in the hope that it will be useful,
+- * but WITHOUT ANY WARRANTY; without even the implied warranty of
+- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- * GNU General Public License for more details.
+- *
+- * You should have received a copy of the GNU General Public License
+- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+- */
+-
+-#include <linux/linkage.h>
+-#include <linux/irqchip/arm-gic.h>
+-
+-#include <asm/assembler.h>
+-#include <asm/memory.h>
+-#include <asm/asm-offsets.h>
+-#include <asm/kvm.h>
+-#include <asm/kvm_asm.h>
+-#include <asm/kvm_arm.h>
+-#include <asm/kvm_mmu.h>
+-
+-	.text
+-	.pushsection	.hyp.text, "ax"
+-
+-/*
+- * Save the VGIC CPU state into memory
+- * x0: Register pointing to VCPU struct
+- * Do not corrupt x1!!!
+- */
+-ENTRY(__save_vgic_v2_state)
+-__save_vgic_v2_state:
+-	/* Get VGIC VCTRL base into x2 */
+-	ldr	x2, [x0, #VCPU_KVM]
+-	kern_hyp_va	x2
+-	ldr	x2, [x2, #KVM_VGIC_VCTRL]
+-	kern_hyp_va	x2
+-	cbz	x2, 2f		// disabled
+-
+-	/* Compute the address of struct vgic_cpu */
+-	add	x3, x0, #VCPU_VGIC_CPU
+-
+-	/* Save all interesting registers */
+-	ldr	w5, [x2, #GICH_VMCR]
+-	ldr	w6, [x2, #GICH_MISR]
+-	ldr	w7, [x2, #GICH_EISR0]
+-	ldr	w8, [x2, #GICH_EISR1]
+-	ldr	w9, [x2, #GICH_ELRSR0]
+-	ldr	w10, [x2, #GICH_ELRSR1]
+-	ldr	w11, [x2, #GICH_APR]
+-CPU_BE(	rev	w5,  w5  )
+-CPU_BE(	rev	w6,  w6  )
+-CPU_BE(	rev	w7,  w7  )
+-CPU_BE(	rev	w8,  w8  )
+-CPU_BE(	rev	w9,  w9  )
+-CPU_BE(	rev	w10, w10 )
+-CPU_BE(	rev	w11, w11 )
+-
+-	str	w5, [x3, #VGIC_V2_CPU_VMCR]
+-	str	w6, [x3, #VGIC_V2_CPU_MISR]
+-CPU_LE(	str	w7, [x3, #VGIC_V2_CPU_EISR] )
+-CPU_LE(	str	w8, [x3, #(VGIC_V2_CPU_EISR + 4)] )
+-CPU_LE(	str	w9, [x3, #VGIC_V2_CPU_ELRSR] )
+-CPU_LE(	str	w10, [x3, #(VGIC_V2_CPU_ELRSR + 4)] )
+-CPU_BE(	str	w7, [x3, #(VGIC_V2_CPU_EISR + 4)] )
+-CPU_BE(	str	w8, [x3, #VGIC_V2_CPU_EISR] )
+-CPU_BE(	str	w9, [x3, #(VGIC_V2_CPU_ELRSR + 4)] )
+-CPU_BE(	str	w10, [x3, #VGIC_V2_CPU_ELRSR] )
+-	str	w11, [x3, #VGIC_V2_CPU_APR]
+-
+-	/* Clear GICH_HCR */
+-	str	wzr, [x2, #GICH_HCR]
+-
+-	/* Save list registers */
+-	add	x2, x2, #GICH_LR0
+-	ldr	w4, [x3, #VGIC_CPU_NR_LR]
+-	add	x3, x3, #VGIC_V2_CPU_LR
+-1:	ldr	w5, [x2], #4
+-CPU_BE(	rev	w5, w5 )
+-	str	w5, [x3], #4
+-	sub	w4, w4, #1
+-	cbnz	w4, 1b
+-2:
+-	ret
+-ENDPROC(__save_vgic_v2_state)
+-
+-/*
+- * Restore the VGIC CPU state from memory
+- * x0: Register pointing to VCPU struct
+- */
+-ENTRY(__restore_vgic_v2_state)
+-__restore_vgic_v2_state:
+-	/* Get VGIC VCTRL base into x2 */
+-	ldr	x2, [x0, #VCPU_KVM]
+-	kern_hyp_va	x2
+-	ldr	x2, [x2, #KVM_VGIC_VCTRL]
+-	kern_hyp_va	x2
+-	cbz	x2, 2f		// disabled
+-
+-	/* Compute the address of struct vgic_cpu */
+-	add	x3, x0, #VCPU_VGIC_CPU
+-
+-	/* We only restore a minimal set of registers */
+-	ldr	w4, [x3, #VGIC_V2_CPU_HCR]
+-	ldr	w5, [x3, #VGIC_V2_CPU_VMCR]
+-	ldr	w6, [x3, #VGIC_V2_CPU_APR]
+-CPU_BE(	rev	w4, w4 )
+-CPU_BE(	rev	w5, w5 )
+-CPU_BE(	rev	w6, w6 )
+-
+-	str	w4, [x2, #GICH_HCR]
+-	str	w5, [x2, #GICH_VMCR]
+-	str	w6, [x2, #GICH_APR]
+-
+-	/* Restore list registers */
+-	add	x2, x2, #GICH_LR0
+-	ldr	w4, [x3, #VGIC_CPU_NR_LR]
+-	add	x3, x3, #VGIC_V2_CPU_LR
+-1:	ldr	w5, [x3], #4
+-CPU_BE(	rev	w5, w5 )
+-	str	w5, [x2], #4
+-	sub	w4, w4, #1
+-	cbnz	w4, 1b
+-2:
+-	ret
+-ENDPROC(__restore_vgic_v2_state)
+-
+-	.popsection
+diff --git a/arch/arm64/kvm/vgic-v3-switch.S b/arch/arm64/kvm/vgic-v3-switch.S
+deleted file mode 100644
+index 3c20730..0000000
+--- a/arch/arm64/kvm/vgic-v3-switch.S
++++ /dev/null
+@@ -1,269 +0,0 @@
+-/*
+- * Copyright (C) 2012,2013 - ARM Ltd
+- * Author: Marc Zyngier <marc.zyngier@arm.com>
+- *
+- * This program is free software; you can redistribute it and/or modify
+- * it under the terms of the GNU General Public License version 2 as
+- * published by the Free Software Foundation.
+- *
+- * This program is distributed in the hope that it will be useful,
+- * but WITHOUT ANY WARRANTY; without even the implied warranty of
+- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- * GNU General Public License for more details.
+- *
+- * You should have received a copy of the GNU General Public License
+- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+- */
+-
+-#include <linux/linkage.h>
+-#include <linux/irqchip/arm-gic-v3.h>
+-
+-#include <asm/assembler.h>
+-#include <asm/memory.h>
+-#include <asm/asm-offsets.h>
+-#include <asm/kvm.h>
+-#include <asm/kvm_asm.h>
+-#include <asm/kvm_arm.h>
+-
+-	.text
+-	.pushsection	.hyp.text, "ax"
+-
+-/*
+- * We store LRs in reverse order to let the CPU deal with streaming
+- * access. Use this macro to make it look saner...
+- */
+-#define LR_OFFSET(n)	(VGIC_V3_CPU_LR + (15 - n) * 8)
+-
+-/*
+- * Save the VGIC CPU state into memory
+- * x0: Register pointing to VCPU struct
+- * Do not corrupt x1!!!
+- */
+-.macro	save_vgic_v3_state
+-	// Compute the address of struct vgic_cpu
+-	add	x3, x0, #VCPU_VGIC_CPU
+-
+-	// Make sure stores to the GIC via the memory mapped interface
+-	// are now visible to the system register interface
+-	dsb	st
+-
+-	// Save all interesting registers
+-	mrs_s	x5, ICH_VMCR_EL2
+-	mrs_s	x6, ICH_MISR_EL2
+-	mrs_s	x7, ICH_EISR_EL2
+-	mrs_s	x8, ICH_ELSR_EL2
+-
+-	str	w5, [x3, #VGIC_V3_CPU_VMCR]
+-	str	w6, [x3, #VGIC_V3_CPU_MISR]
+-	str	w7, [x3, #VGIC_V3_CPU_EISR]
+-	str	w8, [x3, #VGIC_V3_CPU_ELRSR]
+-
+-	msr_s	ICH_HCR_EL2, xzr
+-
+-	mrs_s	x21, ICH_VTR_EL2
+-	mvn	w22, w21
+-	ubfiz	w23, w22, 2, 4	// w23 = (15 - ListRegs) * 4
+-
+-	adr	x24, 1f
+-	add	x24, x24, x23
+-	br	x24
+-
+-1:
+-	mrs_s	x20, ICH_LR15_EL2
+-	mrs_s	x19, ICH_LR14_EL2
+-	mrs_s	x18, ICH_LR13_EL2
+-	mrs_s	x17, ICH_LR12_EL2
+-	mrs_s	x16, ICH_LR11_EL2
+-	mrs_s	x15, ICH_LR10_EL2
+-	mrs_s	x14, ICH_LR9_EL2
+-	mrs_s	x13, ICH_LR8_EL2
+-	mrs_s	x12, ICH_LR7_EL2
+-	mrs_s	x11, ICH_LR6_EL2
+-	mrs_s	x10, ICH_LR5_EL2
+-	mrs_s	x9, ICH_LR4_EL2
+-	mrs_s	x8, ICH_LR3_EL2
+-	mrs_s	x7, ICH_LR2_EL2
+-	mrs_s	x6, ICH_LR1_EL2
+-	mrs_s	x5, ICH_LR0_EL2
+-
+-	adr	x24, 1f
+-	add	x24, x24, x23
+-	br	x24
+-
+-1:
+-	str	x20, [x3, #LR_OFFSET(15)]
+-	str	x19, [x3, #LR_OFFSET(14)]
+-	str	x18, [x3, #LR_OFFSET(13)]
+-	str	x17, [x3, #LR_OFFSET(12)]
+-	str	x16, [x3, #LR_OFFSET(11)]
+-	str	x15, [x3, #LR_OFFSET(10)]
+-	str	x14, [x3, #LR_OFFSET(9)]
+-	str	x13, [x3, #LR_OFFSET(8)]
+-	str	x12, [x3, #LR_OFFSET(7)]
+-	str	x11, [x3, #LR_OFFSET(6)]
+-	str	x10, [x3, #LR_OFFSET(5)]
+-	str	x9, [x3, #LR_OFFSET(4)]
+-	str	x8, [x3, #LR_OFFSET(3)]
+-	str	x7, [x3, #LR_OFFSET(2)]
+-	str	x6, [x3, #LR_OFFSET(1)]
+-	str	x5, [x3, #LR_OFFSET(0)]
+-
+-	tbnz	w21, #29, 6f	// 6 bits
+-	tbz	w21, #30, 5f	// 5 bits
+-				// 7 bits
+-	mrs_s	x20, ICH_AP0R3_EL2
+-	str	w20, [x3, #(VGIC_V3_CPU_AP0R + 3*4)]
+-	mrs_s	x19, ICH_AP0R2_EL2
+-	str	w19, [x3, #(VGIC_V3_CPU_AP0R + 2*4)]
+-6:	mrs_s	x18, ICH_AP0R1_EL2
+-	str	w18, [x3, #(VGIC_V3_CPU_AP0R + 1*4)]
+-5:	mrs_s	x17, ICH_AP0R0_EL2
+-	str	w17, [x3, #VGIC_V3_CPU_AP0R]
+-
+-	tbnz	w21, #29, 6f	// 6 bits
+-	tbz	w21, #30, 5f	// 5 bits
+-				// 7 bits
+-	mrs_s	x20, ICH_AP1R3_EL2
+-	str	w20, [x3, #(VGIC_V3_CPU_AP1R + 3*4)]
+-	mrs_s	x19, ICH_AP1R2_EL2
+-	str	w19, [x3, #(VGIC_V3_CPU_AP1R + 2*4)]
+-6:	mrs_s	x18, ICH_AP1R1_EL2
+-	str	w18, [x3, #(VGIC_V3_CPU_AP1R + 1*4)]
+-5:	mrs_s	x17, ICH_AP1R0_EL2
+-	str	w17, [x3, #VGIC_V3_CPU_AP1R]
+-
+-	// Restore SRE_EL1 access and re-enable SRE at EL1.
+-	mrs_s	x5, ICC_SRE_EL2
+-	orr	x5, x5, #ICC_SRE_EL2_ENABLE
+-	msr_s	ICC_SRE_EL2, x5
+-	isb
+-	mov	x5, #1
+-	msr_s	ICC_SRE_EL1, x5
+-.endm
+-
+-/*
+- * Restore the VGIC CPU state from memory
+- * x0: Register pointing to VCPU struct
+- */
+-.macro	restore_vgic_v3_state
+-	// Compute the address of struct vgic_cpu
+-	add	x3, x0, #VCPU_VGIC_CPU
+-
+-	// Restore all interesting registers
+-	ldr	w4, [x3, #VGIC_V3_CPU_HCR]
+-	ldr	w5, [x3, #VGIC_V3_CPU_VMCR]
+-	ldr	w25, [x3, #VGIC_V3_CPU_SRE]
+-
+-	msr_s	ICC_SRE_EL1, x25
+-
+-	// make sure SRE is valid before writing the other registers
+-	isb
+-
+-	msr_s	ICH_HCR_EL2, x4
+-	msr_s	ICH_VMCR_EL2, x5
+-
+-	mrs_s	x21, ICH_VTR_EL2
+-
+-	tbnz	w21, #29, 6f	// 6 bits
+-	tbz	w21, #30, 5f	// 5 bits
+-				// 7 bits
+-	ldr	w20, [x3, #(VGIC_V3_CPU_AP1R + 3*4)]
+-	msr_s	ICH_AP1R3_EL2, x20
+-	ldr	w19, [x3, #(VGIC_V3_CPU_AP1R + 2*4)]
+-	msr_s	ICH_AP1R2_EL2, x19
+-6:	ldr	w18, [x3, #(VGIC_V3_CPU_AP1R + 1*4)]
+-	msr_s	ICH_AP1R1_EL2, x18
+-5:	ldr	w17, [x3, #VGIC_V3_CPU_AP1R]
+-	msr_s	ICH_AP1R0_EL2, x17
+-
+-	tbnz	w21, #29, 6f	// 6 bits
+-	tbz	w21, #30, 5f	// 5 bits
+-				// 7 bits
+-	ldr	w20, [x3, #(VGIC_V3_CPU_AP0R + 3*4)]
+-	msr_s	ICH_AP0R3_EL2, x20
+-	ldr	w19, [x3, #(VGIC_V3_CPU_AP0R + 2*4)]
+-	msr_s	ICH_AP0R2_EL2, x19
+-6:	ldr	w18, [x3, #(VGIC_V3_CPU_AP0R + 1*4)]
+-	msr_s	ICH_AP0R1_EL2, x18
+-5:	ldr	w17, [x3, #VGIC_V3_CPU_AP0R]
+-	msr_s	ICH_AP0R0_EL2, x17
+-
+-	and	w22, w21, #0xf
+-	mvn	w22, w21
+-	ubfiz	w23, w22, 2, 4	// w23 = (15 - ListRegs) * 4
+-
+-	adr	x24, 1f
+-	add	x24, x24, x23
+-	br	x24
+-
+-1:
+-	ldr	x20, [x3, #LR_OFFSET(15)]
+-	ldr	x19, [x3, #LR_OFFSET(14)]
+-	ldr	x18, [x3, #LR_OFFSET(13)]
+-	ldr	x17, [x3, #LR_OFFSET(12)]
+-	ldr	x16, [x3, #LR_OFFSET(11)]
+-	ldr	x15, [x3, #LR_OFFSET(10)]
+-	ldr	x14, [x3, #LR_OFFSET(9)]
+-	ldr	x13, [x3, #LR_OFFSET(8)]
+-	ldr	x12, [x3, #LR_OFFSET(7)]
+-	ldr	x11, [x3, #LR_OFFSET(6)]
+-	ldr	x10, [x3, #LR_OFFSET(5)]
+-	ldr	x9, [x3, #LR_OFFSET(4)]
+-	ldr	x8, [x3, #LR_OFFSET(3)]
+-	ldr	x7, [x3, #LR_OFFSET(2)]
+-	ldr	x6, [x3, #LR_OFFSET(1)]
+-	ldr	x5, [x3, #LR_OFFSET(0)]
+-
+-	adr	x24, 1f
+-	add	x24, x24, x23
+-	br	x24
+-
+-1:
+-	msr_s	ICH_LR15_EL2, x20
+-	msr_s	ICH_LR14_EL2, x19
+-	msr_s	ICH_LR13_EL2, x18
+-	msr_s	ICH_LR12_EL2, x17
+-	msr_s	ICH_LR11_EL2, x16
+-	msr_s	ICH_LR10_EL2, x15
+-	msr_s	ICH_LR9_EL2,  x14
+-	msr_s	ICH_LR8_EL2,  x13
+-	msr_s	ICH_LR7_EL2,  x12
+-	msr_s	ICH_LR6_EL2,  x11
+-	msr_s	ICH_LR5_EL2,  x10
+-	msr_s	ICH_LR4_EL2,   x9
+-	msr_s	ICH_LR3_EL2,   x8
+-	msr_s	ICH_LR2_EL2,   x7
+-	msr_s	ICH_LR1_EL2,   x6
+-	msr_s	ICH_LR0_EL2,   x5
+-
+-	// Ensure that the above will have reached the
+-	// (re)distributors. This ensure the guest will read
+-	// the correct values from the memory-mapped interface.
+-	isb
+-	dsb	sy
+-
+-	// Prevent the guest from touching the GIC system registers
+-	// if SRE isn't enabled for GICv3 emulation
+-	cbnz	x25, 1f
+-	mrs_s	x5, ICC_SRE_EL2
+-	and	x5, x5, #~ICC_SRE_EL2_ENABLE
+-	msr_s	ICC_SRE_EL2, x5
+-1:
+-.endm
+-
+-ENTRY(__save_vgic_v3_state)
+-	save_vgic_v3_state
+-	ret
+-ENDPROC(__save_vgic_v3_state)
+-
+-ENTRY(__restore_vgic_v3_state)
+-	restore_vgic_v3_state
+-	ret
+-ENDPROC(__restore_vgic_v3_state)
+-
+-ENTRY(__vgic_v3_get_ich_vtr_el2)
+-	mrs_s	x0, ICH_VTR_EL2
+-	ret
+-ENDPROC(__vgic_v3_get_ich_vtr_el2)
+-
+-	.popsection
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0054-arm-arm64-Add-new-is_kernel_in_hyp_mode-predicate.patch b/tools/kdump/0054-arm-arm64-Add-new-is_kernel_in_hyp_mode-predicate.patch
new file mode 100644
index 0000000..de2dc9a
--- /dev/null
+++ b/tools/kdump/0054-arm-arm64-Add-new-is_kernel_in_hyp_mode-predicate.patch
@@ -0,0 +1,78 @@
+From bb38997e6a319cf324a0d0ead9188283b7a19f6f Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 9 Jun 2014 19:47:09 +0100
+Subject: [PATCH 054/123] arm/arm64: Add new is_kernel_in_hyp_mode predicate
+
+With ARMv8.1 VHE extension, it will be possible to run the kernel
+at EL2 (aka HYP mode). In order for the kernel to easily find out
+where it is running, add a new predicate that returns whether or
+not the kernel is in HYP mode.
+
+For completeness, the 32bit code also get such a predicate (always
+returning false) so that code common to both architecture (timers,
+KVM) can use it transparently.
+
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 82deae0fc8ba256c1061dd4de42f0ef6cb9f954f)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm/include/asm/virt.h
+---
+ arch/arm/include/asm/virt.h   |  9 +++++++++
+ arch/arm64/include/asm/virt.h | 10 ++++++++++
+ 2 files changed, 19 insertions(+)
+
+diff --git a/arch/arm/include/asm/virt.h b/arch/arm/include/asm/virt.h
+index 4371f45..d4ceaf5 100644
+--- a/arch/arm/include/asm/virt.h
++++ b/arch/arm/include/asm/virt.h
+@@ -74,6 +74,15 @@ static inline bool is_hyp_mode_mismatched(void)
+ {
+ 	return !!(__boot_cpu_mode & BOOT_CPU_MODE_MISMATCH);
+ }
++
++static inline bool is_kernel_in_hyp_mode(void)
++{
++	return false;
++}
++
++/* The section containing the hypervisor text */
++extern char __hyp_text_start[];
++extern char __hyp_text_end[];
+ #endif
+ 
+ #endif /* __ASSEMBLY__ */
+diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
+index 7a5df52..9f22dd6 100644
+--- a/arch/arm64/include/asm/virt.h
++++ b/arch/arm64/include/asm/virt.h
+@@ -23,6 +23,8 @@
+ 
+ #ifndef __ASSEMBLY__
+ 
++#include <asm/ptrace.h>
++
+ /*
+  * __boot_cpu_mode records what mode CPUs were booted in.
+  * A correctly-implemented bootloader must start all CPUs in the same mode:
+@@ -50,6 +52,14 @@ static inline bool is_hyp_mode_mismatched(void)
+ 	return __boot_cpu_mode[0] != __boot_cpu_mode[1];
+ }
+ 
++static inline bool is_kernel_in_hyp_mode(void)
++{
++	u64 el;
++
++	asm("mrs %0, CurrentEL" : "=r" (el));
++	return el == CurrentEL_EL2;
++}
++
+ /* The section containing the hypervisor text */
+ extern char __hyp_text_start[];
+ extern char __hyp_text_end[];
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0055-arm64-Add-ARM64_HAS_VIRT_HOST_EXTN-feature.patch b/tools/kdump/0055-arm64-Add-ARM64_HAS_VIRT_HOST_EXTN-feature.patch
new file mode 100644
index 0000000..1dac52d
--- /dev/null
+++ b/tools/kdump/0055-arm64-Add-ARM64_HAS_VIRT_HOST_EXTN-feature.patch
@@ -0,0 +1,80 @@
+From f60b9674e10653eb9027092c449e4e3a7be5e656 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Thu, 29 Jan 2015 11:24:05 +0000
+Subject: [PATCH 055/123] arm64: Add ARM64_HAS_VIRT_HOST_EXTN feature
+
+Add a new ARM64_HAS_VIRT_HOST_EXTN features to indicate that the
+CPU has the ARMv8.1 VHE capability.
+
+This will be used to trigger kernel patching in KVM.
+
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit d88701bea3664cea99b8b7380f63a3bd0ec3ead3)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/include/asm/cpufeature.h
+---
+ arch/arm64/include/asm/cpufeature.h |  7 ++++++-
+ arch/arm64/kernel/cpufeature.c      | 11 +++++++++++
+ 2 files changed, 17 insertions(+), 1 deletion(-)
+
+diff --git a/arch/arm64/include/asm/cpufeature.h b/arch/arm64/include/asm/cpufeature.h
+index 8884b5d..7abfd41 100644
+--- a/arch/arm64/include/asm/cpufeature.h
++++ b/arch/arm64/include/asm/cpufeature.h
+@@ -32,7 +32,12 @@
+ #define ARM64_WORKAROUND_834220			7
+ #define ARM64_WORKAROUND_CAVIUM_27456		8
+ 
+-#define ARM64_NCAPS				9
++/* #define ARM64_HAS_NO_HW_PREFETCH		8 */
++/* #define ARM64_HAS_UAO			9 */
++/* #define ARM64_ALT_PAN_NOT_UAO		10 */
++#define ARM64_HAS_VIRT_HOST_EXTN		11
++
++#define ARM64_NCAPS				12
+ 
+ #ifndef __ASSEMBLY__
+ 
+diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
+index 2735bf8..b9c7176 100644
+--- a/arch/arm64/kernel/cpufeature.c
++++ b/arch/arm64/kernel/cpufeature.c
+@@ -28,6 +28,7 @@
+ #include <asm/cpu_ops.h>
+ #include <asm/processor.h>
+ #include <asm/sysreg.h>
++#include <asm/virt.h>
+ 
+ unsigned long elf_hwcap __read_mostly;
+ EXPORT_SYMBOL_GPL(elf_hwcap);
+@@ -623,6 +624,11 @@ static bool has_useable_gicv3_cpuif(const struct arm64_cpu_capabilities *entry)
+ 	return has_sre;
+ }
+ 
++static bool runs_at_el2(const struct arm64_cpu_capabilities *entry)
++{
++	return is_kernel_in_hyp_mode();
++}
++
+ static const struct arm64_cpu_capabilities arm64_features[] = {
+ 	{
+ 		.desc = "GIC system register CPU interface",
+@@ -653,6 +659,11 @@ static const struct arm64_cpu_capabilities arm64_features[] = {
+ 		.min_field_value = 2,
+ 	},
+ #endif /* CONFIG_AS_LSE && CONFIG_ARM64_LSE_ATOMICS */
++	{
++		.desc = "Virtualization Host Extensions",
++		.capability = ARM64_HAS_VIRT_HOST_EXTN,
++		.matches = runs_at_el2,
++	},
+ 	{},
+ };
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0056-arm64-KVM-VHE-Patch-out-use-of-HVC.patch b/tools/kdump/0056-arm64-KVM-VHE-Patch-out-use-of-HVC.patch
new file mode 100644
index 0000000..cbd744c
--- /dev/null
+++ b/tools/kdump/0056-arm64-KVM-VHE-Patch-out-use-of-HVC.patch
@@ -0,0 +1,110 @@
+From c56a387d114f5352cd5b4250dde8d2d8f09ba052 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Thu, 29 Jan 2015 13:52:12 +0000
+Subject: [PATCH 056/123] arm64: KVM: VHE: Patch out use of HVC
+
+With VHE, the host never issues an HVC instruction to get into the
+KVM code, as we can simply branch there.
+
+Use runtime code patching to simplify things a bit.
+
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit b81125c791a2958cc60ae968fc1cdea82b7cd3b0)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp.S           |  7 +++++++
+ arch/arm64/kvm/hyp/hyp-entry.S | 40 +++++++++++++++++++++++++++++++---------
+ 2 files changed, 38 insertions(+), 9 deletions(-)
+
+diff --git a/arch/arm64/kvm/hyp.S b/arch/arm64/kvm/hyp.S
+index 0ccdcbb..0689a74 100644
+--- a/arch/arm64/kvm/hyp.S
++++ b/arch/arm64/kvm/hyp.S
+@@ -17,7 +17,9 @@
+ 
+ #include <linux/linkage.h>
+ 
++#include <asm/alternative.h>
+ #include <asm/assembler.h>
++#include <asm/cpufeature.h>
+ 
+ /*
+  * u64 kvm_call_hyp(void *hypfn, ...);
+@@ -38,6 +40,11 @@
+  * arch/arm64/kernel/hyp_stub.S.
+  */
+ ENTRY(kvm_call_hyp)
++alternative_if_not ARM64_HAS_VIRT_HOST_EXTN	
+ 	hvc	#0
+ 	ret
++alternative_else
++	b	__vhe_hyp_call
++	nop
++alternative_endif
+ ENDPROC(kvm_call_hyp)
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+index 10d6d2a..8950cdb 100644
+--- a/arch/arm64/kvm/hyp/hyp-entry.S
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -38,6 +38,34 @@
+ 	ldp	x0, x1, [sp], #16
+ .endm
+ 
++.macro do_el2_call
++	/*
++	 * Shuffle the parameters before calling the function
++	 * pointed to in x0. Assumes parameters in x[1,2,3].
++	 */
++	sub	sp, sp, #16
++	str	lr, [sp]
++	mov	lr, x0
++	mov	x0, x1
++	mov	x1, x2
++	mov	x2, x3
++	blr	lr
++	ldr	lr, [sp]
++	add	sp, sp, #16
++.endm
++
++ENTRY(__vhe_hyp_call)
++	do_el2_call
++	/*
++	 * We used to rely on having an exception return to get
++	 * an implicit isb. In the E2H case, we don't have it anymore.
++	 * rather than changing all the leaf functions, just do it here
++	 * before returning to the rest of the kernel.
++	 */
++	isb
++	ret
++ENDPROC(__vhe_hyp_call)
++	
+ el1_sync:				// Guest trapped into EL2
+ 	save_x0_to_x3
+ 
+@@ -58,19 +86,13 @@ el1_sync:				// Guest trapped into EL2
+ 	mrs	x0, vbar_el2
+ 	b	2f
+ 
+-1:	stp	lr, xzr, [sp, #-16]!
+-
++1:
+ 	/*
+-	 * Compute the function address in EL2, and shuffle the parameters.
++	 * Perform the EL2 call
+ 	 */
+ 	kern_hyp_va	x0
+-	mov	lr, x0
+-	mov	x0, x1
+-	mov	x1, x2
+-	mov	x2, x3
+-	blr	lr
++	do_el2_call
+ 
+-	ldp	lr, xzr, [sp], #16
+ 2:	eret
+ 
+ el1_trap:
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0057-arm64-KVM-Turn-system-register-numbers-to-an-enum.patch b/tools/kdump/0057-arm64-KVM-Turn-system-register-numbers-to-an-enum.patch
new file mode 100644
index 0000000..e3b6bf2
--- /dev/null
+++ b/tools/kdump/0057-arm64-KVM-Turn-system-register-numbers-to-an-enum.patch
@@ -0,0 +1,349 @@
+From 8ad93ee95278ea04247b185b42d6edfd69b7a055 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Sun, 25 Oct 2015 19:57:11 +0000
+Subject: [PATCH 057/123] arm64: KVM: Turn system register numbers to an enum
+
+Having the system register numbers as #defines has been a pain
+since day one, as the ordering is pretty fragile, and moving
+things around leads to renumbering and epic conflict resolutions.
+
+Now that we're mostly acessing the sysreg file in C, an enum is
+a much better type to use, and we can clean things up a bit.
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 9d8415d6c148a16b6d906a96f0596851d7e4d607)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/kvm_asm.h     | 76 ---------------------------------
+ arch/arm64/include/asm/kvm_emulate.h |  1 -
+ arch/arm64/include/asm/kvm_host.h    | 81 +++++++++++++++++++++++++++++++++++-
+ arch/arm64/include/asm/kvm_mmio.h    |  1 -
+ arch/arm64/kernel/asm-offsets.c      |  1 +
+ arch/arm64/kvm/guest.c               |  1 -
+ arch/arm64/kvm/handle_exit.c         |  1 +
+ arch/arm64/kvm/hyp/debug-sr.c        |  1 +
+ arch/arm64/kvm/hyp/entry.S           |  3 +-
+ arch/arm64/kvm/hyp/sysreg-sr.c       |  1 +
+ arch/arm64/kvm/sys_regs.c            |  1 +
+ virt/kvm/arm/vgic-v3.c               |  1 +
+ 12 files changed, 87 insertions(+), 82 deletions(-)
+
+diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
+index 5e37710..52b777b 100644
+--- a/arch/arm64/include/asm/kvm_asm.h
++++ b/arch/arm64/include/asm/kvm_asm.h
+@@ -20,82 +20,6 @@
+ 
+ #include <asm/virt.h>
+ 
+-/*
+- * 0 is reserved as an invalid value.
+- * Order *must* be kept in sync with the hyp switch code.
+- */
+-#define	MPIDR_EL1	1	/* MultiProcessor Affinity Register */
+-#define	CSSELR_EL1	2	/* Cache Size Selection Register */
+-#define	SCTLR_EL1	3	/* System Control Register */
+-#define	ACTLR_EL1	4	/* Auxiliary Control Register */
+-#define	CPACR_EL1	5	/* Coprocessor Access Control */
+-#define	TTBR0_EL1	6	/* Translation Table Base Register 0 */
+-#define	TTBR1_EL1	7	/* Translation Table Base Register 1 */
+-#define	TCR_EL1		8	/* Translation Control Register */
+-#define	ESR_EL1		9	/* Exception Syndrome Register */
+-#define	AFSR0_EL1	10	/* Auxilary Fault Status Register 0 */
+-#define	AFSR1_EL1	11	/* Auxilary Fault Status Register 1 */
+-#define	FAR_EL1		12	/* Fault Address Register */
+-#define	MAIR_EL1	13	/* Memory Attribute Indirection Register */
+-#define	VBAR_EL1	14	/* Vector Base Address Register */
+-#define	CONTEXTIDR_EL1	15	/* Context ID Register */
+-#define	TPIDR_EL0	16	/* Thread ID, User R/W */
+-#define	TPIDRRO_EL0	17	/* Thread ID, User R/O */
+-#define	TPIDR_EL1	18	/* Thread ID, Privileged */
+-#define	AMAIR_EL1	19	/* Aux Memory Attribute Indirection Register */
+-#define	CNTKCTL_EL1	20	/* Timer Control Register (EL1) */
+-#define	PAR_EL1		21	/* Physical Address Register */
+-#define MDSCR_EL1	22	/* Monitor Debug System Control Register */
+-#define MDCCINT_EL1	23	/* Monitor Debug Comms Channel Interrupt Enable Reg */
+-
+-/* 32bit specific registers. Keep them at the end of the range */
+-#define	DACR32_EL2	24	/* Domain Access Control Register */
+-#define	IFSR32_EL2	25	/* Instruction Fault Status Register */
+-#define	FPEXC32_EL2	26	/* Floating-Point Exception Control Register */
+-#define	DBGVCR32_EL2	27	/* Debug Vector Catch Register */
+-#define	NR_SYS_REGS	28
+-
+-/* 32bit mapping */
+-#define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
+-#define c0_CSSELR	(CSSELR_EL1 * 2)/* Cache Size Selection Register */
+-#define c1_SCTLR	(SCTLR_EL1 * 2)	/* System Control Register */
+-#define c1_ACTLR	(ACTLR_EL1 * 2)	/* Auxiliary Control Register */
+-#define c1_CPACR	(CPACR_EL1 * 2)	/* Coprocessor Access Control */
+-#define c2_TTBR0	(TTBR0_EL1 * 2)	/* Translation Table Base Register 0 */
+-#define c2_TTBR0_high	(c2_TTBR0 + 1)	/* TTBR0 top 32 bits */
+-#define c2_TTBR1	(TTBR1_EL1 * 2)	/* Translation Table Base Register 1 */
+-#define c2_TTBR1_high	(c2_TTBR1 + 1)	/* TTBR1 top 32 bits */
+-#define c2_TTBCR	(TCR_EL1 * 2)	/* Translation Table Base Control R. */
+-#define c3_DACR		(DACR32_EL2 * 2)/* Domain Access Control Register */
+-#define c5_DFSR		(ESR_EL1 * 2)	/* Data Fault Status Register */
+-#define c5_IFSR		(IFSR32_EL2 * 2)/* Instruction Fault Status Register */
+-#define c5_ADFSR	(AFSR0_EL1 * 2)	/* Auxiliary Data Fault Status R */
+-#define c5_AIFSR	(AFSR1_EL1 * 2)	/* Auxiliary Instr Fault Status R */
+-#define c6_DFAR		(FAR_EL1 * 2)	/* Data Fault Address Register */
+-#define c6_IFAR		(c6_DFAR + 1)	/* Instruction Fault Address Register */
+-#define c7_PAR		(PAR_EL1 * 2)	/* Physical Address Register */
+-#define c7_PAR_high	(c7_PAR + 1)	/* PAR top 32 bits */
+-#define c10_PRRR	(MAIR_EL1 * 2)	/* Primary Region Remap Register */
+-#define c10_NMRR	(c10_PRRR + 1)	/* Normal Memory Remap Register */
+-#define c12_VBAR	(VBAR_EL1 * 2)	/* Vector Base Address Register */
+-#define c13_CID		(CONTEXTIDR_EL1 * 2)	/* Context ID Register */
+-#define c13_TID_URW	(TPIDR_EL0 * 2)	/* Thread ID, User R/W */
+-#define c13_TID_URO	(TPIDRRO_EL0 * 2)/* Thread ID, User R/O */
+-#define c13_TID_PRIV	(TPIDR_EL1 * 2)	/* Thread ID, Privileged */
+-#define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
+-#define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
+-#define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
+-
+-#define cp14_DBGDSCRext	(MDSCR_EL1 * 2)
+-#define cp14_DBGBCR0	(DBGBCR0_EL1 * 2)
+-#define cp14_DBGBVR0	(DBGBVR0_EL1 * 2)
+-#define cp14_DBGBXVR0	(cp14_DBGBVR0 + 1)
+-#define cp14_DBGWCR0	(DBGWCR0_EL1 * 2)
+-#define cp14_DBGWVR0	(DBGWVR0_EL1 * 2)
+-#define cp14_DBGDCCINT	(MDCCINT_EL1 * 2)
+-
+-#define NR_COPRO_REGS	(NR_SYS_REGS * 2)
+-
+ #define ARM_EXCEPTION_IRQ	  0
+ #define ARM_EXCEPTION_TRAP	  1
+ 
+diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h
+index 25a4021..3066328 100644
+--- a/arch/arm64/include/asm/kvm_emulate.h
++++ b/arch/arm64/include/asm/kvm_emulate.h
+@@ -26,7 +26,6 @@
+ 
+ #include <asm/esr.h>
+ #include <asm/kvm_arm.h>
+-#include <asm/kvm_asm.h>
+ #include <asm/kvm_mmio.h>
+ #include <asm/ptrace.h>
+ #include <asm/cputype.h>
+diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
+index a35ce72..975db14 100644
+--- a/arch/arm64/include/asm/kvm_host.h
++++ b/arch/arm64/include/asm/kvm_host.h
+@@ -25,7 +25,6 @@
+ #include <linux/types.h>
+ #include <linux/kvm_types.h>
+ #include <asm/kvm.h>
+-#include <asm/kvm_asm.h>
+ #include <asm/kvm_mmio.h>
+ 
+ #define __KVM_HAVE_ARCH_INTC_INITIALIZED
+@@ -85,6 +84,86 @@ struct kvm_vcpu_fault_info {
+ 	u64 hpfar_el2;		/* Hyp IPA Fault Address Register */
+ };
+ 
++/*
++ * 0 is reserved as an invalid value.
++ * Order should be kept in sync with the save/restore code.
++ */
++enum vcpu_sysreg {
++	__INVALID_SYSREG__,
++	MPIDR_EL1,	/* MultiProcessor Affinity Register */
++	CSSELR_EL1,	/* Cache Size Selection Register */
++	SCTLR_EL1,	/* System Control Register */
++	ACTLR_EL1,	/* Auxiliary Control Register */
++	CPACR_EL1,	/* Coprocessor Access Control */
++	TTBR0_EL1,	/* Translation Table Base Register 0 */
++	TTBR1_EL1,	/* Translation Table Base Register 1 */
++	TCR_EL1,	/* Translation Control Register */
++	ESR_EL1,	/* Exception Syndrome Register */
++	AFSR0_EL1,	/* Auxilary Fault Status Register 0 */
++	AFSR1_EL1,	/* Auxilary Fault Status Register 1 */
++	FAR_EL1,	/* Fault Address Register */
++	MAIR_EL1,	/* Memory Attribute Indirection Register */
++	VBAR_EL1,	/* Vector Base Address Register */
++	CONTEXTIDR_EL1,	/* Context ID Register */
++	TPIDR_EL0,	/* Thread ID, User R/W */
++	TPIDRRO_EL0,	/* Thread ID, User R/O */
++	TPIDR_EL1,	/* Thread ID, Privileged */
++	AMAIR_EL1,	/* Aux Memory Attribute Indirection Register */
++	CNTKCTL_EL1,	/* Timer Control Register (EL1) */
++	PAR_EL1,	/* Physical Address Register */
++	MDSCR_EL1,	/* Monitor Debug System Control Register */
++	MDCCINT_EL1,	/* Monitor Debug Comms Channel Interrupt Enable Reg */
++
++	/* 32bit specific registers. Keep them at the end of the range */
++	DACR32_EL2,	/* Domain Access Control Register */
++	IFSR32_EL2,	/* Instruction Fault Status Register */
++	FPEXC32_EL2,	/* Floating-Point Exception Control Register */
++	DBGVCR32_EL2,	/* Debug Vector Catch Register */
++
++	NR_SYS_REGS	/* Nothing after this line! */
++};
++
++/* 32bit mapping */
++#define c0_MPIDR	(MPIDR_EL1 * 2)	/* MultiProcessor ID Register */
++#define c0_CSSELR	(CSSELR_EL1 * 2)/* Cache Size Selection Register */
++#define c1_SCTLR	(SCTLR_EL1 * 2)	/* System Control Register */
++#define c1_ACTLR	(ACTLR_EL1 * 2)	/* Auxiliary Control Register */
++#define c1_CPACR	(CPACR_EL1 * 2)	/* Coprocessor Access Control */
++#define c2_TTBR0	(TTBR0_EL1 * 2)	/* Translation Table Base Register 0 */
++#define c2_TTBR0_high	(c2_TTBR0 + 1)	/* TTBR0 top 32 bits */
++#define c2_TTBR1	(TTBR1_EL1 * 2)	/* Translation Table Base Register 1 */
++#define c2_TTBR1_high	(c2_TTBR1 + 1)	/* TTBR1 top 32 bits */
++#define c2_TTBCR	(TCR_EL1 * 2)	/* Translation Table Base Control R. */
++#define c3_DACR		(DACR32_EL2 * 2)/* Domain Access Control Register */
++#define c5_DFSR		(ESR_EL1 * 2)	/* Data Fault Status Register */
++#define c5_IFSR		(IFSR32_EL2 * 2)/* Instruction Fault Status Register */
++#define c5_ADFSR	(AFSR0_EL1 * 2)	/* Auxiliary Data Fault Status R */
++#define c5_AIFSR	(AFSR1_EL1 * 2)	/* Auxiliary Instr Fault Status R */
++#define c6_DFAR		(FAR_EL1 * 2)	/* Data Fault Address Register */
++#define c6_IFAR		(c6_DFAR + 1)	/* Instruction Fault Address Register */
++#define c7_PAR		(PAR_EL1 * 2)	/* Physical Address Register */
++#define c7_PAR_high	(c7_PAR + 1)	/* PAR top 32 bits */
++#define c10_PRRR	(MAIR_EL1 * 2)	/* Primary Region Remap Register */
++#define c10_NMRR	(c10_PRRR + 1)	/* Normal Memory Remap Register */
++#define c12_VBAR	(VBAR_EL1 * 2)	/* Vector Base Address Register */
++#define c13_CID		(CONTEXTIDR_EL1 * 2)	/* Context ID Register */
++#define c13_TID_URW	(TPIDR_EL0 * 2)	/* Thread ID, User R/W */
++#define c13_TID_URO	(TPIDRRO_EL0 * 2)/* Thread ID, User R/O */
++#define c13_TID_PRIV	(TPIDR_EL1 * 2)	/* Thread ID, Privileged */
++#define c10_AMAIR0	(AMAIR_EL1 * 2)	/* Aux Memory Attr Indirection Reg */
++#define c10_AMAIR1	(c10_AMAIR0 + 1)/* Aux Memory Attr Indirection Reg */
++#define c14_CNTKCTL	(CNTKCTL_EL1 * 2) /* Timer Control Register (PL1) */
++
++#define cp14_DBGDSCRext	(MDSCR_EL1 * 2)
++#define cp14_DBGBCR0	(DBGBCR0_EL1 * 2)
++#define cp14_DBGBVR0	(DBGBVR0_EL1 * 2)
++#define cp14_DBGBXVR0	(cp14_DBGBVR0 + 1)
++#define cp14_DBGWCR0	(DBGWCR0_EL1 * 2)
++#define cp14_DBGWVR0	(DBGWVR0_EL1 * 2)
++#define cp14_DBGDCCINT	(MDCCINT_EL1 * 2)
++
++#define NR_COPRO_REGS	(NR_SYS_REGS * 2)
++
+ struct kvm_cpu_context {
+ 	struct kvm_regs	gp_regs;
+ 	union {
+diff --git a/arch/arm64/include/asm/kvm_mmio.h b/arch/arm64/include/asm/kvm_mmio.h
+index 889c908..fe612a9 100644
+--- a/arch/arm64/include/asm/kvm_mmio.h
++++ b/arch/arm64/include/asm/kvm_mmio.h
+@@ -19,7 +19,6 @@
+ #define __ARM64_KVM_MMIO_H__
+ 
+ #include <linux/kvm_host.h>
+-#include <asm/kvm_asm.h>
+ #include <asm/kvm_arm.h>
+ 
+ /*
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 2a948bf..35e6bc6 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -125,6 +125,7 @@ int main(void)
+   DEFINE(CPU_ELR_EL1,		offsetof(struct kvm_regs, elr_el1));
+   DEFINE(CPU_SPSR,		offsetof(struct kvm_regs, spsr));
+   DEFINE(CPU_SYSREGS,		offsetof(struct kvm_cpu_context, sys_regs));
++  DEFINE(VCPU_FPEXC32_EL2,	offsetof(struct kvm_vcpu, arch.ctxt.sys_regs[FPEXC32_EL2]));
+   DEFINE(VCPU_ESR_EL2,		offsetof(struct kvm_vcpu, arch.fault.esr_el2));
+   DEFINE(VCPU_FAR_EL2,		offsetof(struct kvm_vcpu, arch.fault.far_el2));
+   DEFINE(VCPU_HPFAR_EL2,	offsetof(struct kvm_vcpu, arch.fault.hpfar_el2));
+diff --git a/arch/arm64/kvm/guest.c b/arch/arm64/kvm/guest.c
+index 3039f08..e5ee888 100644
+--- a/arch/arm64/kvm/guest.c
++++ b/arch/arm64/kvm/guest.c
+@@ -28,7 +28,6 @@
+ #include <asm/cputype.h>
+ #include <asm/uaccess.h>
+ #include <asm/kvm.h>
+-#include <asm/kvm_asm.h>
+ #include <asm/kvm_emulate.h>
+ #include <asm/kvm_coproc.h>
+ 
+diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
+index 15f0477..198cf10 100644
+--- a/arch/arm64/kvm/handle_exit.c
++++ b/arch/arm64/kvm/handle_exit.c
+@@ -23,6 +23,7 @@
+ #include <linux/kvm_host.h>
+ 
+ #include <asm/esr.h>
++#include <asm/kvm_asm.h>
+ #include <asm/kvm_coproc.h>
+ #include <asm/kvm_emulate.h>
+ #include <asm/kvm_mmu.h>
+diff --git a/arch/arm64/kvm/hyp/debug-sr.c b/arch/arm64/kvm/hyp/debug-sr.c
+index d071f45..567a0d6 100644
+--- a/arch/arm64/kvm/hyp/debug-sr.c
++++ b/arch/arm64/kvm/hyp/debug-sr.c
+@@ -18,6 +18,7 @@
+ #include <linux/compiler.h>
+ #include <linux/kvm_host.h>
+ 
++#include <asm/kvm_asm.h>
+ #include <asm/kvm_mmu.h>
+ 
+ #include "hyp.h"
+diff --git a/arch/arm64/kvm/hyp/entry.S b/arch/arm64/kvm/hyp/entry.S
+index 1050b2b..fd0fbe9 100644
+--- a/arch/arm64/kvm/hyp/entry.S
++++ b/arch/arm64/kvm/hyp/entry.S
+@@ -27,7 +27,6 @@
+ 
+ #define CPU_GP_REG_OFFSET(x)	(CPU_GP_REGS + x)
+ #define CPU_XREG_OFFSET(x)	CPU_GP_REG_OFFSET(CPU_USER_PT_REGS + 8*x)
+-#define CPU_SYSREG_OFFSET(x)	(CPU_SYSREGS + 8*x)
+ 
+ 	.text
+ 	.pushsection	.hyp.text, "ax"
+@@ -150,7 +149,7 @@ ENTRY(__fpsimd_guest_restore)
+ 	// Skip restoring fpexc32 for AArch64 guests
+ 	mrs	x1, hcr_el2
+ 	tbnz	x1, #HCR_RW_SHIFT, 1f
+-	ldr	x4, [x2, #CPU_SYSREG_OFFSET(FPEXC32_EL2)]
++	ldr	x4, [x3, #VCPU_FPEXC32_EL2]
+ 	msr	fpexc32_el2, x4
+ 1:
+ 	ldp	x4, lr, [sp], #16
+diff --git a/arch/arm64/kvm/hyp/sysreg-sr.c b/arch/arm64/kvm/hyp/sysreg-sr.c
+index 3603541..42563098 100644
+--- a/arch/arm64/kvm/hyp/sysreg-sr.c
++++ b/arch/arm64/kvm/hyp/sysreg-sr.c
+@@ -18,6 +18,7 @@
+ #include <linux/compiler.h>
+ #include <linux/kvm_host.h>
+ 
++#include <asm/kvm_asm.h>
+ #include <asm/kvm_mmu.h>
+ 
+ #include "hyp.h"
+diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
+index d2650e8..88adebf 100644
+--- a/arch/arm64/kvm/sys_regs.c
++++ b/arch/arm64/kvm/sys_regs.c
+@@ -29,6 +29,7 @@
+ #include <asm/debug-monitors.h>
+ #include <asm/esr.h>
+ #include <asm/kvm_arm.h>
++#include <asm/kvm_asm.h>
+ #include <asm/kvm_coproc.h>
+ #include <asm/kvm_emulate.h>
+ #include <asm/kvm_host.h>
+diff --git a/virt/kvm/arm/vgic-v3.c b/virt/kvm/arm/vgic-v3.c
+index 3813d23..453eafd 100644
+--- a/virt/kvm/arm/vgic-v3.c
++++ b/virt/kvm/arm/vgic-v3.c
+@@ -28,6 +28,7 @@
+ 
+ #include <asm/kvm_emulate.h>
+ #include <asm/kvm_arm.h>
++#include <asm/kvm_asm.h>
+ #include <asm/kvm_mmu.h>
+ 
+ /* These are for GICv2 emulation only */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0058-arm64-KVM-Cleanup-asm-offset.c.patch b/tools/kdump/0058-arm64-KVM-Cleanup-asm-offset.c.patch
new file mode 100644
index 0000000..9abbf46
--- /dev/null
+++ b/tools/kdump/0058-arm64-KVM-Cleanup-asm-offset.c.patch
@@ -0,0 +1,74 @@
+From b946cf0f61f836e0ce28afe385e9e8940cfbbdcb Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Sun, 25 Oct 2015 20:03:08 +0000
+Subject: [PATCH 058/123] arm64: KVM: Cleanup asm-offset.c
+
+As we've now rewritten most of our code-base in C, most of the
+KVM-specific code in asm-offset.c is useless. Delete-time again!
+
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 23a13465c84c51ec4330863b59e9d50ee671f8b4)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kernel/asm-offsets.c | 39 ---------------------------------------
+ 1 file changed, 39 deletions(-)
+
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 35e6bc6..66ce4fd 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -121,50 +121,11 @@ int main(void)
+   DEFINE(CPU_GP_REGS,		offsetof(struct kvm_cpu_context, gp_regs));
+   DEFINE(CPU_USER_PT_REGS,	offsetof(struct kvm_regs, regs));
+   DEFINE(CPU_FP_REGS,		offsetof(struct kvm_regs, fp_regs));
+-  DEFINE(CPU_SP_EL1,		offsetof(struct kvm_regs, sp_el1));
+-  DEFINE(CPU_ELR_EL1,		offsetof(struct kvm_regs, elr_el1));
+-  DEFINE(CPU_SPSR,		offsetof(struct kvm_regs, spsr));
+-  DEFINE(CPU_SYSREGS,		offsetof(struct kvm_cpu_context, sys_regs));
+   DEFINE(VCPU_FPEXC32_EL2,	offsetof(struct kvm_vcpu, arch.ctxt.sys_regs[FPEXC32_EL2]));
+   DEFINE(VCPU_ESR_EL2,		offsetof(struct kvm_vcpu, arch.fault.esr_el2));
+   DEFINE(VCPU_FAR_EL2,		offsetof(struct kvm_vcpu, arch.fault.far_el2));
+   DEFINE(VCPU_HPFAR_EL2,	offsetof(struct kvm_vcpu, arch.fault.hpfar_el2));
+-  DEFINE(VCPU_DEBUG_FLAGS,	offsetof(struct kvm_vcpu, arch.debug_flags));
+-  DEFINE(VCPU_DEBUG_PTR,	offsetof(struct kvm_vcpu, arch.debug_ptr));
+-  DEFINE(DEBUG_BCR, 		offsetof(struct kvm_guest_debug_arch, dbg_bcr));
+-  DEFINE(DEBUG_BVR, 		offsetof(struct kvm_guest_debug_arch, dbg_bvr));
+-  DEFINE(DEBUG_WCR, 		offsetof(struct kvm_guest_debug_arch, dbg_wcr));
+-  DEFINE(DEBUG_WVR, 		offsetof(struct kvm_guest_debug_arch, dbg_wvr));
+-  DEFINE(VCPU_HCR_EL2,		offsetof(struct kvm_vcpu, arch.hcr_el2));
+-  DEFINE(VCPU_MDCR_EL2,	offsetof(struct kvm_vcpu, arch.mdcr_el2));
+-  DEFINE(VCPU_IRQ_LINES,	offsetof(struct kvm_vcpu, arch.irq_lines));
+   DEFINE(VCPU_HOST_CONTEXT,	offsetof(struct kvm_vcpu, arch.host_cpu_context));
+-  DEFINE(VCPU_HOST_DEBUG_STATE, offsetof(struct kvm_vcpu, arch.host_debug_state));
+-  DEFINE(VCPU_TIMER_CNTV_CTL,	offsetof(struct kvm_vcpu, arch.timer_cpu.cntv_ctl));
+-  DEFINE(VCPU_TIMER_CNTV_CVAL,	offsetof(struct kvm_vcpu, arch.timer_cpu.cntv_cval));
+-  DEFINE(KVM_TIMER_CNTVOFF,	offsetof(struct kvm, arch.timer.cntvoff));
+-  DEFINE(KVM_TIMER_ENABLED,	offsetof(struct kvm, arch.timer.enabled));
+-  DEFINE(VCPU_KVM,		offsetof(struct kvm_vcpu, kvm));
+-  DEFINE(VCPU_VGIC_CPU,		offsetof(struct kvm_vcpu, arch.vgic_cpu));
+-  DEFINE(VGIC_V2_CPU_HCR,	offsetof(struct vgic_cpu, vgic_v2.vgic_hcr));
+-  DEFINE(VGIC_V2_CPU_VMCR,	offsetof(struct vgic_cpu, vgic_v2.vgic_vmcr));
+-  DEFINE(VGIC_V2_CPU_MISR,	offsetof(struct vgic_cpu, vgic_v2.vgic_misr));
+-  DEFINE(VGIC_V2_CPU_EISR,	offsetof(struct vgic_cpu, vgic_v2.vgic_eisr));
+-  DEFINE(VGIC_V2_CPU_ELRSR,	offsetof(struct vgic_cpu, vgic_v2.vgic_elrsr));
+-  DEFINE(VGIC_V2_CPU_APR,	offsetof(struct vgic_cpu, vgic_v2.vgic_apr));
+-  DEFINE(VGIC_V2_CPU_LR,	offsetof(struct vgic_cpu, vgic_v2.vgic_lr));
+-  DEFINE(VGIC_V3_CPU_SRE,	offsetof(struct vgic_cpu, vgic_v3.vgic_sre));
+-  DEFINE(VGIC_V3_CPU_HCR,	offsetof(struct vgic_cpu, vgic_v3.vgic_hcr));
+-  DEFINE(VGIC_V3_CPU_VMCR,	offsetof(struct vgic_cpu, vgic_v3.vgic_vmcr));
+-  DEFINE(VGIC_V3_CPU_MISR,	offsetof(struct vgic_cpu, vgic_v3.vgic_misr));
+-  DEFINE(VGIC_V3_CPU_EISR,	offsetof(struct vgic_cpu, vgic_v3.vgic_eisr));
+-  DEFINE(VGIC_V3_CPU_ELRSR,	offsetof(struct vgic_cpu, vgic_v3.vgic_elrsr));
+-  DEFINE(VGIC_V3_CPU_AP0R,	offsetof(struct vgic_cpu, vgic_v3.vgic_ap0r));
+-  DEFINE(VGIC_V3_CPU_AP1R,	offsetof(struct vgic_cpu, vgic_v3.vgic_ap1r));
+-  DEFINE(VGIC_V3_CPU_LR,	offsetof(struct vgic_cpu, vgic_v3.vgic_lr));
+-  DEFINE(VGIC_CPU_NR_LR,	offsetof(struct vgic_cpu, nr_lr));
+-  DEFINE(KVM_VTTBR,		offsetof(struct kvm, arch.vttbr));
+-  DEFINE(KVM_VGIC_VCTRL,	offsetof(struct kvm, arch.vgic.vctrl_base));
+ #endif
+ #ifdef CONFIG_CPU_PM
+   DEFINE(CPU_SUSPEND_SZ,	sizeof(struct cpu_suspend_ctx));
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0059-arm64-KVM-Remove-weak-attributes.patch b/tools/kdump/0059-arm64-KVM-Remove-weak-attributes.patch
new file mode 100644
index 0000000..7755fc1
--- /dev/null
+++ b/tools/kdump/0059-arm64-KVM-Remove-weak-attributes.patch
@@ -0,0 +1,148 @@
+From 28ab72af633b49cb525dc664cd18a1c3899f3004 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 26 Oct 2015 09:10:07 +0000
+Subject: [PATCH 059/123] arm64: KVM: Remove weak attributes
+
+As we've now switched to the new world switch implementation,
+remove the weak attributes, as nobody is supposed to override
+it anymore.
+
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 3ffa75cd18134a03f86f9d9b8b6e9128e0eda254)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/hyp/debug-sr.c   |  5 ++---
+ arch/arm64/kvm/hyp/hyp-entry.S  |  3 ---
+ arch/arm64/kvm/hyp/switch.c     |  5 ++---
+ arch/arm64/kvm/hyp/tlb.c        | 16 +++++++---------
+ arch/arm64/kvm/hyp/vgic-v3-sr.c |  5 ++---
+ 5 files changed, 13 insertions(+), 21 deletions(-)
+
+diff --git a/arch/arm64/kvm/hyp/debug-sr.c b/arch/arm64/kvm/hyp/debug-sr.c
+index 567a0d6..c9c1e97 100644
+--- a/arch/arm64/kvm/hyp/debug-sr.c
++++ b/arch/arm64/kvm/hyp/debug-sr.c
+@@ -132,10 +132,9 @@ void __hyp_text __debug_cond_restore_host_state(struct kvm_vcpu *vcpu)
+ 		vcpu->arch.debug_flags &= ~KVM_ARM64_DEBUG_DIRTY;
+ }
+ 
+-u32 __hyp_text __debug_read_mdcr_el2(void)
++static u32 __hyp_text __debug_read_mdcr_el2(void)
+ {
+ 	return read_sysreg(mdcr_el2);
+ }
+ 
+-__alias(__debug_read_mdcr_el2)
+-u32 __weak __kvm_get_mdcr_el2(void);
++__alias(__debug_read_mdcr_el2) u32 __kvm_get_mdcr_el2(void);
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+index 8950cdb..1bdeee7 100644
+--- a/arch/arm64/kvm/hyp/hyp-entry.S
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -211,9 +211,7 @@ ENDPROC(\label)
+ 
+ 	.align 11
+ 
+-	.weak	__kvm_hyp_vector
+ ENTRY(__kvm_hyp_vector)
+-ENTRY(__hyp_vector)
+ 	ventry	el2t_sync_invalid		// Synchronous EL2t
+ 	ventry	el2t_irq_invalid		// IRQ EL2t
+ 	ventry	el2t_fiq_invalid		// FIQ EL2t
+@@ -233,5 +231,4 @@ ENTRY(__hyp_vector)
+ 	ventry	el1_irq				// IRQ 32-bit EL1
+ 	ventry	el1_fiq_invalid			// FIQ 32-bit EL1
+ 	ventry	el1_error_invalid		// Error 32-bit EL1
+-ENDPROC(__hyp_vector)
+ ENDPROC(__kvm_hyp_vector)
+diff --git a/arch/arm64/kvm/hyp/switch.c b/arch/arm64/kvm/hyp/switch.c
+index 7457ae4..ca8f5a5 100644
+--- a/arch/arm64/kvm/hyp/switch.c
++++ b/arch/arm64/kvm/hyp/switch.c
+@@ -85,7 +85,7 @@ static void __hyp_text __vgic_restore_state(struct kvm_vcpu *vcpu)
+ 	__vgic_call_restore_state()(vcpu);
+ }
+ 
+-int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
++static int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ {
+ 	struct kvm_cpu_context *host_ctxt;
+ 	struct kvm_cpu_context *guest_ctxt;
+@@ -142,8 +142,7 @@ int __hyp_text __guest_run(struct kvm_vcpu *vcpu)
+ 	return exit_code;
+ }
+ 
+-__alias(__guest_run)
+-int __weak __kvm_vcpu_run(struct kvm_vcpu *vcpu);
++__alias(__guest_run) int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
+ 
+ static const char __hyp_panic_string[] = "HYP panic:\nPS:%08llx PC:%016llx ESR:%08llx\nFAR:%016llx HPFAR:%016llx PAR:%016llx\nVCPU:%p\n";
+ 
+diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c
+index 5f815cf..2a7e0d8 100644
+--- a/arch/arm64/kvm/hyp/tlb.c
++++ b/arch/arm64/kvm/hyp/tlb.c
+@@ -17,7 +17,7 @@
+ 
+ #include "hyp.h"
+ 
+-void __hyp_text __tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
++static void __hyp_text __tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
+ {
+ 	dsb(ishst);
+ 
+@@ -48,10 +48,10 @@ void __hyp_text __tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa)
+ 	write_sysreg(0, vttbr_el2);
+ }
+ 
+-__alias(__tlb_flush_vmid_ipa)
+-void __weak __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
++__alias(__tlb_flush_vmid_ipa) void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm,
++							    phys_addr_t ipa);
+ 
+-void __hyp_text __tlb_flush_vmid(struct kvm *kvm)
++static void __hyp_text __tlb_flush_vmid(struct kvm *kvm)
+ {
+ 	dsb(ishst);
+ 
+@@ -67,10 +67,9 @@ void __hyp_text __tlb_flush_vmid(struct kvm *kvm)
+ 	write_sysreg(0, vttbr_el2);
+ }
+ 
+-__alias(__tlb_flush_vmid)
+-void __weak __kvm_tlb_flush_vmid(struct kvm *kvm);
++__alias(__tlb_flush_vmid) void __kvm_tlb_flush_vmid(struct kvm *kvm);
+ 
+-void __hyp_text __tlb_flush_vm_context(void)
++static void __hyp_text __tlb_flush_vm_context(void)
+ {
+ 	dsb(ishst);
+ 	asm volatile("tlbi alle1is	\n"
+@@ -78,5 +77,4 @@ void __hyp_text __tlb_flush_vm_context(void)
+ 	dsb(ish);
+ }
+ 
+-__alias(__tlb_flush_vm_context)
+-void __weak __kvm_flush_vm_context(void);
++__alias(__tlb_flush_vm_context) void __kvm_flush_vm_context(void);
+diff --git a/arch/arm64/kvm/hyp/vgic-v3-sr.c b/arch/arm64/kvm/hyp/vgic-v3-sr.c
+index a769458..9142e08 100644
+--- a/arch/arm64/kvm/hyp/vgic-v3-sr.c
++++ b/arch/arm64/kvm/hyp/vgic-v3-sr.c
+@@ -220,10 +220,9 @@ void __hyp_text __vgic_v3_restore_state(struct kvm_vcpu *vcpu)
+ 	}
+ }
+ 
+-u64 __hyp_text __vgic_v3_read_ich_vtr_el2(void)
++static u64 __hyp_text __vgic_v3_read_ich_vtr_el2(void)
+ {
+ 	return read_gicreg(ICH_VTR_EL2);
+ }
+ 
+-__alias(__vgic_v3_read_ich_vtr_el2)
+-u64 __weak __vgic_v3_get_ich_vtr_el2(void);
++__alias(__vgic_v3_read_ich_vtr_el2) u64 __vgic_v3_get_ich_vtr_el2(void);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0060-ARM-KVM-Cleanup-exception-injection.patch b/tools/kdump/0060-ARM-KVM-Cleanup-exception-injection.patch
new file mode 100644
index 0000000..024e60b
--- /dev/null
+++ b/tools/kdump/0060-ARM-KVM-Cleanup-exception-injection.patch
@@ -0,0 +1,137 @@
+From 3c912d78dd075b756695de079e935c6eeb9964fd Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 14 Dec 2015 17:58:33 +0000
+Subject: [PATCH 060/123] ARM: KVM: Cleanup exception injection
+
+David Binderman reported that the exception injection code had a
+couple of unused variables lingering around.
+
+Upon examination, it looked like this code could do with an
+anticipated spring cleaning, which amounts to deduplicating
+the CPSR/SPSR update, and making it look a bit more like
+the architecture spec.
+
+The spurious variables are removed in the process.
+
+Reported-by: David Binderman <dcb314@hotmail.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit e078ef81514222ffc10bf1767c15df16ca0b84db)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/kvm/emulate.c | 74 ++++++++++++++++++++++++++------------------------
+ 1 file changed, 38 insertions(+), 36 deletions(-)
+
+diff --git a/arch/arm/kvm/emulate.c b/arch/arm/kvm/emulate.c
+index d6c0052..dc99159 100644
+--- a/arch/arm/kvm/emulate.c
++++ b/arch/arm/kvm/emulate.c
+@@ -275,6 +275,40 @@ static u32 exc_vector_base(struct kvm_vcpu *vcpu)
+ 		return vbar;
+ }
+ 
++/*
++ * Switch to an exception mode, updating both CPSR and SPSR. Follow
++ * the logic described in AArch32.EnterMode() from the ARMv8 ARM.
++ */
++static void kvm_update_psr(struct kvm_vcpu *vcpu, unsigned long mode)
++{
++	unsigned long cpsr = *vcpu_cpsr(vcpu);
++	u32 sctlr = vcpu->arch.cp15[c1_SCTLR];
++
++	*vcpu_cpsr(vcpu) = (cpsr & ~MODE_MASK) | mode;
++
++	switch (mode) {
++	case FIQ_MODE:
++		*vcpu_cpsr(vcpu) |= PSR_F_BIT;
++		/* Fall through */
++	case ABT_MODE:
++	case IRQ_MODE:
++		*vcpu_cpsr(vcpu) |= PSR_A_BIT;
++		/* Fall through */
++	default:
++		*vcpu_cpsr(vcpu) |= PSR_I_BIT;
++	}
++
++	*vcpu_cpsr(vcpu) &= ~(PSR_IT_MASK | PSR_J_BIT | PSR_E_BIT | PSR_T_BIT);
++
++	if (sctlr & SCTLR_TE)
++		*vcpu_cpsr(vcpu) |= PSR_T_BIT;
++	if (sctlr & SCTLR_EE)
++		*vcpu_cpsr(vcpu) |= PSR_E_BIT;
++
++	/* Note: These now point to the mode banked copies */
++	*vcpu_spsr(vcpu) = cpsr;
++}
++
+ /**
+  * kvm_inject_undefined - inject an undefined exception into the guest
+  * @vcpu: The VCPU to receive the undefined exception
+@@ -286,29 +320,13 @@ static u32 exc_vector_base(struct kvm_vcpu *vcpu)
+  */
+ void kvm_inject_undefined(struct kvm_vcpu *vcpu)
+ {
+-	unsigned long new_lr_value;
+-	unsigned long new_spsr_value;
+ 	unsigned long cpsr = *vcpu_cpsr(vcpu);
+-	u32 sctlr = vcpu->arch.cp15[c1_SCTLR];
+ 	bool is_thumb = (cpsr & PSR_T_BIT);
+ 	u32 vect_offset = 4;
+ 	u32 return_offset = (is_thumb) ? 2 : 4;
+ 
+-	new_spsr_value = cpsr;
+-	new_lr_value = *vcpu_pc(vcpu) - return_offset;
+-
+-	*vcpu_cpsr(vcpu) = (cpsr & ~MODE_MASK) | UND_MODE;
+-	*vcpu_cpsr(vcpu) |= PSR_I_BIT;
+-	*vcpu_cpsr(vcpu) &= ~(PSR_IT_MASK | PSR_J_BIT | PSR_E_BIT | PSR_T_BIT);
+-
+-	if (sctlr & SCTLR_TE)
+-		*vcpu_cpsr(vcpu) |= PSR_T_BIT;
+-	if (sctlr & SCTLR_EE)
+-		*vcpu_cpsr(vcpu) |= PSR_E_BIT;
+-
+-	/* Note: These now point to UND banked copies */
+-	*vcpu_spsr(vcpu) = cpsr;
+-	*vcpu_reg(vcpu, 14) = new_lr_value;
++	kvm_update_psr(vcpu, UND_MODE);
++	*vcpu_reg(vcpu, 14) = *vcpu_pc(vcpu) - return_offset;
+ 
+ 	/* Branch to exception vector */
+ 	*vcpu_pc(vcpu) = exc_vector_base(vcpu) + vect_offset;
+@@ -320,30 +338,14 @@ void kvm_inject_undefined(struct kvm_vcpu *vcpu)
+  */
+ static void inject_abt(struct kvm_vcpu *vcpu, bool is_pabt, unsigned long addr)
+ {
+-	unsigned long new_lr_value;
+-	unsigned long new_spsr_value;
+ 	unsigned long cpsr = *vcpu_cpsr(vcpu);
+-	u32 sctlr = vcpu->arch.cp15[c1_SCTLR];
+ 	bool is_thumb = (cpsr & PSR_T_BIT);
+ 	u32 vect_offset;
+ 	u32 return_offset = (is_thumb) ? 4 : 0;
+ 	bool is_lpae;
+ 
+-	new_spsr_value = cpsr;
+-	new_lr_value = *vcpu_pc(vcpu) + return_offset;
+-
+-	*vcpu_cpsr(vcpu) = (cpsr & ~MODE_MASK) | ABT_MODE;
+-	*vcpu_cpsr(vcpu) |= PSR_I_BIT | PSR_A_BIT;
+-	*vcpu_cpsr(vcpu) &= ~(PSR_IT_MASK | PSR_J_BIT | PSR_E_BIT | PSR_T_BIT);
+-
+-	if (sctlr & SCTLR_TE)
+-		*vcpu_cpsr(vcpu) |= PSR_T_BIT;
+-	if (sctlr & SCTLR_EE)
+-		*vcpu_cpsr(vcpu) |= PSR_E_BIT;
+-
+-	/* Note: These now point to ABT banked copies */
+-	*vcpu_spsr(vcpu) = cpsr;
+-	*vcpu_reg(vcpu, 14) = new_lr_value;
++	kvm_update_psr(vcpu, ABT_MODE);
++	*vcpu_reg(vcpu, 14) = *vcpu_pc(vcpu) + return_offset;
+ 
+ 	if (is_pabt)
+ 		vect_offset = 12;
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0061-arm64-KVM-debug-Remove-spurious-inline-attributes.patch b/tools/kdump/0061-arm64-KVM-debug-Remove-spurious-inline-attributes.patch
new file mode 100644
index 0000000..cfdd870
--- /dev/null
+++ b/tools/kdump/0061-arm64-KVM-debug-Remove-spurious-inline-attributes.patch
@@ -0,0 +1,156 @@
+From 2ea4016ca76b8d8e7ff1d294560ae4fa172b1a22 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Wed, 16 Dec 2015 15:41:12 +0000
+Subject: [PATCH 061/123] arm64: KVM: debug: Remove spurious inline attributes
+MIME-Version: 1.0
+Content-Type: text/plain; charset=UTF-8
+Content-Transfer-Encoding: 8bit
+
+The debug trapping code is pretty heavy on the "inline" attribute,
+but most functions are actually referenced in the sysreg tables,
+making the inlining imposible.
+
+Removing the useless inline qualifier seems the right thing to do,
+having verified that the output code is similar.
+
+Cc: Alex Benne <alex.bennee@linaro.org>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 281243cbe075d27ab884858d6e0b15d8ed61bc25)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kvm/sys_regs.c | 58 +++++++++++++++++++++++------------------------
+ 1 file changed, 29 insertions(+), 29 deletions(-)
+
+diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c
+index 88adebf..eec3598 100644
+--- a/arch/arm64/kvm/sys_regs.c
++++ b/arch/arm64/kvm/sys_regs.c
+@@ -220,9 +220,9 @@ static bool trap_debug_regs(struct kvm_vcpu *vcpu,
+  * All writes will set the KVM_ARM64_DEBUG_DIRTY flag to ensure the
+  * hyp.S code switches between host and guest values in future.
+  */
+-static inline void reg_to_dbg(struct kvm_vcpu *vcpu,
+-			      struct sys_reg_params *p,
+-			      u64 *dbg_reg)
++static void reg_to_dbg(struct kvm_vcpu *vcpu,
++		       struct sys_reg_params *p,
++		       u64 *dbg_reg)
+ {
+ 	u64 val = p->regval;
+ 
+@@ -235,18 +235,18 @@ static inline void reg_to_dbg(struct kvm_vcpu *vcpu,
+ 	vcpu->arch.debug_flags |= KVM_ARM64_DEBUG_DIRTY;
+ }
+ 
+-static inline void dbg_to_reg(struct kvm_vcpu *vcpu,
+-			      struct sys_reg_params *p,
+-			      u64 *dbg_reg)
++static void dbg_to_reg(struct kvm_vcpu *vcpu,
++		       struct sys_reg_params *p,
++		       u64 *dbg_reg)
+ {
+ 	p->regval = *dbg_reg;
+ 	if (p->is_32bit)
+ 		p->regval &= 0xffffffffUL;
+ }
+ 
+-static inline bool trap_bvr(struct kvm_vcpu *vcpu,
+-			    struct sys_reg_params *p,
+-			    const struct sys_reg_desc *rd)
++static bool trap_bvr(struct kvm_vcpu *vcpu,
++		     struct sys_reg_params *p,
++		     const struct sys_reg_desc *rd)
+ {
+ 	u64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];
+ 
+@@ -280,15 +280,15 @@ static int get_bvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,
+ 	return 0;
+ }
+ 
+-static inline void reset_bvr(struct kvm_vcpu *vcpu,
+-			     const struct sys_reg_desc *rd)
++static void reset_bvr(struct kvm_vcpu *vcpu,
++		      const struct sys_reg_desc *rd)
+ {
+ 	vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg] = rd->val;
+ }
+ 
+-static inline bool trap_bcr(struct kvm_vcpu *vcpu,
+-			    struct sys_reg_params *p,
+-			    const struct sys_reg_desc *rd)
++static bool trap_bcr(struct kvm_vcpu *vcpu,
++		     struct sys_reg_params *p,
++		     const struct sys_reg_desc *rd)
+ {
+ 	u64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg];
+ 
+@@ -323,15 +323,15 @@ static int get_bcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,
+ 	return 0;
+ }
+ 
+-static inline void reset_bcr(struct kvm_vcpu *vcpu,
+-			     const struct sys_reg_desc *rd)
++static void reset_bcr(struct kvm_vcpu *vcpu,
++		      const struct sys_reg_desc *rd)
+ {
+ 	vcpu->arch.vcpu_debug_state.dbg_bcr[rd->reg] = rd->val;
+ }
+ 
+-static inline bool trap_wvr(struct kvm_vcpu *vcpu,
+-			    struct sys_reg_params *p,
+-			    const struct sys_reg_desc *rd)
++static bool trap_wvr(struct kvm_vcpu *vcpu,
++		     struct sys_reg_params *p,
++		     const struct sys_reg_desc *rd)
+ {
+ 	u64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg];
+ 
+@@ -366,15 +366,15 @@ static int get_wvr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,
+ 	return 0;
+ }
+ 
+-static inline void reset_wvr(struct kvm_vcpu *vcpu,
+-			     const struct sys_reg_desc *rd)
++static void reset_wvr(struct kvm_vcpu *vcpu,
++		      const struct sys_reg_desc *rd)
+ {
+ 	vcpu->arch.vcpu_debug_state.dbg_wvr[rd->reg] = rd->val;
+ }
+ 
+-static inline bool trap_wcr(struct kvm_vcpu *vcpu,
+-			    struct sys_reg_params *p,
+-			    const struct sys_reg_desc *rd)
++static bool trap_wcr(struct kvm_vcpu *vcpu,
++		     struct sys_reg_params *p,
++		     const struct sys_reg_desc *rd)
+ {
+ 	u64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg];
+ 
+@@ -408,8 +408,8 @@ static int get_wcr(struct kvm_vcpu *vcpu, const struct sys_reg_desc *rd,
+ 	return 0;
+ }
+ 
+-static inline void reset_wcr(struct kvm_vcpu *vcpu,
+-			     const struct sys_reg_desc *rd)
++static void reset_wcr(struct kvm_vcpu *vcpu,
++		      const struct sys_reg_desc *rd)
+ {
+ 	vcpu->arch.vcpu_debug_state.dbg_wcr[rd->reg] = rd->val;
+ }
+@@ -723,9 +723,9 @@ static bool trap_debug32(struct kvm_vcpu *vcpu,
+  * system is in.
+  */
+ 
+-static inline bool trap_xvr(struct kvm_vcpu *vcpu,
+-			    struct sys_reg_params *p,
+-			    const struct sys_reg_desc *rd)
++static bool trap_xvr(struct kvm_vcpu *vcpu,
++		     struct sys_reg_params *p,
++		     const struct sys_reg_desc *rd)
+ {
+ 	u64 *dbg_reg = &vcpu->arch.vcpu_debug_state.dbg_bvr[rd->reg];
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0062-arm-arm64-KVM-Remove-unreferenced-S2_PGD_ORDER.patch b/tools/kdump/0062-arm-arm64-KVM-Remove-unreferenced-S2_PGD_ORDER.patch
new file mode 100644
index 0000000..11f459d
--- /dev/null
+++ b/tools/kdump/0062-arm-arm64-KVM-Remove-unreferenced-S2_PGD_ORDER.patch
@@ -0,0 +1,64 @@
+From e8bb6f0d23746c5177cf7b31c5a8c758c24f66d0 Mon Sep 17 00:00:00 2001
+From: Vladimir Murzin <vladimir.murzin@arm.com>
+Date: Mon, 16 Nov 2015 11:28:16 +0000
+Subject: [PATCH 062/123] arm/arm64: KVM: Remove unreferenced S2_PGD_ORDER
+
+Since commit a987370 ("arm64: KVM: Fix stage-2 PGD allocation to have
+per-page refcounting") there is no reference to S2_PGD_ORDER, so kill it
+for the good.
+
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 9d4dc688342a3cbda43a1789cd2c6c888658c60d)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/include/asm/kvm_arm.h   | 1 -
+ arch/arm/kvm/mmu.c               | 6 +++---
+ arch/arm64/include/asm/kvm_mmu.h | 1 -
+ 3 files changed, 3 insertions(+), 5 deletions(-)
+
+diff --git a/arch/arm/include/asm/kvm_arm.h b/arch/arm/include/asm/kvm_arm.h
+index dc641dd..b05bb5a 100644
+--- a/arch/arm/include/asm/kvm_arm.h
++++ b/arch/arm/include/asm/kvm_arm.h
+@@ -135,7 +135,6 @@
+ #define KVM_PHYS_SIZE	(1ULL << KVM_PHYS_SHIFT)
+ #define KVM_PHYS_MASK	(KVM_PHYS_SIZE - 1ULL)
+ #define PTRS_PER_S2_PGD	(1ULL << (KVM_PHYS_SHIFT - 30))
+-#define S2_PGD_ORDER	get_order(PTRS_PER_S2_PGD * sizeof(pgd_t))
+ 
+ /* Virtualization Translation Control Register (VTCR) bits */
+ #define VTCR_SH0	(3 << 12)
+diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c
+index 11b6595..e2b6801 100644
+--- a/arch/arm/kvm/mmu.c
++++ b/arch/arm/kvm/mmu.c
+@@ -656,9 +656,9 @@ static void *kvm_alloc_hwpgd(void)
+  * kvm_alloc_stage2_pgd - allocate level-1 table for stage-2 translation.
+  * @kvm:	The KVM struct pointer for the VM.
+  *
+- * Allocates the 1st level table only of size defined by S2_PGD_ORDER (can
+- * support either full 40-bit input addresses or limited to 32-bit input
+- * addresses). Clears the allocated pages.
++ * Allocates only the stage-2 HW PGD level table(s) (can support either full
++ * 40-bit input addresses or limited to 32-bit input addresses). Clears the
++ * allocated pages.
+  *
+  * Note we don't need locking here as this is only called when the VM is
+  * created, which can only be done once.
+diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
+index 6150567..54cba80 100644
+--- a/arch/arm64/include/asm/kvm_mmu.h
++++ b/arch/arm64/include/asm/kvm_mmu.h
+@@ -158,7 +158,6 @@ static inline bool kvm_s2pmd_readonly(pmd_t *pmd)
+ #define PTRS_PER_S2_PGD_SHIFT	(KVM_PHYS_SHIFT - PGDIR_SHIFT)
+ #endif
+ #define PTRS_PER_S2_PGD		(1 << PTRS_PER_S2_PGD_SHIFT)
+-#define S2_PGD_ORDER		get_order(PTRS_PER_S2_PGD * sizeof(pgd_t))
+ 
+ #define kvm_pgd_index(addr)	(((addr) >> PGDIR_SHIFT) & (PTRS_PER_S2_PGD - 1))
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0063-arm-KVM-Make-kvm_arm.h-friendly-to-assembly-code.patch b/tools/kdump/0063-arm-KVM-Make-kvm_arm.h-friendly-to-assembly-code.patch
new file mode 100644
index 0000000..1a13218
--- /dev/null
+++ b/tools/kdump/0063-arm-KVM-Make-kvm_arm.h-friendly-to-assembly-code.patch
@@ -0,0 +1,102 @@
+From fddebddea86a37919a0c2d03e4100d21e681d4bd Mon Sep 17 00:00:00 2001
+From: Vladimir Murzin <vladimir.murzin@arm.com>
+Date: Mon, 16 Nov 2015 11:28:17 +0000
+Subject: [PATCH 063/123] arm: KVM: Make kvm_arm.h friendly to assembly code
+
+kvm_arm.h is included from both C code and assembly code; however some
+definitions in this header supplied with U/UL/ULL suffixes which might
+confuse assembly once they got evaluated.
+We have _AC macro for such cases, so just wrap problem places with it.
+
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 8420dcd37ef34040c8fc5a27bf66887b3b2faf80)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/include/asm/kvm_arm.h | 33 +++++++++++++++++----------------
+ 1 file changed, 17 insertions(+), 16 deletions(-)
+
+diff --git a/arch/arm/include/asm/kvm_arm.h b/arch/arm/include/asm/kvm_arm.h
+index b05bb5a..01d4d7a 100644
+--- a/arch/arm/include/asm/kvm_arm.h
++++ b/arch/arm/include/asm/kvm_arm.h
+@@ -19,6 +19,7 @@
+ #ifndef __ARM_KVM_ARM_H__
+ #define __ARM_KVM_ARM_H__
+ 
++#include <linux/const.h>
+ #include <linux/types.h>
+ 
+ /* Hyp Configuration Register (HCR) bits */
+@@ -132,9 +133,9 @@
+  * space.
+  */
+ #define KVM_PHYS_SHIFT	(40)
+-#define KVM_PHYS_SIZE	(1ULL << KVM_PHYS_SHIFT)
+-#define KVM_PHYS_MASK	(KVM_PHYS_SIZE - 1ULL)
+-#define PTRS_PER_S2_PGD	(1ULL << (KVM_PHYS_SHIFT - 30))
++#define KVM_PHYS_SIZE	(_AC(1, ULL) << KVM_PHYS_SHIFT)
++#define KVM_PHYS_MASK	(KVM_PHYS_SIZE - _AC(1, ULL))
++#define PTRS_PER_S2_PGD	(_AC(1, ULL) << (KVM_PHYS_SHIFT - 30))
+ 
+ /* Virtualization Translation Control Register (VTCR) bits */
+ #define VTCR_SH0	(3 << 12)
+@@ -161,17 +162,17 @@
+ #define VTTBR_X		(5 - KVM_T0SZ)
+ #endif
+ #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
+-#define VTTBR_BADDR_MASK  (((1LLU << (40 - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+-#define VTTBR_VMID_SHIFT  (48LLU)
+-#define VTTBR_VMID_MASK	  (0xffLLU << VTTBR_VMID_SHIFT)
++#define VTTBR_BADDR_MASK  (((_AC(1, ULL) << (40 - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
++#define VTTBR_VMID_SHIFT  _AC(48, ULL)
++#define VTTBR_VMID_MASK	  (_AC(0xff, ULL) << VTTBR_VMID_SHIFT)
+ 
+ /* Hyp Syndrome Register (HSR) bits */
+ #define HSR_EC_SHIFT	(26)
+-#define HSR_EC		(0x3fU << HSR_EC_SHIFT)
+-#define HSR_IL		(1U << 25)
++#define HSR_EC		(_AC(0x3f, UL) << HSR_EC_SHIFT)
++#define HSR_IL		(_AC(1, UL) << 25)
+ #define HSR_ISS		(HSR_IL - 1)
+ #define HSR_ISV_SHIFT	(24)
+-#define HSR_ISV		(1U << HSR_ISV_SHIFT)
++#define HSR_ISV		(_AC(1, UL) << HSR_ISV_SHIFT)
+ #define HSR_SRT_SHIFT	(16)
+ #define HSR_SRT_MASK	(0xf << HSR_SRT_SHIFT)
+ #define HSR_FSC		(0x3f)
+@@ -179,9 +180,9 @@
+ #define HSR_SSE		(1 << 21)
+ #define HSR_WNR		(1 << 6)
+ #define HSR_CV_SHIFT	(24)
+-#define HSR_CV		(1U << HSR_CV_SHIFT)
++#define HSR_CV		(_AC(1, UL) << HSR_CV_SHIFT)
+ #define HSR_COND_SHIFT	(20)
+-#define HSR_COND	(0xfU << HSR_COND_SHIFT)
++#define HSR_COND	(_AC(0xf, UL) << HSR_COND_SHIFT)
+ 
+ #define FSC_FAULT	(0x04)
+ #define FSC_ACCESS	(0x08)
+@@ -209,13 +210,13 @@
+ #define HSR_EC_DABT	(0x24)
+ #define HSR_EC_DABT_HYP	(0x25)
+ 
+-#define HSR_WFI_IS_WFE		(1U << 0)
++#define HSR_WFI_IS_WFE		(_AC(1, UL) << 0)
+ 
+-#define HSR_HVC_IMM_MASK	((1UL << 16) - 1)
++#define HSR_HVC_IMM_MASK	((_AC(1, UL) << 16) - 1)
+ 
+-#define HSR_DABT_S1PTW		(1U << 7)
+-#define HSR_DABT_CM		(1U << 8)
+-#define HSR_DABT_EA		(1U << 9)
++#define HSR_DABT_S1PTW		(_AC(1, UL) << 7)
++#define HSR_DABT_CM		(_AC(1, UL) << 8)
++#define HSR_DABT_EA		(_AC(1, UL) << 9)
+ 
+ #define kvm_arm_exception_type	\
+ 	{0, "RESET" }, 		\
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0064-arm64-KVM-Add-support-for-16-bit-VMID.patch b/tools/kdump/0064-arm64-KVM-Add-support-for-16-bit-VMID.patch
new file mode 100644
index 0000000..706ebb4
--- /dev/null
+++ b/tools/kdump/0064-arm64-KVM-Add-support-for-16-bit-VMID.patch
@@ -0,0 +1,162 @@
+From e147800059292d7b73b75ae811f0dae694706743 Mon Sep 17 00:00:00 2001
+From: Vladimir Murzin <vladimir.murzin@arm.com>
+Date: Mon, 16 Nov 2015 11:28:18 +0000
+Subject: [PATCH 064/123] arm64: KVM: Add support for 16-bit VMID
+
+The ARMv8.1 architecture extension allows to choose between 8-bit and
+16-bit of VMID, so use this capability for KVM.
+
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Vladimir Murzin <vladimir.murzin@arm.com>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 20475f784d29991b3b843c80c38a36f2ebb35ac4)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	camptiable with LTS 849e28efb04c4c:arm64: KVM: Configure TCR_EL2.PS at runtime
+	in arch/arm64/kvm/hyp-init.S
+---
+ arch/arm/include/asm/kvm_arm.h   |  2 +-
+ arch/arm/include/asm/kvm_mmu.h   |  5 +++++
+ arch/arm/kvm/arm.c               | 10 ++++++++--
+ arch/arm64/include/asm/kvm_arm.h |  3 ++-
+ arch/arm64/include/asm/kvm_mmu.h |  8 ++++++++
+ arch/arm64/kvm/hyp-init.S        |  8 ++++++++
+ 6 files changed, 32 insertions(+), 4 deletions(-)
+
+diff --git a/arch/arm/include/asm/kvm_arm.h b/arch/arm/include/asm/kvm_arm.h
+index 01d4d7a..e22089f 100644
+--- a/arch/arm/include/asm/kvm_arm.h
++++ b/arch/arm/include/asm/kvm_arm.h
+@@ -164,7 +164,7 @@
+ #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
+ #define VTTBR_BADDR_MASK  (((_AC(1, ULL) << (40 - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+ #define VTTBR_VMID_SHIFT  _AC(48, ULL)
+-#define VTTBR_VMID_MASK	  (_AC(0xff, ULL) << VTTBR_VMID_SHIFT)
++#define VTTBR_VMID_MASK(size)	(_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
+ 
+ /* Hyp Syndrome Register (HSR) bits */
+ #define HSR_EC_SHIFT	(26)
+diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h
+index 405aa18..9203c21 100644
+--- a/arch/arm/include/asm/kvm_mmu.h
++++ b/arch/arm/include/asm/kvm_mmu.h
+@@ -279,6 +279,11 @@ static inline void __kvm_extend_hypmap(pgd_t *boot_hyp_pgd,
+ 				       pgd_t *merged_hyp_pgd,
+ 				       unsigned long hyp_idmap_start) { }
+ 
++static inline unsigned int kvm_get_vmid_bits(void)
++{
++	return 8;
++}
++
+ #endif	/* !__ASSEMBLY__ */
+ 
+ #endif /* __ARM_KVM_MMU_H__ */
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index 449e312..0178376 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -59,7 +59,8 @@ static DEFINE_PER_CPU(struct kvm_vcpu *, kvm_arm_running_vcpu);
+ 
+ /* The VMID used in the VTTBR */
+ static atomic64_t kvm_vmid_gen = ATOMIC64_INIT(1);
+-static u8 kvm_next_vmid;
++static u32 kvm_next_vmid;
++static unsigned int kvm_vmid_bits __read_mostly;
+ static DEFINE_SPINLOCK(kvm_vmid_lock);
+ 
+ static void kvm_arm_set_running_vcpu(struct kvm_vcpu *vcpu)
+@@ -432,11 +433,12 @@ static void update_vttbr(struct kvm *kvm)
+ 	kvm->arch.vmid_gen = atomic64_read(&kvm_vmid_gen);
+ 	kvm->arch.vmid = kvm_next_vmid;
+ 	kvm_next_vmid++;
++	kvm_next_vmid &= (1 << kvm_vmid_bits) - 1;
+ 
+ 	/* update vttbr to be used with the new vmid */
+ 	pgd_phys = virt_to_phys(kvm_get_hwpgd(kvm));
+ 	BUG_ON(pgd_phys & ~VTTBR_BADDR_MASK);
+-	vmid = ((u64)(kvm->arch.vmid) << VTTBR_VMID_SHIFT) & VTTBR_VMID_MASK;
++	vmid = ((u64)(kvm->arch.vmid) << VTTBR_VMID_SHIFT) & VTTBR_VMID_MASK(kvm_vmid_bits);
+ 	kvm->arch.vttbr = pgd_phys | vmid;
+ 
+ 	spin_unlock(&kvm_vmid_lock);
+@@ -1132,6 +1134,10 @@ static int init_hyp_mode(void)
+ 
+ 	kvm_perf_init();
+ 
++	/* set size of VMID supported by CPU */
++	kvm_vmid_bits = kvm_get_vmid_bits();
++	kvm_info("%d-bit VMID\n", kvm_vmid_bits);
++
+ 	kvm_info("Hyp mode initialized successfully\n");
+ 
+ 	return 0;
+diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
+index 2d960f85..3776db0 100644
+--- a/arch/arm64/include/asm/kvm_arm.h
++++ b/arch/arm64/include/asm/kvm_arm.h
+@@ -123,6 +123,7 @@
+ #define VTCR_EL2_SL0_LVL1	(1 << 6)
+ #define VTCR_EL2_T0SZ_MASK	0x3f
+ #define VTCR_EL2_T0SZ_40B	24
++#define VTCR_EL2_VS		19
+ 
+ /*
+  * We configure the Stage-2 page tables to always restrict the IPA space to be
+@@ -167,7 +168,7 @@
+ #define VTTBR_BADDR_SHIFT (VTTBR_X - 1)
+ #define VTTBR_BADDR_MASK  (((UL(1) << (PHYS_MASK_SHIFT - VTTBR_X)) - 1) << VTTBR_BADDR_SHIFT)
+ #define VTTBR_VMID_SHIFT  (UL(48))
+-#define VTTBR_VMID_MASK	  (UL(0xFF) << VTTBR_VMID_SHIFT)
++#define VTTBR_VMID_MASK(size) (_AT(u64, (1 << size) - 1) << VTTBR_VMID_SHIFT)
+ 
+ /* Hyp System Trap Register */
+ #define HSTR_EL2_T(x)	(1 << x)
+diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
+index 54cba80..0bf8b43 100644
+--- a/arch/arm64/include/asm/kvm_mmu.h
++++ b/arch/arm64/include/asm/kvm_mmu.h
+@@ -20,6 +20,7 @@
+ 
+ #include <asm/page.h>
+ #include <asm/memory.h>
++#include <asm/cpufeature.h>
+ 
+ /*
+  * As we only have the TTBR0_EL2 register, we cannot express
+@@ -301,5 +302,12 @@ static inline void __kvm_extend_hypmap(pgd_t *boot_hyp_pgd,
+ 	merged_hyp_pgd[idmap_idx] = __pgd(__pa(boot_hyp_pgd) | PMD_TYPE_TABLE);
+ }
+ 
++static inline unsigned int kvm_get_vmid_bits(void)
++{
++	int reg = read_system_reg(SYS_ID_AA64MMFR1_EL1);
++
++	return (cpuid_feature_extract_field(reg, ID_AA64MMFR1_VMIDBITS_SHIFT) == 2) ? 16 : 8;
++}
++
+ #endif /* __ASSEMBLY__ */
+ #endif /* __ARM64_KVM_MMU_H__ */
+diff --git a/arch/arm64/kvm/hyp-init.S b/arch/arm64/kvm/hyp-init.S
+index 84c338f..d073b5a 100644
+--- a/arch/arm64/kvm/hyp-init.S
++++ b/arch/arm64/kvm/hyp-init.S
+@@ -96,6 +96,14 @@ __do_hyp_init:
+ 
+ 	ldr	x4, =VTCR_EL2_FLAGS
+ 	bfi	x4, x5, #16, #3
++	/*
++	 * Read the VMIDBits bits from ID_AA64MMFR1_EL1 and set the VS bit in
++	 * VTCR_EL2.
++	 */
++	mrs	x5, ID_AA64MMFR1_EL1
++	ubfx	x5, x5, #5, #1
++	lsl	x5, x5, #VTCR_EL2_VS
++	orr	x4, x4, x5
+ 
+ 	msr	vtcr_el2, x4
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0065-arm-arm64-KVM-Detect-vGIC-presence-at-runtime.patch b/tools/kdump/0065-arm-arm64-KVM-Detect-vGIC-presence-at-runtime.patch
new file mode 100644
index 0000000..5f0889a
--- /dev/null
+++ b/tools/kdump/0065-arm-arm64-KVM-Detect-vGIC-presence-at-runtime.patch
@@ -0,0 +1,101 @@
+From d86e04923e7b56a7c1a6c512640526ab533ff92e Mon Sep 17 00:00:00 2001
+From: Pavel Fedin <p.fedin@samsung.com>
+Date: Fri, 18 Dec 2015 14:38:43 +0300
+Subject: [PATCH 065/123] arm/arm64: KVM: Detect vGIC presence at runtime
+
+Before commit 662d9715840aef44dcb573b0f9fab9e8319c868a
+("arm/arm64: KVM: Kill CONFIG_KVM_ARM_{VGIC,TIMER}") is was possible to
+compile the kernel without vGIC and vTimer support. Commit message says
+about possibility to detect vGIC support in runtime, but this has never
+been implemented.
+
+This patch introduces runtime check, restoring the lost functionality.
+It again allows to use KVM on hardware without vGIC. Interrupt
+controller has to be emulated in userspace in this case.
+
+-ENODEV return code from probe function means there's no GIC at all.
+-ENXIO happens when, for example, there is GIC node in the device tree,
+but it does not specify vGIC resources. Any other error code is still
+treated as full stop because it might mean some really serious problems.
+
+Signed-off-by: Pavel Fedin <p.fedin@samsung.com>
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit c7da6fa43cb1c5e649da0f478a491feb9208cae7)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/kvm/arm.c | 22 ++++++++++++++++++++--
+ 1 file changed, 20 insertions(+), 2 deletions(-)
+
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index 0178376..b936475 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -63,6 +63,8 @@ static u32 kvm_next_vmid;
+ static unsigned int kvm_vmid_bits __read_mostly;
+ static DEFINE_SPINLOCK(kvm_vmid_lock);
+ 
++static bool vgic_present;
++
+ static void kvm_arm_set_running_vcpu(struct kvm_vcpu *vcpu)
+ {
+ 	BUG_ON(preemptible());
+@@ -134,7 +136,8 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
+ 	kvm->arch.vmid_gen = 0;
+ 
+ 	/* The maximum number of VCPUs is limited by the host's GIC model */
+-	kvm->arch.max_vcpus = kvm_vgic_get_max_vcpus();
++	kvm->arch.max_vcpus = vgic_present ?
++				kvm_vgic_get_max_vcpus() : KVM_MAX_VCPUS;
+ 
+ 	return ret;
+ out_free_stage2_pgd:
+@@ -172,6 +175,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
+ 	int r;
+ 	switch (ext) {
+ 	case KVM_CAP_IRQCHIP:
++		r = vgic_present;
++		break;
+ 	case KVM_CAP_IOEVENTFD:
+ 	case KVM_CAP_DEVICE_CTRL:
+ 	case KVM_CAP_USER_MEMORY:
+@@ -914,6 +919,8 @@ static int kvm_vm_ioctl_set_device_addr(struct kvm *kvm,
+ 
+ 	switch (dev_id) {
+ 	case KVM_ARM_DEVICE_VGIC_V2:
++		if (!vgic_present)
++			return -ENXIO;
+ 		return kvm_vgic_addr(kvm, type, &dev_addr->addr, true);
+ 	default:
+ 		return -ENODEV;
+@@ -928,6 +935,8 @@ long kvm_arch_vm_ioctl(struct file *filp,
+ 
+ 	switch (ioctl) {
+ 	case KVM_CREATE_IRQCHIP: {
++		if (!vgic_present)
++			return -ENXIO;
+ 		return kvm_vgic_create(kvm, KVM_DEV_TYPE_ARM_VGIC_V2);
+ 	}
+ 	case KVM_ARM_SET_DEVICE_ADDR: {
+@@ -1118,8 +1127,17 @@ static int init_hyp_mode(void)
+ 	 * Init HYP view of VGIC
+ 	 */
+ 	err = kvm_vgic_hyp_init();
+-	if (err)
++	switch (err) {
++	case 0:
++		vgic_present = true;
++		break;
++	case -ENODEV:
++	case -ENXIO:
++		vgic_present = false;
++		break;
++	default:
+ 		goto out_free_context;
++	}
+ 
+ 	/*
+ 	 * Init HYP architected timer support
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0066-arm-arm64-KVM-Add-hook-for-C-based-stage2-init.patch b/tools/kdump/0066-arm-arm64-KVM-Add-hook-for-C-based-stage2-init.patch
new file mode 100644
index 0000000..848d62a
--- /dev/null
+++ b/tools/kdump/0066-arm-arm64-KVM-Add-hook-for-C-based-stage2-init.patch
@@ -0,0 +1,63 @@
+From ec0b2813a6a693ed6b067119e089fcb102f716bb Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Mon, 1 Feb 2016 17:54:35 +0000
+Subject: [PATCH 066/123] arm/arm64: KVM: Add hook for C-based stage2 init
+
+As we're about to move the stage2 init to C code, introduce some
+C hooks that will later be populated with arch-specific implementations.
+
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 35a2491a624af1fa7ab6990639f5246cd5f12592)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/include/asm/kvm_host.h   | 4 ++++
+ arch/arm/kvm/arm.c                | 1 +
+ arch/arm64/include/asm/kvm_host.h | 4 ++++
+ 3 files changed, 9 insertions(+)
+
+diff --git a/arch/arm/include/asm/kvm_host.h b/arch/arm/include/asm/kvm_host.h
+index 6692982..945bfa5 100644
+--- a/arch/arm/include/asm/kvm_host.h
++++ b/arch/arm/include/asm/kvm_host.h
+@@ -214,6 +214,10 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
+ 	kvm_call_hyp((void*)hyp_stack_ptr, vector_ptr, pgd_ptr);
+ }
+ 
++static inline void __cpu_init_stage2(void)
++{
++}
++
+ static inline int kvm_arch_dev_ioctl_check_extension(long ext)
+ {
+ 	return 0;
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index b936475..30c9f7b 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -982,6 +982,7 @@ static void cpu_init_hyp_mode(void *dummy)
+ 	vector_ptr = (unsigned long)__kvm_hyp_vector;
+ 
+ 	__cpu_init_hyp_mode(boot_pgd_ptr, pgd_ptr, hyp_stack_ptr, vector_ptr);
++	__cpu_init_stage2();
+ 
+ 	kvm_arm_init_debug();
+ }
+diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
+index 975db14..1b37b5d 100644
+--- a/arch/arm64/include/asm/kvm_host.h
++++ b/arch/arm64/include/asm/kvm_host.h
+@@ -326,6 +326,10 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
+ 		     hyp_stack_ptr, vector_ptr);
+ }
+ 
++static inline void __cpu_init_stage2(void)
++{
++}
++
+ static inline void kvm_arch_hardware_disable(void) {}
+ static inline void kvm_arch_hardware_unsetup(void) {}
+ static inline void kvm_arch_sync_events(struct kvm *kvm) {}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0067-arm64-Fold-proc-macros.S-into-assembler.h.patch b/tools/kdump/0067-arm64-Fold-proc-macros.S-into-assembler.h.patch
new file mode 100644
index 0000000..873a54b
--- /dev/null
+++ b/tools/kdump/0067-arm64-Fold-proc-macros.S-into-assembler.h.patch
@@ -0,0 +1,272 @@
+From 08691b6621fbc1c8306997ae8cc027a68d2f87ff Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Wed, 27 Apr 2016 17:47:00 +0100
+Subject: [PATCH 067/123] arm64: Fold proc-macros.S into assembler.h
+
+To allow the assembler macros defined in arch/arm64/mm/proc-macros.S to
+be used outside the mm code move the contents of proc-macros.S to
+asm/assembler.h.  Also, delete proc-macros.S, and fix up all references
+to proc-macros.S.
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+Acked-by: Pavel Machek <pavel@ucw.cz>
+[rebased, included dcache_by_line_op]
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+
+(cherry picked from commit 7b7293ae3dbd0a1965bf310b77fed5f9bb37bb93)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	remove obsolete arch/arm64/mm/proc-macros.S
+---
+ arch/arm64/include/asm/assembler.h | 82 +++++++++++++++++++++++++++++++-
+ arch/arm64/mm/cache.S              |  2 -
+ arch/arm64/mm/proc-macros.S        | 97 --------------------------------------
+ arch/arm64/mm/proc.S               |  3 --
+ 4 files changed, 81 insertions(+), 103 deletions(-)
+ delete mode 100644 arch/arm64/mm/proc-macros.S
+
+diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
+index 12eff92..55fab0a 100644
+--- a/arch/arm64/include/asm/assembler.h
++++ b/arch/arm64/include/asm/assembler.h
+@@ -1,5 +1,5 @@
+ /*
+- * Based on arch/arm/include/asm/assembler.h
++ * Based on arch/arm/include/asm/assembler.h, arch/arm/mm/proc-macros.S
+  *
+  * Copyright (C) 1996-2000 Russell King
+  * Copyright (C) 2012 ARM Ltd.
+@@ -23,6 +23,8 @@
+ #ifndef __ASM_ASSEMBLER_H
+ #define __ASM_ASSEMBLER_H
+ 
++#include <asm/asm-offsets.h>
++#include <asm/pgtable-hwdef.h>
+ #include <asm/ptrace.h>
+ #include <asm/thread_info.h>
+ 
+@@ -194,6 +196,84 @@ lr	.req	x30		// link register
+ 	.endm
+ 
+ /*
++ * vma_vm_mm - get mm pointer from vma pointer (vma->vm_mm)
++ */
++	.macro	vma_vm_mm, rd, rn
++	ldr	\rd, [\rn, #VMA_VM_MM]
++	.endm
++
++/*
++ * mmid - get context id from mm pointer (mm->context.id)
++ */
++	.macro	mmid, rd, rn
++	ldr	\rd, [\rn, #MM_CONTEXT_ID]
++	.endm
++
++/*
++ * dcache_line_size - get the minimum D-cache line size from the CTR register.
++ */
++	.macro	dcache_line_size, reg, tmp
++	mrs	\tmp, ctr_el0			// read CTR
++	ubfm	\tmp, \tmp, #16, #19		// cache line size encoding
++	mov	\reg, #4			// bytes per word
++	lsl	\reg, \reg, \tmp		// actual cache line size
++	.endm
++
++/*
++ * icache_line_size - get the minimum I-cache line size from the CTR register.
++ */
++	.macro	icache_line_size, reg, tmp
++	mrs	\tmp, ctr_el0			// read CTR
++	and	\tmp, \tmp, #0xf		// cache line size encoding
++	mov	\reg, #4			// bytes per word
++	lsl	\reg, \reg, \tmp		// actual cache line size
++	.endm
++
++/*
++ * tcr_set_idmap_t0sz - update TCR.T0SZ so that we can load the ID map
++ */
++	.macro	tcr_set_idmap_t0sz, valreg, tmpreg
++#ifndef CONFIG_ARM64_VA_BITS_48
++	ldr_l	\tmpreg, idmap_t0sz
++	bfi	\valreg, \tmpreg, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
++#endif
++	.endm
++
++/*
++ * Macro to perform a data cache maintenance for the interval
++ * [kaddr, kaddr + size)
++ *
++ * 	op:		operation passed to dc instruction
++ * 	domain:		domain used in dsb instruciton
++ * 	kaddr:		starting virtual address of the region
++ * 	size:		size of the region
++ * 	Corrupts:	kaddr, size, tmp1, tmp2
++ */
++	.macro dcache_by_line_op op, domain, kaddr, size, tmp1, tmp2
++	dcache_line_size \tmp1, \tmp2
++	add	\size, \kaddr, \size
++	sub	\tmp2, \tmp1, #1
++	bic	\kaddr, \kaddr, \tmp2
++9998:	dc	\op, \kaddr
++	add	\kaddr, \kaddr, \tmp1
++	cmp	\kaddr, \size
++	b.lo	9998b
++	dsb	\domain
++	.endm
++
++/*
++ * reset_pmuserenr_el0 - reset PMUSERENR_EL0 if PMUv3 present
++ */
++	.macro	reset_pmuserenr_el0, tmpreg
++	mrs	\tmpreg, id_aa64dfr0_el1	// Check ID_AA64DFR0_EL1 PMUVer
++	sbfx	\tmpreg, \tmpreg, #8, #4
++	cmp	\tmpreg, #1			// Skip if no PMU present
++	b.lt	9000f
++	msr	pmuserenr_el0, xzr		// Disable PMU access from EL0
++9000:
++	.endm
++
++/*
+  * Annotate a function as position independent, i.e., safe to be called before
+  * the kernel virtual mapping is activated.
+  */
+diff --git a/arch/arm64/mm/cache.S b/arch/arm64/mm/cache.S
+index 6df0706..50ff9ba 100644
+--- a/arch/arm64/mm/cache.S
++++ b/arch/arm64/mm/cache.S
+@@ -24,8 +24,6 @@
+ #include <asm/cpufeature.h>
+ #include <asm/alternative.h>
+ 
+-#include "proc-macros.S"
+-
+ /*
+  *	flush_icache_range(start,end)
+  *
+diff --git a/arch/arm64/mm/proc-macros.S b/arch/arm64/mm/proc-macros.S
+deleted file mode 100644
+index 2bce364..0000000
+--- a/arch/arm64/mm/proc-macros.S
++++ /dev/null
+@@ -1,97 +0,0 @@
+-/*
+- * Based on arch/arm/mm/proc-macros.S
+- *
+- * Copyright (C) 2012 ARM Ltd.
+- *
+- * This program is free software; you can redistribute it and/or modify
+- * it under the terms of the GNU General Public License version 2 as
+- * published by the Free Software Foundation.
+- *
+- * This program is distributed in the hope that it will be useful,
+- * but WITHOUT ANY WARRANTY; without even the implied warranty of
+- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+- * GNU General Public License for more details.
+- *
+- * You should have received a copy of the GNU General Public License
+- * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+- */
+-
+-#include <asm/asm-offsets.h>
+-#include <asm/thread_info.h>
+-
+-/*
+- * vma_vm_mm - get mm pointer from vma pointer (vma->vm_mm)
+- */
+-	.macro	vma_vm_mm, rd, rn
+-	ldr	\rd, [\rn, #VMA_VM_MM]
+-	.endm
+-
+-/*
+- * mmid - get context id from mm pointer (mm->context.id)
+- */
+-	.macro	mmid, rd, rn
+-	ldr	\rd, [\rn, #MM_CONTEXT_ID]
+-	.endm
+-
+-/*
+- * dcache_line_size - get the minimum D-cache line size from the CTR register.
+- */
+-	.macro	dcache_line_size, reg, tmp
+-	mrs	\tmp, ctr_el0			// read CTR
+-	ubfm	\tmp, \tmp, #16, #19		// cache line size encoding
+-	mov	\reg, #4			// bytes per word
+-	lsl	\reg, \reg, \tmp		// actual cache line size
+-	.endm
+-
+-/*
+- * icache_line_size - get the minimum I-cache line size from the CTR register.
+- */
+-	.macro	icache_line_size, reg, tmp
+-	mrs	\tmp, ctr_el0			// read CTR
+-	and	\tmp, \tmp, #0xf		// cache line size encoding
+-	mov	\reg, #4			// bytes per word
+-	lsl	\reg, \reg, \tmp		// actual cache line size
+-	.endm
+-
+-/*
+- * tcr_set_idmap_t0sz - update TCR.T0SZ so that we can load the ID map
+- */
+-	.macro	tcr_set_idmap_t0sz, valreg, tmpreg
+-#ifndef CONFIG_ARM64_VA_BITS_48
+-	ldr_l	\tmpreg, idmap_t0sz
+-	bfi	\valreg, \tmpreg, #TCR_T0SZ_OFFSET, #TCR_TxSZ_WIDTH
+-#endif
+-	.endm
+-
+-/*
+- * reset_pmuserenr_el0 - reset PMUSERENR_EL0 if PMUv3 present
+- */
+-	.macro	reset_pmuserenr_el0, tmpreg
+-	mrs	\tmpreg, id_aa64dfr0_el1	// Check ID_AA64DFR0_EL1 PMUVer
+-	sbfx	\tmpreg, \tmpreg, #8, #4
+-	cmp	\tmpreg, #1			// Skip if no PMU present
+-	b.lt	9000f
+-	msr	pmuserenr_el0, xzr		// Disable PMU access from EL0
+-9000:
+-	.endm
+-/*
+- * Macro to perform a data cache maintenance for the interval
+- * [kaddr, kaddr + size)
+- *
+- * 	op:		operation passed to dc instruction
+- * 	domain:		domain used in dsb instruciton
+- * 	kaddr:		starting virtual address of the region
+- * 	size:		size of the region
+- * 	Corrupts: 	kaddr, size, tmp1, tmp2
+- */
+-	.macro dcache_by_line_op op, domain, kaddr, size, tmp1, tmp2
+-	dcache_line_size \tmp1, \tmp2
+-	add	\size, \kaddr, \size
+-	sub	\tmp2, \tmp1, #1
+-	bic	\kaddr, \kaddr, \tmp2
+-9998:	dc	\op, \kaddr
+-	add	\kaddr, \kaddr, \tmp1
+-	cmp	\kaddr, \size
+-	b.lo	9998b
+-	dsb	\domain
+-	.endm
+diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
+index a92738e..b6a141f 100644
+--- a/arch/arm64/mm/proc.S
++++ b/arch/arm64/mm/proc.S
+@@ -23,13 +23,10 @@
+ #include <asm/assembler.h>
+ #include <asm/asm-offsets.h>
+ #include <asm/hwcap.h>
+-#include <asm/pgtable-hwdef.h>
+ #include <asm/pgtable.h>
+ #include <asm/cpufeature.h>
+ #include <asm/alternative.h>
+ 
+-#include "proc-macros.S"
+-
+ #ifdef CONFIG_ARM64_64K_PAGES
+ #define TCR_TG_FLAGS	TCR_TG0_64K | TCR_TG1_64K
+ #elif defined(CONFIG_ARM64_16K_PAGES)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0068-arm64-Cleanup-SCTLR-flags.patch b/tools/kdump/0068-arm64-Cleanup-SCTLR-flags.patch
new file mode 100644
index 0000000..bb62394
--- /dev/null
+++ b/tools/kdump/0068-arm64-Cleanup-SCTLR-flags.patch
@@ -0,0 +1,104 @@
+From d097b915ae4a08eb786eab434f044df2f5719c3f Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Wed, 27 Apr 2016 17:47:01 +0100
+Subject: [PATCH 068/123] arm64: Cleanup SCTLR flags
+
+We currently have macros defining flags for the arm64 sctlr registers in
+both kvm_arm.h and sysreg.h.  To clean things up and simplify move the
+definitions of the SCTLR_EL2 flags from kvm_arm.h to sysreg.h, rename any
+SCTLR_EL1 or SCTLR_EL2 flags that are common to both registers to be
+SCTLR_ELx, with 'x' indicating a common flag, and fixup all files to
+include the proper header or to use the new macro names.
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+[Restored pgtable-hwdef.h include]
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+
+(cherry picked from commit e7227d0e528f9a96d4a866f43e20dd9b33f0e782)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/kvm_arm.h | 11 -----------
+ arch/arm64/include/asm/sysreg.h  | 19 +++++++++++++++----
+ arch/arm64/kvm/hyp-init.S        |  5 +++--
+ 3 files changed, 18 insertions(+), 17 deletions(-)
+
+diff --git a/arch/arm64/include/asm/kvm_arm.h b/arch/arm64/include/asm/kvm_arm.h
+index 3776db0..8b709f5 100644
+--- a/arch/arm64/include/asm/kvm_arm.h
++++ b/arch/arm64/include/asm/kvm_arm.h
+@@ -83,17 +83,6 @@
+ #define HCR_INT_OVERRIDE   (HCR_FMO | HCR_IMO)
+ 
+ 
+-/* Hyp System Control Register (SCTLR_EL2) bits */
+-#define SCTLR_EL2_EE	(1 << 25)
+-#define SCTLR_EL2_WXN	(1 << 19)
+-#define SCTLR_EL2_I	(1 << 12)
+-#define SCTLR_EL2_SA	(1 << 3)
+-#define SCTLR_EL2_C	(1 << 2)
+-#define SCTLR_EL2_A	(1 << 1)
+-#define SCTLR_EL2_M	1
+-#define SCTLR_EL2_FLAGS	(SCTLR_EL2_M | SCTLR_EL2_A | SCTLR_EL2_C |	\
+-			 SCTLR_EL2_SA | SCTLR_EL2_I)
+-
+ /* TCR_EL2 Registers bits */
+ #define TCR_EL2_RES1	((1 << 31) | (1 << 23))
+ #define TCR_EL2_TBI	(1 << 20)
+diff --git a/arch/arm64/include/asm/sysreg.h b/arch/arm64/include/asm/sysreg.h
+index 4aeebec..99dac2f 100644
+--- a/arch/arm64/include/asm/sysreg.h
++++ b/arch/arm64/include/asm/sysreg.h
+@@ -82,10 +82,21 @@
+ #define SET_PSTATE_PAN(x) __inst_arm(0xd5000000 | REG_PSTATE_PAN_IMM |\
+ 				     (!!x)<<8 | 0x1f)
+ 
+-/* SCTLR_EL1 */
+-#define SCTLR_EL1_CP15BEN	(0x1 << 5)
+-#define SCTLR_EL1_SED		(0x1 << 8)
+-#define SCTLR_EL1_SPAN		(0x1 << 23)
++/* Common SCTLR_ELx flags. */
++#define SCTLR_ELx_EE    (1 << 25)
++#define SCTLR_ELx_I	(1 << 12)
++#define SCTLR_ELx_SA	(1 << 3)
++#define SCTLR_ELx_C	(1 << 2)
++#define SCTLR_ELx_A	(1 << 1)
++#define SCTLR_ELx_M	1
++
++#define SCTLR_ELx_FLAGS	(SCTLR_ELx_M | SCTLR_ELx_A | SCTLR_ELx_C | \
++			 SCTLR_ELx_SA | SCTLR_ELx_I)
++
++/* SCTLR_EL1 specific flags. */
++#define SCTLR_EL1_SPAN		(1 << 23)
++#define SCTLR_EL1_SED		(1 << 8)
++#define SCTLR_EL1_CP15BEN	(1 << 5)
+ 
+ 
+ /* id_aa64isar0 */
+diff --git a/arch/arm64/kvm/hyp-init.S b/arch/arm64/kvm/hyp-init.S
+index d073b5a..034d152c 100644
+--- a/arch/arm64/kvm/hyp-init.S
++++ b/arch/arm64/kvm/hyp-init.S
+@@ -21,6 +21,7 @@
+ #include <asm/kvm_arm.h>
+ #include <asm/kvm_mmu.h>
+ #include <asm/pgtable-hwdef.h>
++#include <asm/sysreg.h>
+ 
+ 	.text
+ 	.pushsection	.hyp.idmap.text, "ax"
+@@ -116,8 +117,8 @@ __do_hyp_init:
+ 	dsb	sy
+ 
+ 	mrs	x4, sctlr_el2
+-	and	x4, x4, #SCTLR_EL2_EE	// preserve endianness of EL2
+-	ldr	x5, =SCTLR_EL2_FLAGS
++	and	x4, x4, #SCTLR_ELx_EE	// preserve endianness of EL2
++	ldr	x5, =SCTLR_ELx_FLAGS
+ 	orr	x4, x4, x5
+ 	msr	sctlr_el2, x4
+ 	isb
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0069-arm64-kernel-Rework-finisher-callback-out-of-__cpu_s.patch b/tools/kdump/0069-arm64-kernel-Rework-finisher-callback-out-of-__cpu_s.patch
new file mode 100644
index 0000000..83e51b8
--- /dev/null
+++ b/tools/kdump/0069-arm64-kernel-Rework-finisher-callback-out-of-__cpu_s.patch
@@ -0,0 +1,348 @@
+From 0d2eb79e6358e0cd8ac97620d48ebaad8d20a8a9 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:06 +0100
+Subject: [PATCH 069/123] arm64: kernel: Rework finisher callback out of
+ __cpu_suspend_enter()
+
+Hibernate could make use of the cpu_suspend() code to save/restore cpu
+state, however it needs to be able to return '0' from the 'finisher'.
+
+Rework cpu_suspend() so that the finisher is called from C code,
+independently from the save/restore of cpu state. Space to save the context
+in is allocated in the caller's stack frame, and passed into
+__cpu_suspend_enter().
+
+Hibernate's use of this API will look like a copy of the cpu_suspend()
+function.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit adc9b2dfd00924e9e9b98613f36a6cb8c51f0dc6)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kernel/suspend.c
+---
+ arch/arm64/include/asm/suspend.h | 20 +++++++++
+ arch/arm64/kernel/asm-offsets.c  |  2 +
+ arch/arm64/kernel/sleep.S        | 93 ++++++++++++++--------------------------
+ arch/arm64/kernel/suspend.c      | 72 ++++++++++++++++++-------------
+ 4 files changed, 97 insertions(+), 90 deletions(-)
+
+diff --git a/arch/arm64/include/asm/suspend.h b/arch/arm64/include/asm/suspend.h
+index 59a5b0f1..365d8cd 100644
+--- a/arch/arm64/include/asm/suspend.h
++++ b/arch/arm64/include/asm/suspend.h
+@@ -2,6 +2,7 @@
+ #define __ASM_SUSPEND_H
+ 
+ #define NR_CTX_REGS 11
++#define NR_CALLEE_SAVED_REGS 12
+ 
+ /*
+  * struct cpu_suspend_ctx must be 16-byte aligned since it is allocated on
+@@ -21,6 +22,25 @@ struct sleep_save_sp {
+ 	phys_addr_t save_ptr_stash_phys;
+ };
+ 
++/*
++ * Memory to save the cpu state is allocated on the stack by
++ * __cpu_suspend_enter()'s caller, and populated by __cpu_suspend_enter().
++ * This data must survive until cpu_resume() is called.
++ *
++ * This struct desribes the size and the layout of the saved cpu state.
++ * The layout of the callee_saved_regs is defined by the implementation
++ * of __cpu_suspend_enter(), and cpu_resume(). This struct must be passed
++ * in by the caller as __cpu_suspend_enter()'s stack-frame is gone once it
++ * returns, and the data would be subsequently corrupted by the call to the
++ * finisher.
++ */
++struct sleep_stack_data {
++	struct cpu_suspend_ctx	system_regs;
++	unsigned long		callee_saved_regs[NR_CALLEE_SAVED_REGS];
++};
++
+ extern int cpu_suspend(unsigned long arg, int (*fn)(unsigned long));
+ extern void cpu_resume(void);
++int __cpu_suspend_enter(struct sleep_stack_data *state);
++void __cpu_suspend_exit(void);
+ #endif
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 66ce4fd..2abd57f 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -135,6 +135,8 @@ int main(void)
+   DEFINE(SLEEP_SAVE_SP_SZ,	sizeof(struct sleep_save_sp));
+   DEFINE(SLEEP_SAVE_SP_PHYS,	offsetof(struct sleep_save_sp, save_ptr_stash_phys));
+   DEFINE(SLEEP_SAVE_SP_VIRT,	offsetof(struct sleep_save_sp, save_ptr_stash));
++  DEFINE(SLEEP_STACK_DATA_SYSTEM_REGS,	offsetof(struct sleep_stack_data, system_regs));
++  DEFINE(SLEEP_STACK_DATA_CALLEE_REGS,	offsetof(struct sleep_stack_data, callee_saved_regs));
+ #endif
+   DEFINE(ARM_SMCCC_RES_X0_OFFS,	offsetof(struct arm_smccc_res, a0));
+   DEFINE(ARM_SMCCC_RES_X2_OFFS,	offsetof(struct arm_smccc_res, a2));
+diff --git a/arch/arm64/kernel/sleep.S b/arch/arm64/kernel/sleep.S
+index f586f7c..c1c28c0 100644
+--- a/arch/arm64/kernel/sleep.S
++++ b/arch/arm64/kernel/sleep.S
+@@ -49,37 +49,30 @@
+ 	orr	\dst, \dst, \mask		// dst|=(aff3>>rs3)
+ 	.endm
+ /*
+- * Save CPU state for a suspend and execute the suspend finisher.
+- * On success it will return 0 through cpu_resume - ie through a CPU
+- * soft/hard reboot from the reset vector.
+- * On failure it returns the suspend finisher return value or force
+- * -EOPNOTSUPP if the finisher erroneously returns 0 (the suspend finisher
+- * is not allowed to return, if it does this must be considered failure).
+- * It saves callee registers, and allocates space on the kernel stack
+- * to save the CPU specific registers + some other data for resume.
++ * Save CPU state in the provided sleep_stack_data area, and publish its
++ * location for cpu_resume()'s use in sleep_save_stash.
+  *
+- *  x0 = suspend finisher argument
+- *  x1 = suspend finisher function pointer
++ * cpu_resume() will restore this saved state, and return. Because the
++ * link-register is saved and restored, it will appear to return from this
++ * function. So that the caller can tell the suspend/resume paths apart,
++ * __cpu_suspend_enter() will always return a non-zero value, whereas the
++ * path through cpu_resume() will return 0.
++ *
++ *  x0 = struct sleep_stack_data area
+  */
+ ENTRY(__cpu_suspend_enter)
+-	stp	x29, lr, [sp, #-96]!
+-	stp	x19, x20, [sp,#16]
+-	stp	x21, x22, [sp,#32]
+-	stp	x23, x24, [sp,#48]
+-	stp	x25, x26, [sp,#64]
+-	stp	x27, x28, [sp,#80]
+-	/*
+-	 * Stash suspend finisher and its argument in x20 and x19
+-	 */
+-	mov	x19, x0
+-	mov	x20, x1
++	stp	x29, lr, [x0, #SLEEP_STACK_DATA_CALLEE_REGS]
++	stp	x19, x20, [x0,#SLEEP_STACK_DATA_CALLEE_REGS+16]
++	stp	x21, x22, [x0,#SLEEP_STACK_DATA_CALLEE_REGS+32]
++	stp	x23, x24, [x0,#SLEEP_STACK_DATA_CALLEE_REGS+48]
++	stp	x25, x26, [x0,#SLEEP_STACK_DATA_CALLEE_REGS+64]
++	stp	x27, x28, [x0,#SLEEP_STACK_DATA_CALLEE_REGS+80]
++
++	/* save the sp in cpu_suspend_ctx */
+ 	mov	x2, sp
+-	sub	sp, sp, #CPU_SUSPEND_SZ	// allocate cpu_suspend_ctx
+-	mov	x0, sp
+-	/*
+-	 * x0 now points to struct cpu_suspend_ctx allocated on the stack
+-	 */
+-	str	x2, [x0, #CPU_CTX_SP]
++	str	x2, [x0, #SLEEP_STACK_DATA_SYSTEM_REGS + CPU_CTX_SP]
++
++	/* find the mpidr_hash */
+ 	ldr	x1, =sleep_save_sp
+ 	ldr	x1, [x1, #SLEEP_SAVE_SP_VIRT]
+ 	mrs	x7, mpidr_el1
+@@ -93,34 +86,11 @@ ENTRY(__cpu_suspend_enter)
+ 	ldp	w5, w6, [x9, #(MPIDR_HASH_SHIFTS + 8)]
+ 	compute_mpidr_hash x8, x3, x4, x5, x6, x7, x10
+ 	add	x1, x1, x8, lsl #3
++
++	stp	x29, lr, [sp, #-16]!
+ 	bl	__cpu_suspend_save
+-	/*
+-	 * Grab suspend finisher in x20 and its argument in x19
+-	 */
+-	mov	x0, x19
+-	mov	x1, x20
+-	/*
+-	 * We are ready for power down, fire off the suspend finisher
+-	 * in x1, with argument in x0
+-	 */
+-	blr	x1
+-        /*
+-	 * Never gets here, unless suspend finisher fails.
+-	 * Successful cpu_suspend should return from cpu_resume, returning
+-	 * through this code path is considered an error
+-	 * If the return value is set to 0 force x0 = -EOPNOTSUPP
+-	 * to make sure a proper error condition is propagated
+-	 */
+-	cmp	x0, #0
+-	mov	x3, #-EOPNOTSUPP
+-	csel	x0, x3, x0, eq
+-	add	sp, sp, #CPU_SUSPEND_SZ	// rewind stack pointer
+-	ldp	x19, x20, [sp, #16]
+-	ldp	x21, x22, [sp, #32]
+-	ldp	x23, x24, [sp, #48]
+-	ldp	x25, x26, [sp, #64]
+-	ldp	x27, x28, [sp, #80]
+-	ldp	x29, lr, [sp], #96
++	ldp	x29, lr, [sp], #16
++	mov	x0, #1
+ 	ret
+ ENDPROC(__cpu_suspend_enter)
+ 	.ltorg
+@@ -146,12 +116,6 @@ ENDPROC(cpu_resume_mmu)
+ 	.popsection
+ cpu_resume_after_mmu:
+ 	mov	x0, #0			// return zero on success
+-	ldp	x19, x20, [sp, #16]
+-	ldp	x21, x22, [sp, #32]
+-	ldp	x23, x24, [sp, #48]
+-	ldp	x25, x26, [sp, #64]
+-	ldp	x27, x28, [sp, #80]
+-	ldp	x29, lr, [sp], #96
+ 	ret
+ ENDPROC(cpu_resume_after_mmu)
+ 
+@@ -168,6 +132,8 @@ ENTRY(cpu_resume)
+         /* x7 contains hash index, let's use it to grab context pointer */
+ 	ldr_l	x0, sleep_save_sp + SLEEP_SAVE_SP_PHYS
+ 	ldr	x0, [x0, x7, lsl #3]
++	add	x29, x0, #SLEEP_STACK_DATA_CALLEE_REGS
++	add	x0, x0, #SLEEP_STACK_DATA_SYSTEM_REGS
+ 	/* load sp from context */
+ 	ldr	x2, [x0, #CPU_CTX_SP]
+ 	/* load physical address of identity map page table in x1 */
+@@ -178,5 +144,12 @@ ENTRY(cpu_resume)
+ 	 * pointer and x1 to contain physical address of 1:1 page tables
+ 	 */
+ 	bl	cpu_do_resume		// PC relative jump, MMU off
++	/* Can't access these by physical address once the MMU is on */
++	ldp	x19, x20, [x29, #16]
++	ldp	x21, x22, [x29, #32]
++	ldp	x23, x24, [x29, #48]
++	ldp	x25, x26, [x29, #64]
++	ldp	x27, x28, [x29, #80]
++	ldp	x29, lr, [x29]
+ 	b	cpu_resume_mmu		// Resume MMU, never returns
+ ENDPROC(cpu_resume)
+diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
+index 20b6b9b..0088cd2 100644
+--- a/arch/arm64/kernel/suspend.c
++++ b/arch/arm64/kernel/suspend.c
+@@ -12,22 +12,22 @@
+ #include <asm/suspend.h>
+ #include <asm/tlbflush.h>
+ 
+-extern int __cpu_suspend_enter(unsigned long arg, int (*fn)(unsigned long));
++
+ /*
+  * This is called by __cpu_suspend_enter() to save the state, and do whatever
+  * flushing is required to ensure that when the CPU goes to sleep we have
+  * the necessary data available when the caches are not searched.
+  *
+- * ptr: CPU context virtual address
++ * ptr: sleep_stack_data containing cpu state virtual address.
+  * save_ptr: address of the location where the context physical address
+  *           must be saved
+  */
+-void notrace __cpu_suspend_save(struct cpu_suspend_ctx *ptr,
++void notrace __cpu_suspend_save(struct sleep_stack_data *ptr,
+ 				phys_addr_t *save_ptr)
+ {
+ 	*save_ptr = virt_to_phys(ptr);
+ 
+-	cpu_do_suspend(ptr);
++	cpu_do_suspend(&ptr->system_regs);
+ 	/*
+ 	 * Only flush the context that must be retrieved with the MMU
+ 	 * off. VA primitives ensure the flush is applied to all
+@@ -53,6 +53,30 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
+ 	hw_breakpoint_restore = hw_bp_restore;
+ }
+ 
++void notrace __cpu_suspend_exit(void)
++{
++	/*
++	 * We are resuming from reset with the idmap active in TTBR0_EL1.
++	 * We must uninstall the idmap and restore the expected MMU
++	 * state before we can possibly return to userspace.
++	 */
++	cpu_uninstall_idmap();
++
++	/*
++	 * Restore per-cpu offset before any kernel
++	 * subsystem relying on it has a chance to run.
++	 */
++	set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
++
++	/*
++	 * Restore HW breakpoint registers to sane values
++	 * before debug exceptions are possibly reenabled
++	 * through local_dbg_restore.
++	 */
++	if (hw_breakpoint_restore)
++		hw_breakpoint_restore(NULL);
++}
++
+ /*
+  * cpu_suspend
+  *
+@@ -62,8 +86,9 @@ void __init cpu_suspend_set_dbg_restorer(void (*hw_bp_restore)(void *))
+  */
+ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+ {
+-	int ret;
++	int ret = 0;
+ 	unsigned long flags;
++	struct sleep_stack_data state;
+ 
+ 	/*
+ 	 * From this point debug exceptions are disabled to prevent
+@@ -79,26 +104,9 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+ 	 */
+ 	pause_graph_tracing();
+ 
+-	/*
+-	 * mm context saved on the stack, it will be restored when
+-	 * the cpu comes out of reset through the identity mapped
+-	 * page tables, so that the thread address space is properly
+-	 * set-up on function return.
+-	 */
+-	ret = __cpu_suspend_enter(arg, fn);
+-	if (ret == 0) {
+-		/*
+-		 * We are resuming from reset with the idmap active in TTBR0_EL1.
+-		 * We must uninstall the idmap and restore the expected MMU
+-		 * state before we can possibly return to userspace.
+-		 */
+-		cpu_uninstall_idmap();
+-
+-		/*
+-		 * Restore per-cpu offset before any kernel
+-		 * subsystem relying on it has a chance to run.
+-		 */
+-		set_my_cpu_offset(per_cpu_offset(smp_processor_id()));
++	if (__cpu_suspend_enter(&state)) {
++		/* Call the suspend finisher */
++		ret = fn(arg);
+ 
+ 		/*
+ 		 * PSTATE was not saved over suspend/resume, re-enable any
+@@ -108,12 +116,16 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+ 				CONFIG_ARM64_PAN));
+ 
+ 		/*
+-		 * Restore HW breakpoint registers to sane values
+-		 * before debug exceptions are possibly reenabled
+-		 * through local_dbg_restore.
++		 * Never gets here, unless the suspend finisher fails.
++		 * Successful cpu_suspend() should return from cpu_resume(),
++		 * returning through this code path is considered an error
++		 * If the return value is set to 0 force ret = -EOPNOTSUPP
++		 * to make sure a proper error condition is propagated
+ 		 */
+-		if (hw_breakpoint_restore)
+-			hw_breakpoint_restore(NULL);
++		if (!ret)
++			ret = -EOPNOTSUPP;
++	} else {
++		__cpu_suspend_exit();
+ 	}
+ 
+ 	unpause_graph_tracing();
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0070-arm64-Change-cpu_resume-to-enable-mmu-early-then-acc.patch b/tools/kdump/0070-arm64-Change-cpu_resume-to-enable-mmu-early-then-acc.patch
new file mode 100644
index 0000000..659de94
--- /dev/null
+++ b/tools/kdump/0070-arm64-Change-cpu_resume-to-enable-mmu-early-then-acc.patch
@@ -0,0 +1,378 @@
+From 0fdd71a93ef36e23e47470c622e0bc84bfd94863 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:07 +0100
+Subject: [PATCH 070/123] arm64: Change cpu_resume() to enable mmu early then
+ access sleep_sp by va
+
+By enabling the MMU early in cpu_resume(), the sleep_save_sp and stack can
+be accessed by VA, which avoids the need to convert-addresses and clean to
+PoC on the suspend path.
+
+MMU setup is shared with the boot path, meaning the swapper_pg_dir is
+restored directly: ttbr1_el1 is no longer saved/restored.
+
+struct sleep_save_sp is removed, replacing it with a single array of
+pointers.
+
+cpu_do_{suspend,resume} could be further reduced to not restore: cpacr_el1,
+mdscr_el1, tcr_el1, vbar_el1 and sctlr_el1, all of which are set by
+__cpu_setup(). However these values all contain res0 bits that may be used
+to enable future features.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Reviewed-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit cabe1c81ea5be983425d117912d7883e252a3b09)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kernel/head.S
+	remove KASAN change in arch/arm64/kernel/sleep.S
+---
+ arch/arm64/include/asm/suspend.h |  9 +++----
+ arch/arm64/kernel/asm-offsets.c  |  3 ---
+ arch/arm64/kernel/head.S         |  2 +-
+ arch/arm64/kernel/setup.c        |  1 -
+ arch/arm64/kernel/sleep.S        | 57 +++++++++++++++-------------------------
+ arch/arm64/kernel/suspend.c      | 38 +++++----------------------
+ arch/arm64/mm/proc.S             | 53 +++++++++++++++----------------------
+ 7 files changed, 52 insertions(+), 111 deletions(-)
+
+diff --git a/arch/arm64/include/asm/suspend.h b/arch/arm64/include/asm/suspend.h
+index 365d8cd..29d3c71 100644
+--- a/arch/arm64/include/asm/suspend.h
++++ b/arch/arm64/include/asm/suspend.h
+@@ -1,7 +1,7 @@
+ #ifndef __ASM_SUSPEND_H
+ #define __ASM_SUSPEND_H
+ 
+-#define NR_CTX_REGS 11
++#define NR_CTX_REGS 10
+ #define NR_CALLEE_SAVED_REGS 12
+ 
+ /*
+@@ -17,11 +17,6 @@ struct cpu_suspend_ctx {
+ 	u64 sp;
+ } __aligned(16);
+ 
+-struct sleep_save_sp {
+-	phys_addr_t *save_ptr_stash;
+-	phys_addr_t save_ptr_stash_phys;
+-};
+-
+ /*
+  * Memory to save the cpu state is allocated on the stack by
+  * __cpu_suspend_enter()'s caller, and populated by __cpu_suspend_enter().
+@@ -39,6 +34,8 @@ struct sleep_stack_data {
+ 	unsigned long		callee_saved_regs[NR_CALLEE_SAVED_REGS];
+ };
+ 
++extern unsigned long *sleep_save_stash;
++
+ extern int cpu_suspend(unsigned long arg, int (*fn)(unsigned long));
+ extern void cpu_resume(void);
+ int __cpu_suspend_enter(struct sleep_stack_data *state);
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 2abd57f..52b4c8c 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -132,9 +132,6 @@ int main(void)
+   DEFINE(CPU_CTX_SP,		offsetof(struct cpu_suspend_ctx, sp));
+   DEFINE(MPIDR_HASH_MASK,	offsetof(struct mpidr_hash, mask));
+   DEFINE(MPIDR_HASH_SHIFTS,	offsetof(struct mpidr_hash, shift_aff));
+-  DEFINE(SLEEP_SAVE_SP_SZ,	sizeof(struct sleep_save_sp));
+-  DEFINE(SLEEP_SAVE_SP_PHYS,	offsetof(struct sleep_save_sp, save_ptr_stash_phys));
+-  DEFINE(SLEEP_SAVE_SP_VIRT,	offsetof(struct sleep_save_sp, save_ptr_stash));
+   DEFINE(SLEEP_STACK_DATA_SYSTEM_REGS,	offsetof(struct sleep_stack_data, system_regs));
+   DEFINE(SLEEP_STACK_DATA_CALLEE_REGS,	offsetof(struct sleep_stack_data, callee_saved_regs));
+ #endif
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index 8e22dfe..bb92dd0 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -628,7 +628,7 @@ ENDPROC(__secondary_switched)
+  * If it isn't, park the CPU
+  */
+ 	.section	".idmap.text", "ax"
+-__enable_mmu:
++ENTRY(__enable_mmu)
+ 	mrs	x1, ID_AA64MMFR0_EL1
+ 	ubfx	x2, x1, #ID_AA64MMFR0_TGRAN_SHIFT, 4
+ 	cmp	x2, #ID_AA64MMFR0_TGRAN_SUPPORTED
+diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c
+index cfed56f..ad29060 100644
+--- a/arch/arm64/kernel/setup.c
++++ b/arch/arm64/kernel/setup.c
+@@ -175,7 +175,6 @@ static void __init smp_build_mpidr_hash(void)
+ 	 */
+ 	if (mpidr_hash_size() > 4 * num_possible_cpus())
+ 		pr_warn("Large number of MPIDR hash buckets detected\n");
+-	__flush_dcache_area(&mpidr_hash, sizeof(struct mpidr_hash));
+ }
+ 
+ static void __init setup_machine_fdt(phys_addr_t dt_phys)
+diff --git a/arch/arm64/kernel/sleep.S b/arch/arm64/kernel/sleep.S
+index c1c28c0..b7d8f81 100644
+--- a/arch/arm64/kernel/sleep.S
++++ b/arch/arm64/kernel/sleep.S
+@@ -73,8 +73,8 @@ ENTRY(__cpu_suspend_enter)
+ 	str	x2, [x0, #SLEEP_STACK_DATA_SYSTEM_REGS + CPU_CTX_SP]
+ 
+ 	/* find the mpidr_hash */
+-	ldr	x1, =sleep_save_sp
+-	ldr	x1, [x1, #SLEEP_SAVE_SP_VIRT]
++	ldr	x1, =sleep_save_stash
++	ldr	x1, [x1]
+ 	mrs	x7, mpidr_el1
+ 	ldr	x9, =mpidr_hash
+ 	ldr	x10, [x9, #MPIDR_HASH_MASK]
+@@ -87,40 +87,27 @@ ENTRY(__cpu_suspend_enter)
+ 	compute_mpidr_hash x8, x3, x4, x5, x6, x7, x10
+ 	add	x1, x1, x8, lsl #3
+ 
++	str	x0, [x1]
++	add	x0, x0, #SLEEP_STACK_DATA_SYSTEM_REGS
+ 	stp	x29, lr, [sp, #-16]!
+-	bl	__cpu_suspend_save
++	bl	cpu_do_suspend
+ 	ldp	x29, lr, [sp], #16
+ 	mov	x0, #1
+ 	ret
+ ENDPROC(__cpu_suspend_enter)
+ 	.ltorg
+ 
+-/*
+- * x0 must contain the sctlr value retrieved from restored context
+- */
+-	.pushsection	".idmap.text", "ax"
+-ENTRY(cpu_resume_mmu)
+-	ldr	x3, =cpu_resume_after_mmu
+-	msr	sctlr_el1, x0		// restore sctlr_el1
+-	isb
+-	/*
+-	 * Invalidate the local I-cache so that any instructions fetched
+-	 * speculatively from the PoC are discarded, since they may have
+-	 * been dynamically patched at the PoU.
+-	 */
+-	ic	iallu
+-	dsb	nsh
+-	isb
+-	br	x3			// global jump to virtual address
+-ENDPROC(cpu_resume_mmu)
+-	.popsection
+-cpu_resume_after_mmu:
+-	mov	x0, #0			// return zero on success
+-	ret
+-ENDPROC(cpu_resume_after_mmu)
+-
+ ENTRY(cpu_resume)
+ 	bl	el2_setup		// if in EL2 drop to EL1 cleanly
++	/* enable the MMU early - so we can access sleep_save_stash by va */
++	adr_l	lr, __enable_mmu	/* __cpu_setup will return here */
++	ldr	x27, =_cpu_resume	/* __enable_mmu will branch here */
++	adrp	x25, idmap_pg_dir
++	adrp	x26, swapper_pg_dir
++	b	__cpu_setup
++ENDPROC(cpu_resume)
++
++ENTRY(_cpu_resume)
+ 	mrs	x1, mpidr_el1
+ 	adrp	x8, mpidr_hash
+ 	add x8, x8, #:lo12:mpidr_hash // x8 = struct mpidr_hash phys address
+@@ -130,26 +117,24 @@ ENTRY(cpu_resume)
+ 	ldp	w5, w6, [x8, #(MPIDR_HASH_SHIFTS + 8)]
+ 	compute_mpidr_hash x7, x3, x4, x5, x6, x1, x2
+         /* x7 contains hash index, let's use it to grab context pointer */
+-	ldr_l	x0, sleep_save_sp + SLEEP_SAVE_SP_PHYS
++	ldr_l	x0, sleep_save_stash
+ 	ldr	x0, [x0, x7, lsl #3]
+ 	add	x29, x0, #SLEEP_STACK_DATA_CALLEE_REGS
+ 	add	x0, x0, #SLEEP_STACK_DATA_SYSTEM_REGS
+ 	/* load sp from context */
+ 	ldr	x2, [x0, #CPU_CTX_SP]
+-	/* load physical address of identity map page table in x1 */
+-	adrp	x1, idmap_pg_dir
+ 	mov	sp, x2
+ 	/*
+-	 * cpu_do_resume expects x0 to contain context physical address
+-	 * pointer and x1 to contain physical address of 1:1 page tables
++	 * cpu_do_resume expects x0 to contain context address pointer
+ 	 */
+-	bl	cpu_do_resume		// PC relative jump, MMU off
+-	/* Can't access these by physical address once the MMU is on */
++	bl	cpu_do_resume
++
+ 	ldp	x19, x20, [x29, #16]
+ 	ldp	x21, x22, [x29, #32]
+ 	ldp	x23, x24, [x29, #48]
+ 	ldp	x25, x26, [x29, #64]
+ 	ldp	x27, x28, [x29, #80]
+ 	ldp	x29, lr, [x29]
+-	b	cpu_resume_mmu		// Resume MMU, never returns
+-ENDPROC(cpu_resume)
++	mov	x0, #0
++	ret
++ENDPROC(_cpu_resume)
+diff --git a/arch/arm64/kernel/suspend.c b/arch/arm64/kernel/suspend.c
+index 0088cd2..6a5dbae 100644
+--- a/arch/arm64/kernel/suspend.c
++++ b/arch/arm64/kernel/suspend.c
+@@ -12,30 +12,11 @@
+ #include <asm/suspend.h>
+ #include <asm/tlbflush.h>
+ 
+-
+ /*
+- * This is called by __cpu_suspend_enter() to save the state, and do whatever
+- * flushing is required to ensure that when the CPU goes to sleep we have
+- * the necessary data available when the caches are not searched.
+- *
+- * ptr: sleep_stack_data containing cpu state virtual address.
+- * save_ptr: address of the location where the context physical address
+- *           must be saved
++ * This is allocated by cpu_suspend_init(), and used to store a pointer to
++ * the 'struct sleep_stack_data' the contains a particular CPUs state.
+  */
+-void notrace __cpu_suspend_save(struct sleep_stack_data *ptr,
+-				phys_addr_t *save_ptr)
+-{
+-	*save_ptr = virt_to_phys(ptr);
+-
+-	cpu_do_suspend(&ptr->system_regs);
+-	/*
+-	 * Only flush the context that must be retrieved with the MMU
+-	 * off. VA primitives ensure the flush is applied to all
+-	 * cache levels so context is pushed to DRAM.
+-	 */
+-	__flush_dcache_area(ptr, sizeof(*ptr));
+-	__flush_dcache_area(save_ptr, sizeof(*save_ptr));
+-}
++unsigned long *sleep_save_stash;
+ 
+ /*
+  * This hook is provided so that cpu_suspend code can restore HW
+@@ -140,22 +121,15 @@ int cpu_suspend(unsigned long arg, int (*fn)(unsigned long))
+ 	return ret;
+ }
+ 
+-struct sleep_save_sp sleep_save_sp;
+-
+ static int __init cpu_suspend_init(void)
+ {
+-	void *ctx_ptr;
+-
+ 	/* ctx_ptr is an array of physical addresses */
+-	ctx_ptr = kcalloc(mpidr_hash_size(), sizeof(phys_addr_t), GFP_KERNEL);
++	sleep_save_stash = kcalloc(mpidr_hash_size(), sizeof(*sleep_save_stash),
++				   GFP_KERNEL);
+ 
+-	if (WARN_ON(!ctx_ptr))
++	if (WARN_ON(!sleep_save_stash))
+ 		return -ENOMEM;
+ 
+-	sleep_save_sp.save_ptr_stash = ctx_ptr;
+-	sleep_save_sp.save_ptr_stash_phys = virt_to_phys(ctx_ptr);
+-	__flush_dcache_area(&sleep_save_sp, sizeof(struct sleep_save_sp));
+-
+ 	return 0;
+ }
+ early_initcall(cpu_suspend_init);
+diff --git a/arch/arm64/mm/proc.S b/arch/arm64/mm/proc.S
+index b6a141f..5bb61de 100644
+--- a/arch/arm64/mm/proc.S
++++ b/arch/arm64/mm/proc.S
+@@ -24,6 +24,7 @@
+ #include <asm/asm-offsets.h>
+ #include <asm/hwcap.h>
+ #include <asm/pgtable.h>
++#include <asm/pgtable-hwdef.h>
+ #include <asm/cpufeature.h>
+ #include <asm/alternative.h>
+ 
+@@ -63,62 +64,50 @@ ENTRY(cpu_do_suspend)
+ 	mrs	x2, tpidr_el0
+ 	mrs	x3, tpidrro_el0
+ 	mrs	x4, contextidr_el1
+-	mrs	x5, mair_el1
+-	mrs	x6, cpacr_el1
+-	mrs	x7, ttbr1_el1
+-	mrs	x8, tcr_el1
+-	mrs	x9, vbar_el1
+-	mrs	x10, mdscr_el1
+-	mrs	x11, oslsr_el1
+-	mrs	x12, sctlr_el1
++	mrs	x5, cpacr_el1
++	mrs	x6, tcr_el1
++	mrs	x7, vbar_el1
++	mrs	x8, mdscr_el1
++	mrs	x9, oslsr_el1
++	mrs	x10, sctlr_el1
+ 	stp	x2, x3, [x0]
+-	stp	x4, x5, [x0, #16]
+-	stp	x6, x7, [x0, #32]
+-	stp	x8, x9, [x0, #48]
+-	stp	x10, x11, [x0, #64]
+-	str	x12, [x0, #80]
++	stp	x4, xzr, [x0, #16]
++	stp	x5, x6, [x0, #32]
++	stp	x7, x8, [x0, #48]
++	stp	x9, x10, [x0, #64]
+ 	ret
+ ENDPROC(cpu_do_suspend)
+ 
+ /**
+  * cpu_do_resume - restore CPU register context
+  *
+- * x0: Physical address of context pointer
+- * x1: ttbr0_el1 to be restored
+- *
+- * Returns:
+- *	sctlr_el1 value in x0
++ * x0: Address of context pointer
+  */
+ ENTRY(cpu_do_resume)
+-	/*
+-	 * Invalidate local tlb entries before turning on MMU
+-	 */
+-	tlbi	vmalle1
+ 	ldp	x2, x3, [x0]
+ 	ldp	x4, x5, [x0, #16]
+-	ldp	x6, x7, [x0, #32]
+-	ldp	x8, x9, [x0, #48]
+-	ldp	x10, x11, [x0, #64]
+-	ldr	x12, [x0, #80]
++	ldp	x6, x8, [x0, #32]
++	ldp	x9, x10, [x0, #48]
++	ldp	x11, x12, [x0, #64]
+ 	msr	tpidr_el0, x2
+ 	msr	tpidrro_el0, x3
+ 	msr	contextidr_el1, x4
+-	msr	mair_el1, x5
+ 	msr	cpacr_el1, x6
+-	msr	ttbr0_el1, x1
+-	msr	ttbr1_el1, x7
+-	tcr_set_idmap_t0sz x8, x7
++
++	/* Don't change t0sz here, mask those bits when restoring */
++	mrs	x5, tcr_el1
++	bfi	x8, x5, TCR_T0SZ_OFFSET, TCR_TxSZ_WIDTH
++
+ 	msr	tcr_el1, x8
+ 	msr	vbar_el1, x9
+ 	msr	mdscr_el1, x10
++	msr	sctlr_el1, x12
+ 	/*
+ 	 * Restore oslsr_el1 by writing oslar_el1
+ 	 */
+ 	ubfx	x11, x11, #1, #1
+ 	msr	oslar_el1, x11
+ 	reset_pmuserenr_el0 x0			// Disable PMU access from EL0
+-	mov	x0, x12
+-	dsb	nsh		// Make sure local tlb invalidation completed
+ 	isb
+ 	ret
+ ENDPROC(cpu_do_resume)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0071-arm64-kernel-Include-_AC-definition-in-page.h.patch b/tools/kdump/0071-arm64-kernel-Include-_AC-definition-in-page.h.patch
new file mode 100644
index 0000000..9aa9f5a
--- /dev/null
+++ b/tools/kdump/0071-arm64-kernel-Include-_AC-definition-in-page.h.patch
@@ -0,0 +1,35 @@
+From a5a02b409300d52d0585ccdf233daae94054cd02 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:08 +0100
+Subject: [PATCH 071/123] arm64: kernel: Include _AC definition in page.h
+
+page.h uses '_AC' in the definition of PAGE_SIZE, but doesn't include
+linux/const.h where this is defined. This produces build warnings when only
+asm/page.h is included by asm code.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 812264550dcba6cdbe84bfac2f27e7d23b5b8733)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/page.h | 2 ++
+ 1 file changed, 2 insertions(+)
+
+diff --git a/arch/arm64/include/asm/page.h b/arch/arm64/include/asm/page.h
+index 9b2f5a9..fbafd0a 100644
+--- a/arch/arm64/include/asm/page.h
++++ b/arch/arm64/include/asm/page.h
+@@ -19,6 +19,8 @@
+ #ifndef __ASM_PAGE_H
+ #define __ASM_PAGE_H
+ 
++#include <linux/const.h>
++
+ /* PAGE_SHIFT determines the page size */
+ /* CONT_SHIFT determines the number of pages which can be tracked together  */
+ #ifdef CONFIG_ARM64_64K_PAGES
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0072-arm64-Promote-KERNEL_START-KERNEL_END-definitions-to.patch b/tools/kdump/0072-arm64-Promote-KERNEL_START-KERNEL_END-definitions-to.patch
new file mode 100644
index 0000000..65b995b
--- /dev/null
+++ b/tools/kdump/0072-arm64-Promote-KERNEL_START-KERNEL_END-definitions-to.patch
@@ -0,0 +1,50 @@
+From 6cda01f69d18d3fd107c59c92b2b58e659ce3f53 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:09 +0100
+Subject: [PATCH 072/123] arm64: Promote KERNEL_START/KERNEL_END definitions to
+ a header file
+
+KERNEL_START and KERNEL_END are useful outside head.S, move them to a
+header file.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 28c7258330ee4ce701a4da7af96d6605d1a0b3bd)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/memory.h | 3 +++
+ arch/arm64/kernel/head.S        | 3 ---
+ 2 files changed, 3 insertions(+), 3 deletions(-)
+
+diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
+index 853953c..5773a66 100644
+--- a/arch/arm64/include/asm/memory.h
++++ b/arch/arm64/include/asm/memory.h
+@@ -70,6 +70,9 @@
+ 
+ #define TASK_UNMAPPED_BASE	(PAGE_ALIGN(TASK_SIZE / 4))
+ 
++#define KERNEL_START      _text
++#define KERNEL_END        _end
++
+ /*
+  * Physical vs virtual RAM address space conversion.  These are
+  * private definitions which should NOT be used outside memory.h
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index bb92dd0..0d84e73 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -48,9 +48,6 @@
+ #error TEXT_OFFSET must be less than 2MB
+ #endif
+ 
+-#define KERNEL_START	_text
+-#define KERNEL_END	_end
+-
+ /*
+  * Kernel startup entry point.
+  * ---------------------------
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0073-arm64-Add-new-asm-macro-copy_page.patch b/tools/kdump/0073-arm64-Add-new-asm-macro-copy_page.patch
new file mode 100644
index 0000000..e1899fb
--- /dev/null
+++ b/tools/kdump/0073-arm64-Add-new-asm-macro-copy_page.patch
@@ -0,0 +1,64 @@
+From e9dd73690db341b17ff7fd32e86624c72947a82f Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Wed, 27 Apr 2016 17:47:10 +0100
+Subject: [PATCH 073/123] arm64: Add new asm macro copy_page
+
+Kexec and hibernate need to copy pages of memory, but may not have all
+of the kernel mapped, and are unable to call copy_page().
+
+Add a simplistic copy_page() macro, that can be inlined in these
+situations. lib/copy_page.S provides a bigger better version, but
+uses more registers.
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+[Changed asm label to 9998, added commit message]
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+
+(cherry picked from commit 5003dbde45961dd7ab3d8a09ab9ad8bcb604db40)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/assembler.h | 19 +++++++++++++++++++
+ 1 file changed, 19 insertions(+)
+
+diff --git a/arch/arm64/include/asm/assembler.h b/arch/arm64/include/asm/assembler.h
+index 55fab0a..3c6d0df 100644
+--- a/arch/arm64/include/asm/assembler.h
++++ b/arch/arm64/include/asm/assembler.h
+@@ -24,6 +24,7 @@
+ #define __ASM_ASSEMBLER_H
+ 
+ #include <asm/asm-offsets.h>
++#include <asm/page.h>
+ #include <asm/pgtable-hwdef.h>
+ #include <asm/ptrace.h>
+ #include <asm/thread_info.h>
+@@ -274,6 +275,24 @@ lr	.req	x30		// link register
+ 	.endm
+ 
+ /*
++ * copy_page - copy src to dest using temp registers t1-t8
++ */
++	.macro copy_page dest:req src:req t1:req t2:req t3:req t4:req t5:req t6:req t7:req t8:req
++9998:	ldp	\t1, \t2, [\src]
++	ldp	\t3, \t4, [\src, #16]
++	ldp	\t5, \t6, [\src, #32]
++	ldp	\t7, \t8, [\src, #48]
++	add	\src, \src, #64
++	stnp	\t1, \t2, [\dest]
++	stnp	\t3, \t4, [\dest, #16]
++	stnp	\t5, \t6, [\dest, #32]
++	stnp	\t7, \t8, [\dest, #48]
++	add	\dest, \dest, #64
++	tst	\src, #(PAGE_SIZE - 1)
++	b.ne	9998b
++	.endm
++
++/*
+  * Annotate a function as position independent, i.e., safe to be called before
+  * the kernel virtual mapping is activated.
+  */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0074-PM-Hibernate-Call-flush_icache_range-on-pages-restor.patch b/tools/kdump/0074-PM-Hibernate-Call-flush_icache_range-on-pages-restor.patch
new file mode 100644
index 0000000..31652d2
--- /dev/null
+++ b/tools/kdump/0074-PM-Hibernate-Call-flush_icache_range-on-pages-restor.patch
@@ -0,0 +1,89 @@
+From 54a297a71b11bfaaec81dace55df4d24845c0394 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:11 +0100
+Subject: [PATCH 074/123] PM / Hibernate: Call flush_icache_range() on pages
+ restored in-place
+
+Some architectures require code written to memory as if it were data to be
+'cleaned' from any data caches before the processor can fetch them as new
+instructions.
+
+During resume from hibernate, the snapshot code copies some pages directly,
+meaning these architectures do not get a chance to perform their cache
+maintenance. Modify the read and decompress code to call
+flush_icache_range() on all pages that are restored, so that the restored
+in-place pages are guaranteed to be executable on these architectures.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Pavel Machek <pavel@ucw.cz>
+Acked-by: Rafael J. Wysocki <rjw@rjwysocki.net>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+[will: make clean_pages_on_* static and remove initialisers]
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+
+(cherry picked from commit f6cf0545ec697ddc278b7457b7d0c0d86a2ea88e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ kernel/power/swap.c | 18 ++++++++++++++++++
+ 1 file changed, 18 insertions(+)
+
+diff --git a/kernel/power/swap.c b/kernel/power/swap.c
+index 12cd989..160e100 100644
+--- a/kernel/power/swap.c
++++ b/kernel/power/swap.c
+@@ -37,6 +37,14 @@
+ #define HIBERNATE_SIG	"S1SUSPEND"
+ 
+ /*
++ * When reading an {un,}compressed image, we may restore pages in place,
++ * in which case some architectures need these pages cleaning before they
++ * can be executed. We don't know which pages these may be, so clean the lot.
++ */
++static bool clean_pages_on_read;
++static bool clean_pages_on_decompress;
++
++/*
+  *	The swap map is a data structure used for keeping track of each page
+  *	written to a swap partition.  It consists of many swap_map_page
+  *	structures that contain each an array of MAP_PAGE_ENTRIES swap entries.
+@@ -241,6 +249,9 @@ static void hib_end_io(struct bio *bio)
+ 
+ 	if (bio_data_dir(bio) == WRITE)
+ 		put_page(page);
++	else if (clean_pages_on_read)
++		flush_icache_range((unsigned long)page_address(page),
++				   (unsigned long)page_address(page) + PAGE_SIZE);
+ 
+ 	if (bio->bi_error && !hb->error)
+ 		hb->error = bio->bi_error;
+@@ -1049,6 +1060,7 @@ static int load_image(struct swap_map_handle *handle,
+ 
+ 	hib_init_batch(&hb);
+ 
++	clean_pages_on_read = true;
+ 	printk(KERN_INFO "PM: Loading image data pages (%u pages)...\n",
+ 		nr_to_read);
+ 	m = nr_to_read / 10;
+@@ -1124,6 +1136,10 @@ static int lzo_decompress_threadfn(void *data)
+ 		d->unc_len = LZO_UNC_SIZE;
+ 		d->ret = lzo1x_decompress_safe(d->cmp + LZO_HEADER, d->cmp_len,
+ 		                               d->unc, &d->unc_len);
++		if (clean_pages_on_decompress)
++			flush_icache_range((unsigned long)d->unc,
++					   (unsigned long)d->unc + d->unc_len);
++
+ 		atomic_set(&d->stop, 1);
+ 		wake_up(&d->done);
+ 	}
+@@ -1189,6 +1205,8 @@ static int load_image_lzo(struct swap_map_handle *handle,
+ 	}
+ 	memset(crc, 0, offsetof(struct crc_data, go));
+ 
++	clean_pages_on_decompress = true;
++
+ 	/*
+ 	 * Start the decompression threads.
+ 	 */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0075-arm64-mm-move-pte_-macros.patch b/tools/kdump/0075-arm64-mm-move-pte_-macros.patch
new file mode 100644
index 0000000..8c2d8bf
--- /dev/null
+++ b/tools/kdump/0075-arm64-mm-move-pte_-macros.patch
@@ -0,0 +1,73 @@
+From 1fd7e60179496bb6c1dde16dc0f78a817b91d0d4 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:03 +0000
+Subject: [PATCH 075/123] arm64: mm: move pte_* macros
+
+For pmd, pud, and pgd levels of table, functions including p?d_index and
+p?d_offset are defined after the p?d_page_vaddr function for the
+immediately higher level of table.
+
+The pte functions however are defined much earlier, even though several
+rely on the later definition of pmd_page_vaddr. While this isn't
+currently a problem as these are macros, it prevents the logical
+grouping of later C functions (which cannot rely on prototypes for
+functions not yet defined).
+
+Move these definitions after pmd_page_vaddr, for consistency with the
+placement of these functions for other levels of table.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 053520f7d3923cc6d37afb28f9887cb1e7d77454)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/pgtable.h | 20 ++++++++++----------
+ 1 file changed, 10 insertions(+), 10 deletions(-)
+
+diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
+index a11053a..0dba915 100644
+--- a/arch/arm64/include/asm/pgtable.h
++++ b/arch/arm64/include/asm/pgtable.h
+@@ -136,16 +136,6 @@ extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];
+ #define pte_clear(mm,addr,ptep)	set_pte(ptep, __pte(0))
+ #define pte_page(pte)		(pfn_to_page(pte_pfn(pte)))
+ 
+-/* Find an entry in the third-level page table. */
+-#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+-
+-#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
+-
+-#define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
+-#define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
+-#define pte_unmap(pte)			do { } while (0)
+-#define pte_unmap_nested(pte)		do { } while (0)
+-
+ /*
+  * The following only work if pte_present(). Undefined behaviour otherwise.
+  */
+@@ -430,6 +420,16 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
+ 	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
+ }
+ 
++/* Find an entry in the third-level page table. */
++#define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
++
++#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
++
++#define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
++#define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
++#define pte_unmap(pte)			do { } while (0)
++#define pte_unmap_nested(pte)		do { } while (0)
++
+ #define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+ 
+ /*
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0076-arm64-mm-add-functions-to-walk-page-tables-by-PA.patch b/tools/kdump/0076-arm64-mm-add-functions-to-walk-page-tables-by-PA.patch
new file mode 100644
index 0000000..a421424
--- /dev/null
+++ b/tools/kdump/0076-arm64-mm-add-functions-to-walk-page-tables-by-PA.patch
@@ -0,0 +1,129 @@
+From 8f1941a150c03134c31bd0b09d6f94ab4dd15f2c Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:04 +0000
+Subject: [PATCH 076/123] arm64: mm: add functions to walk page tables by PA
+
+To allow us to walk tables allocated into the fixmap, we need to acquire
+the physical address of a page, rather than the virtual address in the
+linear map.
+
+This patch adds new p??_page_paddr and p??_offset_phys functions to
+acquire the physical address of a next-level table, and changes
+p??_offset* into macros which simply convert this to a linear map VA.
+This renders p??_page_vaddr unused, and hence they are removed.
+
+At the pgd level, a new pgd_offset_raw function is added to find the
+relevant PGD entry given the base of a PGD and a virtual address.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit dca56dca7124709f3dfca81afe61b4d98eb9cacf)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/pgtable.h | 39 +++++++++++++++++++++++----------------
+ 1 file changed, 23 insertions(+), 16 deletions(-)
+
+diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
+index 0dba915..774854e3 100644
+--- a/arch/arm64/include/asm/pgtable.h
++++ b/arch/arm64/include/asm/pgtable.h
+@@ -415,15 +415,16 @@ static inline void pmd_clear(pmd_t *pmdp)
+ 	set_pmd(pmdp, __pmd(0));
+ }
+ 
+-static inline pte_t *pmd_page_vaddr(pmd_t pmd)
++static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
+ {
+-	return __va(pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK);
++	return pmd_val(pmd) & PHYS_MASK & (s32)PAGE_MASK;
+ }
+ 
+ /* Find an entry in the third-level page table. */
+ #define pte_index(addr)		(((addr) >> PAGE_SHIFT) & (PTRS_PER_PTE - 1))
+ 
+-#define pte_offset_kernel(dir,addr)	(pmd_page_vaddr(*(dir)) + pte_index(addr))
++#define pte_offset_phys(dir,addr)	(pmd_page_paddr(*(dir)) + pte_index(addr) * sizeof(pte_t))
++#define pte_offset_kernel(dir,addr)	((pte_t *)__va(pte_offset_phys((dir), (addr))))
+ 
+ #define pte_offset_map(dir,addr)	pte_offset_kernel((dir), (addr))
+ #define pte_offset_map_nested(dir,addr)	pte_offset_kernel((dir), (addr))
+@@ -458,21 +459,23 @@ static inline void pud_clear(pud_t *pudp)
+ 	set_pud(pudp, __pud(0));
+ }
+ 
+-static inline pmd_t *pud_page_vaddr(pud_t pud)
++static inline phys_addr_t pud_page_paddr(pud_t pud)
+ {
+-	return __va(pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK);
++	return pud_val(pud) & PHYS_MASK & (s32)PAGE_MASK;
+ }
+ 
+ /* Find an entry in the second-level page table. */
+ #define pmd_index(addr)		(((addr) >> PMD_SHIFT) & (PTRS_PER_PMD - 1))
+ 
+-static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)
+-{
+-	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);
+-}
++#define pmd_offset_phys(dir, addr)	(pud_page_paddr(*(dir)) + pmd_index(addr) * sizeof(pmd_t))
++#define pmd_offset(dir, addr)		((pmd_t *)__va(pmd_offset_phys((dir), (addr))))
+ 
+ #define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
+ 
++#else
++
++#define pud_page_paddr(pud)	({ BUILD_BUG(); 0; })
++
+ #endif	/* CONFIG_PGTABLE_LEVELS > 2 */
+ 
+ #if CONFIG_PGTABLE_LEVELS > 3
+@@ -494,21 +497,23 @@ static inline void pgd_clear(pgd_t *pgdp)
+ 	set_pgd(pgdp, __pgd(0));
+ }
+ 
+-static inline pud_t *pgd_page_vaddr(pgd_t pgd)
++static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
+ {
+-	return __va(pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK);
++	return pgd_val(pgd) & PHYS_MASK & (s32)PAGE_MASK;
+ }
+ 
+ /* Find an entry in the frst-level page table. */
+ #define pud_index(addr)		(((addr) >> PUD_SHIFT) & (PTRS_PER_PUD - 1))
+ 
+-static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
+-{
+-	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(addr);
+-}
++#define pud_offset_phys(dir, addr)	(pgd_page_paddr(*(dir)) + pud_index(addr) * sizeof(pud_t))
++#define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
+ 
+ #define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
+ 
++#else
++
++#define pgd_page_paddr(pgd)	({ BUILD_BUG(); 0;})
++
+ #endif  /* CONFIG_PGTABLE_LEVELS > 3 */
+ 
+ #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
+@@ -516,7 +521,9 @@ static inline pud_t *pud_offset(pgd_t *pgd, unsigned long addr)
+ /* to find an entry in a page-table-directory */
+ #define pgd_index(addr)		(((addr) >> PGDIR_SHIFT) & (PTRS_PER_PGD - 1))
+ 
+-#define pgd_offset(mm, addr)	((mm)->pgd+pgd_index(addr))
++#define pgd_offset_raw(pgd, addr)	((pgd) + pgd_index(addr))
++
++#define pgd_offset(mm, addr)	(pgd_offset_raw((mm)->pgd, (addr)))
+ 
+ /* to find an entry in a kernel page-table-directory */
+ #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0077-arm64-kernel-Add-support-for-hibernate-suspend-to-di.patch b/tools/kdump/0077-arm64-kernel-Add-support-for-hibernate-suspend-to-di.patch
new file mode 100644
index 0000000..6ab9564
--- /dev/null
+++ b/tools/kdump/0077-arm64-kernel-Add-support-for-hibernate-suspend-to-di.patch
@@ -0,0 +1,804 @@
+From 80e80499a8d390d23959b7c02ed3ff3d31bc6be0 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:12 +0100
+Subject: [PATCH 077/123] arm64: kernel: Add support for
+ hibernate/suspend-to-disk
+
+Add support for hibernate/suspend-to-disk.
+
+Suspend borrows code from cpu_suspend() to write cpu state onto the stack,
+before calling swsusp_save() to save the memory image.
+
+Restore creates a set of temporary page tables, covering only the
+linear map, copies the restore code to a 'safe' page, then uses the copy to
+restore the memory image. The copied code executes in the lower half of the
+address space, and once complete, restores the original kernel's page
+tables. It then calls into cpu_resume(), and follows the normal
+cpu_suspend() path back into the suspend code.
+
+To restore a kernel using KASLR, the address of the page tables, and
+cpu_resume() are stored in the hibernate arch-header and the el2
+vectors are pivotted via the 'safe' page in low memory.
+
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Kevin Hilman <khilman@baylibre.com> # Tested on Juno R2
+Signed-off-by: James Morse <james.morse@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 82869ac57b5d3b550446932c918dbf2caf020c9e)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kernel/Makefile
+---
+ arch/arm64/Kconfig                |   8 +
+ arch/arm64/include/asm/suspend.h  |   7 +
+ arch/arm64/kernel/Makefile        |   1 +
+ arch/arm64/kernel/asm-offsets.c   |   5 +
+ arch/arm64/kernel/hibernate-asm.S | 176 +++++++++++++++
+ arch/arm64/kernel/hibernate.c     | 461 ++++++++++++++++++++++++++++++++++++++
+ arch/arm64/kernel/vmlinux.lds.S   |  15 ++
+ 7 files changed, 673 insertions(+)
+ create mode 100644 arch/arm64/kernel/hibernate-asm.S
+ create mode 100644 arch/arm64/kernel/hibernate.c
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index 41d2637..b42e3a9 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -820,6 +820,14 @@ menu "Power management options"
+ 
+ source "kernel/power/Kconfig"
+ 
++config ARCH_HIBERNATION_POSSIBLE
++	def_bool y
++	depends on CPU_PM
++
++config ARCH_HIBERNATION_HEADER
++	def_bool y
++	depends on HIBERNATION
++
+ config ARCH_SUSPEND_POSSIBLE
+ 	def_bool y
+ 
+diff --git a/arch/arm64/include/asm/suspend.h b/arch/arm64/include/asm/suspend.h
+index 29d3c71..024d623 100644
+--- a/arch/arm64/include/asm/suspend.h
++++ b/arch/arm64/include/asm/suspend.h
+@@ -40,4 +40,11 @@ extern int cpu_suspend(unsigned long arg, int (*fn)(unsigned long));
+ extern void cpu_resume(void);
+ int __cpu_suspend_enter(struct sleep_stack_data *state);
+ void __cpu_suspend_exit(void);
++void _cpu_resume(void);
++
++int swsusp_arch_suspend(void);
++int swsusp_arch_resume(void);
++int arch_hibernation_header_save(void *addr, unsigned int max_size);
++int arch_hibernation_header_restore(void *addr);
++
+ #endif
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index 85f6924..8867ff7 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -40,6 +40,7 @@ arm64-obj-$(CONFIG_EFI)			+= efi.o efi-entry.stub.o
+ arm64-obj-$(CONFIG_PCI)			+= pci.o
+ arm64-obj-$(CONFIG_ARMV8_DEPRECATED)	+= armv8_deprecated.o
+ arm64-obj-$(CONFIG_ACPI)		+= acpi.o
++arm64-obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
+ 
+ obj-y					+= $(arm64-obj-y) vdso/ probes/
+ obj-m					+= $(arm64-obj-m)
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 52b4c8c..2bb17bd 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -22,6 +22,7 @@
+ #include <linux/mm.h>
+ #include <linux/dma-mapping.h>
+ #include <linux/kvm_host.h>
++#include <linux/suspend.h>
+ #include <asm/thread_info.h>
+ #include <asm/memory.h>
+ #include <asm/smp_plat.h>
+@@ -137,5 +138,9 @@ int main(void)
+ #endif
+   DEFINE(ARM_SMCCC_RES_X0_OFFS,	offsetof(struct arm_smccc_res, a0));
+   DEFINE(ARM_SMCCC_RES_X2_OFFS,	offsetof(struct arm_smccc_res, a2));
++  BLANK();
++  DEFINE(HIBERN_PBE_ORIG,	offsetof(struct pbe, orig_address));
++  DEFINE(HIBERN_PBE_ADDR,	offsetof(struct pbe, address));
++  DEFINE(HIBERN_PBE_NEXT,	offsetof(struct pbe, next));
+   return 0;
+ }
+diff --git a/arch/arm64/kernel/hibernate-asm.S b/arch/arm64/kernel/hibernate-asm.S
+new file mode 100644
+index 0000000..46f29b6
+--- /dev/null
++++ b/arch/arm64/kernel/hibernate-asm.S
+@@ -0,0 +1,176 @@
++/*
++ * Hibernate low-level support
++ *
++ * Copyright (C) 2016 ARM Ltd.
++ * Author:	James Morse <james.morse@arm.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++#include <linux/linkage.h>
++#include <linux/errno.h>
++
++#include <asm/asm-offsets.h>
++#include <asm/assembler.h>
++#include <asm/cputype.h>
++#include <asm/memory.h>
++#include <asm/page.h>
++#include <asm/virt.h>
++
++/*
++ * To prevent the possibility of old and new partial table walks being visible
++ * in the tlb, switch the ttbr to a zero page when we invalidate the old
++ * records. D4.7.1 'General TLB maintenance requirements' in ARM DDI 0487A.i
++ * Even switching to our copied tables will cause a changed output address at
++ * each stage of the walk.
++ */
++.macro break_before_make_ttbr_switch zero_page, page_table
++	msr	ttbr1_el1, \zero_page
++	isb
++	tlbi	vmalle1is
++	dsb	ish
++	msr	ttbr1_el1, \page_table
++	isb
++.endm
++
++
++/*
++ * Resume from hibernate
++ *
++ * Loads temporary page tables then restores the memory image.
++ * Finally branches to cpu_resume() to restore the state saved by
++ * swsusp_arch_suspend().
++ *
++ * Because this code has to be copied to a 'safe' page, it can't call out to
++ * other functions by PC-relative address. Also remember that it may be
++ * mid-way through over-writing other functions. For this reason it contains
++ * code from flush_icache_range() and uses the copy_page() macro.
++ *
++ * This 'safe' page is mapped via ttbr0, and executed from there. This function
++ * switches to a copy of the linear map in ttbr1, performs the restore, then
++ * switches ttbr1 to the original kernel's swapper_pg_dir.
++ *
++ * All of memory gets written to, including code. We need to clean the kernel
++ * text to the Point of Coherence (PoC) before secondary cores can be booted.
++ * Because the kernel modules and executable pages mapped to user space are
++ * also written as data, we clean all pages we touch to the Point of
++ * Unification (PoU).
++ *
++ * x0: physical address of temporary page tables
++ * x1: physical address of swapper page tables
++ * x2: address of cpu_resume
++ * x3: linear map address of restore_pblist in the current kernel
++ * x4: physical address of __hyp_stub_vectors, or 0
++ * x5: physical address of a  zero page that remains zero after resume
++ */
++.pushsection    ".hibernate_exit.text", "ax"
++ENTRY(swsusp_arch_suspend_exit)
++	/*
++	 * We execute from ttbr0, change ttbr1 to our copied linear map tables
++	 * with a break-before-make via the zero page
++	 */
++	break_before_make_ttbr_switch	x5, x0
++
++	mov	x21, x1
++	mov	x30, x2
++	mov	x24, x4
++	mov	x25, x5
++
++	/* walk the restore_pblist and use copy_page() to over-write memory */
++	mov	x19, x3
++
++1:	ldr	x10, [x19, #HIBERN_PBE_ORIG]
++	mov	x0, x10
++	ldr	x1, [x19, #HIBERN_PBE_ADDR]
++
++	copy_page	x0, x1, x2, x3, x4, x5, x6, x7, x8, x9
++
++	add	x1, x10, #PAGE_SIZE
++	/* Clean the copied page to PoU - based on flush_icache_range() */
++	dcache_line_size x2, x3
++	sub	x3, x2, #1
++	bic	x4, x10, x3
++2:	dc	cvau, x4	/* clean D line / unified line */
++	add	x4, x4, x2
++	cmp	x4, x1
++	b.lo	2b
++
++	ldr	x19, [x19, #HIBERN_PBE_NEXT]
++	cbnz	x19, 1b
++	dsb	ish		/* wait for PoU cleaning to finish */
++
++	/* switch to the restored kernels page tables */
++	break_before_make_ttbr_switch	x25, x21
++
++	ic	ialluis
++	dsb	ish
++	isb
++
++	cbz	x24, 3f		/* Do we need to re-initialise EL2? */
++	hvc	#0
++3:	ret
++
++	.ltorg
++ENDPROC(swsusp_arch_suspend_exit)
++
++/*
++ * Restore the hyp stub.
++ * This must be done before the hibernate page is unmapped by _cpu_resume(),
++ * but happens before any of the hyp-stub's code is cleaned to PoC.
++ *
++ * x24: The physical address of __hyp_stub_vectors
++ */
++el1_sync:
++	msr	vbar_el2, x24
++	eret
++ENDPROC(el1_sync)
++
++.macro invalid_vector	label
++\label:
++	b \label
++ENDPROC(\label)
++.endm
++
++	invalid_vector	el2_sync_invalid
++	invalid_vector	el2_irq_invalid
++	invalid_vector	el2_fiq_invalid
++	invalid_vector	el2_error_invalid
++	invalid_vector	el1_sync_invalid
++	invalid_vector	el1_irq_invalid
++	invalid_vector	el1_fiq_invalid
++	invalid_vector	el1_error_invalid
++
++/* el2 vectors - switch el2 here while we restore the memory image. */
++	.align 11
++ENTRY(hibernate_el2_vectors)
++	ventry	el2_sync_invalid		// Synchronous EL2t
++	ventry	el2_irq_invalid			// IRQ EL2t
++	ventry	el2_fiq_invalid			// FIQ EL2t
++	ventry	el2_error_invalid		// Error EL2t
++
++	ventry	el2_sync_invalid		// Synchronous EL2h
++	ventry	el2_irq_invalid			// IRQ EL2h
++	ventry	el2_fiq_invalid			// FIQ EL2h
++	ventry	el2_error_invalid		// Error EL2h
++
++	ventry	el1_sync			// Synchronous 64-bit EL1
++	ventry	el1_irq_invalid			// IRQ 64-bit EL1
++	ventry	el1_fiq_invalid			// FIQ 64-bit EL1
++	ventry	el1_error_invalid		// Error 64-bit EL1
++
++	ventry	el1_sync_invalid		// Synchronous 32-bit EL1
++	ventry	el1_irq_invalid			// IRQ 32-bit EL1
++	ventry	el1_fiq_invalid			// FIQ 32-bit EL1
++	ventry	el1_error_invalid		// Error 32-bit EL1
++END(hibernate_el2_vectors)
++
++.popsection
+diff --git a/arch/arm64/kernel/hibernate.c b/arch/arm64/kernel/hibernate.c
+new file mode 100644
+index 0000000..7e16fb3
+--- /dev/null
++++ b/arch/arm64/kernel/hibernate.c
+@@ -0,0 +1,461 @@
++/*:
++ * Hibernate support specific for ARM64
++ *
++ * Derived from work on ARM hibernation support by:
++ *
++ * Ubuntu project, hibernation support for mach-dove
++ * Copyright (C) 2010 Nokia Corporation (Hiroshi Doyu)
++ * Copyright (C) 2010 Texas Instruments, Inc. (Teerth Reddy et al.)
++ *  https://lkml.org/lkml/2010/6/18/4
++ *  https://lists.linux-foundation.org/pipermail/linux-pm/2010-June/027422.html
++ *  https://patchwork.kernel.org/patch/96442/
++ *
++ * Copyright (C) 2006 Rafael J. Wysocki <rjw@sisk.pl>
++ *
++ * License terms: GNU General Public License (GPL) version 2
++ */
++#define pr_fmt(x) "hibernate: " x
++#include <linux/kvm_host.h>
++#include <linux/mm.h>
++#include <linux/pm.h>
++#include <linux/sched.h>
++#include <linux/suspend.h>
++#include <linux/utsname.h>
++#include <linux/version.h>
++
++#include <asm/barrier.h>
++#include <asm/cacheflush.h>
++#include <asm/irqflags.h>
++#include <asm/memory.h>
++#include <asm/mmu_context.h>
++#include <asm/pgalloc.h>
++#include <asm/pgtable.h>
++#include <asm/pgtable-hwdef.h>
++#include <asm/sections.h>
++#include <asm/suspend.h>
++#include <asm/virt.h>
++
++/*
++ * Hibernate core relies on this value being 0 on resume, and marks it
++ * __nosavedata assuming it will keep the resume kernel's '0' value. This
++ * doesn't happen with either KASLR.
++ *
++ * defined as "__visible int in_suspend __nosavedata" in
++ * kernel/power/hibernate.c
++ */
++extern int in_suspend;
++
++/* Find a symbols alias in the linear map */
++#define LMADDR(x)	phys_to_virt(virt_to_phys(x))
++
++/* Do we need to reset el2? */
++#define el2_reset_needed() (is_hyp_mode_available() && !is_kernel_in_hyp_mode())
++
++/*
++ * Start/end of the hibernate exit code, this must be copied to a 'safe'
++ * location in memory, and executed from there.
++ */
++extern char __hibernate_exit_text_start[], __hibernate_exit_text_end[];
++
++/* temporary el2 vectors in the __hibernate_exit_text section. */
++extern char hibernate_el2_vectors[];
++
++/* hyp-stub vectors, used to restore el2 during resume from hibernate. */
++extern char __hyp_stub_vectors[];
++
++/*
++ * Values that may not change over hibernate/resume. We put the build number
++ * and date in here so that we guarantee not to resume with a different
++ * kernel.
++ */
++struct arch_hibernate_hdr_invariants {
++	char		uts_version[__NEW_UTS_LEN + 1];
++};
++
++/* These values need to be know across a hibernate/restore. */
++static struct arch_hibernate_hdr {
++	struct arch_hibernate_hdr_invariants invariants;
++
++	/* These are needed to find the relocated kernel if built with kaslr */
++	phys_addr_t	ttbr1_el1;
++	void		(*reenter_kernel)(void);
++
++	/*
++	 * We need to know where the __hyp_stub_vectors are after restore to
++	 * re-configure el2.
++	 */
++	phys_addr_t	__hyp_stub_vectors;
++} resume_hdr;
++
++static inline void arch_hdr_invariants(struct arch_hibernate_hdr_invariants *i)
++{
++	memset(i, 0, sizeof(*i));
++	memcpy(i->uts_version, init_utsname()->version, sizeof(i->uts_version));
++}
++
++int pfn_is_nosave(unsigned long pfn)
++{
++	unsigned long nosave_begin_pfn = virt_to_pfn(&__nosave_begin);
++	unsigned long nosave_end_pfn = virt_to_pfn(&__nosave_end - 1);
++
++	return (pfn >= nosave_begin_pfn) && (pfn <= nosave_end_pfn);
++}
++
++void notrace save_processor_state(void)
++{
++	WARN_ON(num_online_cpus() != 1);
++}
++
++void notrace restore_processor_state(void)
++{
++}
++
++int arch_hibernation_header_save(void *addr, unsigned int max_size)
++{
++	struct arch_hibernate_hdr *hdr = addr;
++
++	if (max_size < sizeof(*hdr))
++		return -EOVERFLOW;
++
++	arch_hdr_invariants(&hdr->invariants);
++	hdr->ttbr1_el1		= virt_to_phys(swapper_pg_dir);
++	hdr->reenter_kernel	= _cpu_resume;
++
++	/* We can't use __hyp_get_vectors() because kvm may still be loaded */
++	if (el2_reset_needed())
++		hdr->__hyp_stub_vectors = virt_to_phys(__hyp_stub_vectors);
++	else
++		hdr->__hyp_stub_vectors = 0;
++
++	return 0;
++}
++EXPORT_SYMBOL(arch_hibernation_header_save);
++
++int arch_hibernation_header_restore(void *addr)
++{
++	struct arch_hibernate_hdr_invariants invariants;
++	struct arch_hibernate_hdr *hdr = addr;
++
++	arch_hdr_invariants(&invariants);
++	if (memcmp(&hdr->invariants, &invariants, sizeof(invariants))) {
++		pr_crit("Hibernate image not generated by this kernel!\n");
++		return -EINVAL;
++	}
++
++	resume_hdr = *hdr;
++
++	return 0;
++}
++EXPORT_SYMBOL(arch_hibernation_header_restore);
++
++/*
++ * Copies length bytes, starting at src_start into an new page,
++ * perform cache maintentance, then maps it at the specified address low
++ * address as executable.
++ *
++ * This is used by hibernate to copy the code it needs to execute when
++ * overwriting the kernel text. This function generates a new set of page
++ * tables, which it loads into ttbr0.
++ *
++ * Length is provided as we probably only want 4K of data, even on a 64K
++ * page system.
++ */
++static int create_safe_exec_page(void *src_start, size_t length,
++				 unsigned long dst_addr,
++				 phys_addr_t *phys_dst_addr,
++				 void *(*allocator)(gfp_t mask),
++				 gfp_t mask)
++{
++	int rc = 0;
++	pgd_t *pgd;
++	pud_t *pud;
++	pmd_t *pmd;
++	pte_t *pte;
++	unsigned long dst = (unsigned long)allocator(mask);
++
++	if (!dst) {
++		rc = -ENOMEM;
++		goto out;
++	}
++
++	memcpy((void *)dst, src_start, length);
++	flush_icache_range(dst, dst + length);
++
++	pgd = pgd_offset_raw(allocator(mask), dst_addr);
++	if (pgd_none(*pgd)) {
++		pud = allocator(mask);
++		if (!pud) {
++			rc = -ENOMEM;
++			goto out;
++		}
++		pgd_populate(&init_mm, pgd, pud);
++	}
++
++	pud = pud_offset(pgd, dst_addr);
++	if (pud_none(*pud)) {
++		pmd = allocator(mask);
++		if (!pmd) {
++			rc = -ENOMEM;
++			goto out;
++		}
++		pud_populate(&init_mm, pud, pmd);
++	}
++
++	pmd = pmd_offset(pud, dst_addr);
++	if (pmd_none(*pmd)) {
++		pte = allocator(mask);
++		if (!pte) {
++			rc = -ENOMEM;
++			goto out;
++		}
++		pmd_populate_kernel(&init_mm, pmd, pte);
++	}
++
++	pte = pte_offset_kernel(pmd, dst_addr);
++	set_pte(pte, __pte(virt_to_phys((void *)dst) |
++			 pgprot_val(PAGE_KERNEL_EXEC)));
++
++	/* Load our new page tables */
++	asm volatile("msr	ttbr0_el1, %0;"
++		     "isb;"
++		     "tlbi	vmalle1is;"
++		     "dsb	ish;"
++		     "isb" : : "r"(virt_to_phys(pgd)));
++
++	*phys_dst_addr = virt_to_phys((void *)dst);
++
++out:
++	return rc;
++}
++
++
++int swsusp_arch_suspend(void)
++{
++	int ret = 0;
++	unsigned long flags;
++	struct sleep_stack_data state;
++
++	local_dbg_save(flags);
++
++	if (__cpu_suspend_enter(&state)) {
++		ret = swsusp_save();
++	} else {
++		/* Clean kernel to PoC for secondary core startup */
++		__flush_dcache_area(LMADDR(KERNEL_START), KERNEL_END - KERNEL_START);
++
++		/*
++		 * Tell the hibernation core that we've just restored
++		 * the memory
++		 */
++		in_suspend = 0;
++
++		__cpu_suspend_exit();
++	}
++
++	local_dbg_restore(flags);
++
++	return ret;
++}
++
++static int copy_pte(pmd_t *dst_pmd, pmd_t *src_pmd, unsigned long start,
++		    unsigned long end)
++{
++	pte_t *src_pte;
++	pte_t *dst_pte;
++	unsigned long addr = start;
++
++	dst_pte = (pte_t *)get_safe_page(GFP_ATOMIC);
++	if (!dst_pte)
++		return -ENOMEM;
++	pmd_populate_kernel(&init_mm, dst_pmd, dst_pte);
++	dst_pte = pte_offset_kernel(dst_pmd, start);
++
++	src_pte = pte_offset_kernel(src_pmd, start);
++	do {
++		if (!pte_none(*src_pte))
++			/*
++			 * Resume will overwrite areas that may be marked
++			 * read only (code, rodata). Clear the RDONLY bit from
++			 * the temporary mappings we use during restore.
++			 */
++			set_pte(dst_pte, __pte(pte_val(*src_pte) & ~PTE_RDONLY));
++	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
++
++	return 0;
++}
++
++static int copy_pmd(pud_t *dst_pud, pud_t *src_pud, unsigned long start,
++		    unsigned long end)
++{
++	pmd_t *src_pmd;
++	pmd_t *dst_pmd;
++	unsigned long next;
++	unsigned long addr = start;
++
++	if (pud_none(*dst_pud)) {
++		dst_pmd = (pmd_t *)get_safe_page(GFP_ATOMIC);
++		if (!dst_pmd)
++			return -ENOMEM;
++		pud_populate(&init_mm, dst_pud, dst_pmd);
++	}
++	dst_pmd = pmd_offset(dst_pud, start);
++
++	src_pmd = pmd_offset(src_pud, start);
++	do {
++		next = pmd_addr_end(addr, end);
++		if (pmd_none(*src_pmd))
++			continue;
++		if (pmd_table(*src_pmd)) {
++			if (copy_pte(dst_pmd, src_pmd, addr, next))
++				return -ENOMEM;
++		} else {
++			set_pmd(dst_pmd,
++				__pmd(pmd_val(*src_pmd) & ~PMD_SECT_RDONLY));
++		}
++	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
++
++	return 0;
++}
++
++static int copy_pud(pgd_t *dst_pgd, pgd_t *src_pgd, unsigned long start,
++		    unsigned long end)
++{
++	pud_t *dst_pud;
++	pud_t *src_pud;
++	unsigned long next;
++	unsigned long addr = start;
++
++	if (pgd_none(*dst_pgd)) {
++		dst_pud = (pud_t *)get_safe_page(GFP_ATOMIC);
++		if (!dst_pud)
++			return -ENOMEM;
++		pgd_populate(&init_mm, dst_pgd, dst_pud);
++	}
++	dst_pud = pud_offset(dst_pgd, start);
++
++	src_pud = pud_offset(src_pgd, start);
++	do {
++		next = pud_addr_end(addr, end);
++		if (pud_none(*src_pud))
++			continue;
++		if (pud_table(*(src_pud))) {
++			if (copy_pmd(dst_pud, src_pud, addr, next))
++				return -ENOMEM;
++		} else {
++			set_pud(dst_pud,
++				__pud(pud_val(*src_pud) & ~PMD_SECT_RDONLY));
++		}
++	} while (dst_pud++, src_pud++, addr = next, addr != end);
++
++	return 0;
++}
++
++static int copy_page_tables(pgd_t *dst_pgd, unsigned long start,
++			    unsigned long end)
++{
++	unsigned long next;
++	unsigned long addr = start;
++	pgd_t *src_pgd = pgd_offset_k(start);
++
++	dst_pgd = pgd_offset_raw(dst_pgd, start);
++	do {
++		next = pgd_addr_end(addr, end);
++		if (pgd_none(*src_pgd))
++			continue;
++		if (copy_pud(dst_pgd, src_pgd, addr, next))
++			return -ENOMEM;
++	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
++
++	return 0;
++}
++
++/*
++ * Setup then Resume from the hibernate image using swsusp_arch_suspend_exit().
++ *
++ * Memory allocated by get_safe_page() will be dealt with by the hibernate code,
++ * we don't need to free it here.
++ */
++int swsusp_arch_resume(void)
++{
++	int rc = 0;
++	void *zero_page;
++	size_t exit_size;
++	pgd_t *tmp_pg_dir;
++	void *lm_restore_pblist;
++	phys_addr_t phys_hibernate_exit;
++	void __noreturn (*hibernate_exit)(phys_addr_t, phys_addr_t, void *,
++					  void *, phys_addr_t, phys_addr_t);
++
++	/*
++	 * Locate the exit code in the bottom-but-one page, so that *NULL
++	 * still has disastrous affects.
++	 */
++	hibernate_exit = (void *)PAGE_SIZE;
++	exit_size = __hibernate_exit_text_end - __hibernate_exit_text_start;
++	/*
++	 * Copy swsusp_arch_suspend_exit() to a safe page. This will generate
++	 * a new set of ttbr0 page tables and load them.
++	 */
++	rc = create_safe_exec_page(__hibernate_exit_text_start, exit_size,
++				   (unsigned long)hibernate_exit,
++				   &phys_hibernate_exit,
++				   (void *)get_safe_page, GFP_ATOMIC);
++	if (rc) {
++		pr_err("Failed to create safe executable page for hibernate_exit code.");
++		goto out;
++	}
++
++	/*
++	 * The hibernate exit text contains a set of el2 vectors, that will
++	 * be executed at el2 with the mmu off in order to reload hyp-stub.
++	 */
++	__flush_dcache_area(hibernate_exit, exit_size);
++
++	/*
++	 * Restoring the memory image will overwrite the ttbr1 page tables.
++	 * Create a second copy of just the linear map, and use this when
++	 * restoring.
++	 */
++	tmp_pg_dir = (pgd_t *)get_safe_page(GFP_ATOMIC);
++	if (!tmp_pg_dir) {
++		pr_err("Failed to allocate memory for temporary page tables.");
++		rc = -ENOMEM;
++		goto out;
++	}
++	rc = copy_page_tables(tmp_pg_dir, PAGE_OFFSET, 0);
++	if (rc)
++		goto out;
++
++	/*
++	 * Since we only copied the linear map, we need to find restore_pblist's
++	 * linear map address.
++	 */
++	lm_restore_pblist = LMADDR(restore_pblist);
++
++	/*
++	 * KASLR will cause the el2 vectors to be in a different location in
++	 * the resumed kernel. Load hibernate's temporary copy into el2.
++	 *
++	 * We can skip this step if we booted at EL1, or are running with VHE.
++	 */
++	if (el2_reset_needed()) {
++		phys_addr_t el2_vectors = phys_hibernate_exit;  /* base */
++		el2_vectors += hibernate_el2_vectors -
++			       __hibernate_exit_text_start;     /* offset */
++
++		__hyp_set_vectors(el2_vectors);
++	}
++
++	/*
++	 * We need a zero page that is zero before & after resume in order to
++	 * to break before make on the ttbr1 page tables.
++	 */
++	zero_page = (void *)get_safe_page(GFP_ATOMIC);
++
++	hibernate_exit(virt_to_phys(tmp_pg_dir), resume_hdr.ttbr1_el1,
++		       resume_hdr.reenter_kernel, lm_restore_pblist,
++		       resume_hdr.__hyp_stub_vectors, virt_to_phys(zero_page));
++
++out:
++	return rc;
++}
+diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
+index f04c476..e2d63ed 100644
+--- a/arch/arm64/kernel/vmlinux.lds.S
++++ b/arch/arm64/kernel/vmlinux.lds.S
+@@ -46,6 +46,16 @@ jiffies = jiffies_64;
+ 	*(.idmap.text)					\
+ 	VMLINUX_SYMBOL(__idmap_text_end) = .;
+ 
++#ifdef CONFIG_HIBERNATION
++#define HIBERNATE_TEXT					\
++	. = ALIGN(SZ_4K);				\
++	VMLINUX_SYMBOL(__hibernate_exit_text_start) = .;\
++	*(.hibernate_exit.text)				\
++	VMLINUX_SYMBOL(__hibernate_exit_text_end) = .;
++#else
++#define HIBERNATE_TEXT
++#endif
++
+ /*
+  * The size of the PE/COFF section that covers the kernel image, which
+  * runs from stext to _edata, must be a round multiple of the PE/COFF
+@@ -109,6 +119,7 @@ SECTIONS
+ 			KPROBES_TEXT
+ 			HYPERVISOR_TEXT
+ 			IDMAP_TEXT
++			HIBERNATE_TEXT
+ 			*(.fixup)
+ 			*(.gnu.warning)
+ 		. = ALIGN(16);
+@@ -187,6 +198,10 @@ ASSERT(__hyp_idmap_text_end - (__hyp_idmap_text_start & ~(SZ_4K - 1)) <= SZ_4K,
+ 	"HYP init code too big or misaligned")
+ ASSERT(__idmap_text_end - (__idmap_text_start & ~(SZ_4K - 1)) <= SZ_4K,
+ 	"ID map text too big or misaligned")
++#ifdef CONFIG_HIBERNATION
++ASSERT(__hibernate_exit_text_end - (__hibernate_exit_text_start & ~(SZ_4K - 1))
++	<= SZ_4K, "Hibernate exit text too big or misaligned")
++#endif
+ 
+ /*
+  * If padding is applied before .head.text, virt<->phys conversions will fail.
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0078-PM-sleep-Add-support-for-read-only-sysfs-attributes.patch b/tools/kdump/0078-PM-sleep-Add-support-for-read-only-sysfs-attributes.patch
new file mode 100644
index 0000000..9a12325
--- /dev/null
+++ b/tools/kdump/0078-PM-sleep-Add-support-for-read-only-sysfs-attributes.patch
@@ -0,0 +1,77 @@
+From 4b1f02b38764c091d0887031d9d311e988a70f92 Mon Sep 17 00:00:00 2001
+From: "Rafael J. Wysocki" <rafael.j.wysocki@intel.com>
+Date: Sat, 2 Jan 2016 03:09:16 +0100
+Subject: [PATCH 078/123] PM / sleep: Add support for read-only sysfs
+ attributes
+
+Some sysfs attributes in /sys/power/ should really be read-only,
+so add support for that, convert those attributes to read-only
+and drop the stub .show() routines from them.
+
+Original-by: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
+Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
+(cherry picked from commit a1e9ca6967d68209c70e616a224efa89a6b86ca6)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ kernel/power/main.c  | 17 ++---------------
+ kernel/power/power.h |  9 +++++++++
+ 2 files changed, 11 insertions(+), 15 deletions(-)
+
+diff --git a/kernel/power/main.c b/kernel/power/main.c
+index b2dd4d9..2794697 100644
+--- a/kernel/power/main.c
++++ b/kernel/power/main.c
+@@ -280,13 +280,7 @@ static ssize_t pm_wakeup_irq_show(struct kobject *kobj,
+ 	return pm_wakeup_irq ? sprintf(buf, "%u\n", pm_wakeup_irq) : -ENODATA;
+ }
+ 
+-static ssize_t pm_wakeup_irq_store(struct kobject *kobj,
+-					struct kobj_attribute *attr,
+-					const char *buf, size_t n)
+-{
+-	return -EINVAL;
+-}
+-power_attr(pm_wakeup_irq);
++power_attr_ro(pm_wakeup_irq);
+ 
+ #else /* !CONFIG_PM_SLEEP_DEBUG */
+ static inline void pm_print_times_init(void) {}
+@@ -564,14 +558,7 @@ static ssize_t pm_trace_dev_match_show(struct kobject *kobj,
+ 	return show_trace_dev_match(buf, PAGE_SIZE);
+ }
+ 
+-static ssize_t
+-pm_trace_dev_match_store(struct kobject *kobj, struct kobj_attribute *attr,
+-			 const char *buf, size_t n)
+-{
+-	return -EINVAL;
+-}
+-
+-power_attr(pm_trace_dev_match);
++power_attr_ro(pm_trace_dev_match);
+ 
+ #endif /* CONFIG_PM_TRACE */
+ 
+diff --git a/kernel/power/power.h b/kernel/power/power.h
+index caadb56..efe1b3b 100644
+--- a/kernel/power/power.h
++++ b/kernel/power/power.h
+@@ -77,6 +77,15 @@ static struct kobj_attribute _name##_attr = {	\
+ 	.store	= _name##_store,		\
+ }
+ 
++#define power_attr_ro(_name) \
++static struct kobj_attribute _name##_attr = {	\
++	.attr	= {				\
++		.name = __stringify(_name),	\
++		.mode = S_IRUGO,		\
++	},					\
++	.show	= _name##_show,			\
++}
++
+ /* Preferred image size in bytes (default 500 MB) */
+ extern unsigned long image_size;
+ /* Size of memory reserved for drivers (default SPARE_PAGES x PAGE_SIZE) */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0079-arm64-hibernate-Refuse-to-hibernate-if-the-boot-cpu-.patch b/tools/kdump/0079-arm64-hibernate-Refuse-to-hibernate-if-the-boot-cpu-.patch
new file mode 100644
index 0000000..12f9f81
--- /dev/null
+++ b/tools/kdump/0079-arm64-hibernate-Refuse-to-hibernate-if-the-boot-cpu-.patch
@@ -0,0 +1,79 @@
+From 6d9215745a831a590fdbca32e94ad50b9ecfb6c0 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:13 +0100
+Subject: [PATCH 079/123] arm64: hibernate: Refuse to hibernate if the boot cpu
+ is offline
+
+Hibernation represents a system state save/restore through
+a system reboot; this implies that the logical cpus carrying
+out hibernation/thawing must be the same, so that the context
+saved in the snapshot image on hibernation is consistent with
+the state of the system on resume. If resume from hibernation
+is driven through kernel command line parameter, the cpu responsible
+for thawing the system will be whatever CPU firmware boots the system
+on upon cold-boot (ie logical cpu 0); this means that in order to
+keep system context consistent between the hibernate snapshot image
+and system state on kernel resume from hibernate, logical cpu 0 must
+be online on hibernation and must be the logical cpu that creates
+the snapshot image.
+
+This patch adds a PM notifier that enforces logical cpu 0 is online
+when the hibernation is started (and prevents hibernation if it is
+not), which is sufficient to guarantee it will be the one creating
+the snapshot image therefore providing the resume cpu a consistent
+snapshot of the system to resume to.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Acked-by: Catalin Marinas <catalin.marinas@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 1fe492ce6482b77807b25d29690a48c46456beee)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kernel/hibernate.c | 26 ++++++++++++++++++++++++++
+ 1 file changed, 26 insertions(+)
+
+diff --git a/arch/arm64/kernel/hibernate.c b/arch/arm64/kernel/hibernate.c
+index 7e16fb3..f8df75d 100644
+--- a/arch/arm64/kernel/hibernate.c
++++ b/arch/arm64/kernel/hibernate.c
+@@ -17,6 +17,7 @@
+ #define pr_fmt(x) "hibernate: " x
+ #include <linux/kvm_host.h>
+ #include <linux/mm.h>
++#include <linux/notifier.h>
+ #include <linux/pm.h>
+ #include <linux/sched.h>
+ #include <linux/suspend.h>
+@@ -459,3 +460,28 @@ int swsusp_arch_resume(void)
+ out:
+ 	return rc;
+ }
++
++static int check_boot_cpu_online_pm_callback(struct notifier_block *nb,
++					     unsigned long action, void *ptr)
++{
++	if (action == PM_HIBERNATION_PREPARE &&
++	     cpumask_first(cpu_online_mask) != 0) {
++		pr_warn("CPU0 is offline.\n");
++		return notifier_from_errno(-ENODEV);
++	}
++
++	return NOTIFY_OK;
++}
++
++static int __init check_boot_cpu_online_init(void)
++{
++	/*
++	 * Set this pm_notifier callback with a lower priority than
++	 * cpu_hotplug_pm_callback, so that cpu_hotplug_pm_callback will be
++	 * called earlier to disable cpu hotplug before the cpu online check.
++	 */
++	pm_notifier(check_boot_cpu_online_pm_callback, -INT_MAX);
++
++	return 0;
++}
++core_initcall(check_boot_cpu_online_init);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0080-arm64-mm-avoid-redundant-__pa-__va-x.patch b/tools/kdump/0080-arm64-mm-avoid-redundant-__pa-__va-x.patch
new file mode 100644
index 0000000..f49664a
--- /dev/null
+++ b/tools/kdump/0080-arm64-mm-avoid-redundant-__pa-__va-x.patch
@@ -0,0 +1,56 @@
+From 97434b7f8c4af6b8a337dd40edfc75b38e2cb7cf Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:05 +0000
+Subject: [PATCH 080/123] arm64: mm: avoid redundant __pa(__va(x))
+
+When we "upgrade" to a section mapping, we free any table we made
+redundant by giving it back to memblock. To get the PA, we acquire the
+physical address and convert this to a VA, then subsequently convert
+this back to a PA.
+
+This works currently, but will not work if the tables are not accessed
+via linear map VAs (e.g. is we use fixmap slots).
+
+This patch uses {pmd,pud}_page_paddr to acquire the PA. This avoids the
+__pa(__va()) round trip, saving some work and avoiding reliance on the
+linear mapping.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 316b39db06718d59d82736df9fc65cf05b467cc7)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 4 ++--
+ 1 file changed, 2 insertions(+), 2 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 1e2ae80..e1240cf 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -171,7 +171,7 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 			if (!pmd_none(old_pmd)) {
+ 				flush_tlb_all();
+ 				if (pmd_table(old_pmd)) {
+-					phys_addr_t table = __pa(pte_offset_map(&old_pmd, 0));
++					phys_addr_t table = pmd_page_paddr(old_pmd);
+ 					if (!WARN_ON_ONCE(slab_is_available()))
+ 						memblock_free(table, PAGE_SIZE);
+ 				}
+@@ -232,7 +232,7 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ 			if (!pud_none(old_pud)) {
+ 				flush_tlb_all();
+ 				if (pud_table(old_pud)) {
+-					phys_addr_t table = __pa(pmd_offset(&old_pud, 0));
++					phys_addr_t table = pud_page_paddr(old_pud);
+ 					if (!WARN_ON_ONCE(slab_is_available()))
+ 						memblock_free(table, PAGE_SIZE);
+ 				}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0081-arm64-mm-add-__-pud-pgd-_populate.patch b/tools/kdump/0081-arm64-mm-add-__-pud-pgd-_populate.patch
new file mode 100644
index 0000000..b012d64
--- /dev/null
+++ b/tools/kdump/0081-arm64-mm-add-__-pud-pgd-_populate.patch
@@ -0,0 +1,86 @@
+From 18b08b0f0b374a6e2ec1480759917e484115ecf1 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:06 +0000
+Subject: [PATCH 081/123] arm64: mm: add __{pud,pgd}_populate
+
+We currently have __pmd_populate for creating a pmd table entry given
+the physical address of a pte, but don't have equivalents for the pud or
+pgd levels of table.
+
+To enable us to manipulate tables which are mapped outside of the linear
+mapping (where we have a PA, but not a linear map VA), it is useful to
+have these functions.
+
+This patch adds __{pud,pgd}_populate. As these should not be called when
+the kernel uses folded {pmd,pud}s, in these cases they expand to
+BUILD_BUG(). So long as the appropriate checks are made on the {pud,pgd}
+entry prior to attempting population, these should be optimized out at
+compile time.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 1e531cce68c92b46c7d29f36a72f9a3e5886678f)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/pgalloc.h | 26 ++++++++++++++++++++++----
+ 1 file changed, 22 insertions(+), 4 deletions(-)
+
+diff --git a/arch/arm64/include/asm/pgalloc.h b/arch/arm64/include/asm/pgalloc.h
+index c150539..ff98585 100644
+--- a/arch/arm64/include/asm/pgalloc.h
++++ b/arch/arm64/include/asm/pgalloc.h
+@@ -42,11 +42,20 @@ static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)
+ 	free_page((unsigned long)pmd);
+ }
+ 
+-static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
++static inline void __pud_populate(pud_t *pud, phys_addr_t pmd, pudval_t prot)
+ {
+-	set_pud(pud, __pud(__pa(pmd) | PMD_TYPE_TABLE));
++	set_pud(pud, __pud(pmd | prot));
+ }
+ 
++static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
++{
++	__pud_populate(pud, __pa(pmd), PMD_TYPE_TABLE);
++}
++#else
++static inline void __pud_populate(pud_t *pud, phys_addr_t pmd, pudval_t prot)
++{
++	BUILD_BUG();
++}
+ #endif	/* CONFIG_PGTABLE_LEVELS > 2 */
+ 
+ #if CONFIG_PGTABLE_LEVELS > 3
+@@ -62,11 +71,20 @@ static inline void pud_free(struct mm_struct *mm, pud_t *pud)
+ 	free_page((unsigned long)pud);
+ }
+ 
+-static inline void pgd_populate(struct mm_struct *mm, pgd_t *pgd, pud_t *pud)
++static inline void __pgd_populate(pgd_t *pgdp, phys_addr_t pud, pgdval_t prot)
+ {
+-	set_pgd(pgd, __pgd(__pa(pud) | PUD_TYPE_TABLE));
++	set_pgd(pgdp, __pgd(pud | prot));
+ }
+ 
++static inline void pgd_populate(struct mm_struct *mm, pgd_t *pgd, pud_t *pud)
++{
++	__pgd_populate(pgd, __pa(pud), PUD_TYPE_TABLE);
++}
++#else
++static inline void __pgd_populate(pgd_t *pgdp, phys_addr_t pud, pgdval_t prot)
++{
++	BUILD_BUG();
++}
+ #endif	/* CONFIG_PGTABLE_LEVELS > 3 */
+ 
+ extern pgd_t *pgd_alloc(struct mm_struct *mm);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0082-arm64-mm-add-functions-to-walk-tables-in-fixmap.patch b/tools/kdump/0082-arm64-mm-add-functions-to-walk-tables-in-fixmap.patch
new file mode 100644
index 0000000..d9a2d67
--- /dev/null
+++ b/tools/kdump/0082-arm64-mm-add-functions-to-walk-tables-in-fixmap.patch
@@ -0,0 +1,126 @@
+From 61f1f7a0261d152740465042906a3578988def52 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:07 +0000
+Subject: [PATCH 082/123] arm64: mm: add functions to walk tables in fixmap
+
+As a preparatory step to allow us to allocate early page tables from
+unmapped memory using memblock_alloc, add new p??_{set,clear}_fixmap*
+functions which can be used to walk page tables outside of the linear
+mapping by using fixmap slots.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 961faac114819a01e627fe9c9c82b830bb3849d4)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/fixmap.h  | 10 ++++++++++
+ arch/arm64/include/asm/pgtable.h | 26 ++++++++++++++++++++++++++
+ 2 files changed, 36 insertions(+)
+
+diff --git a/arch/arm64/include/asm/fixmap.h b/arch/arm64/include/asm/fixmap.h
+index 30970454..1a617d4 100644
+--- a/arch/arm64/include/asm/fixmap.h
++++ b/arch/arm64/include/asm/fixmap.h
+@@ -62,6 +62,16 @@ enum fixed_addresses {
+ 
+ 	FIX_BTMAP_END = __end_of_permanent_fixed_addresses,
+ 	FIX_BTMAP_BEGIN = FIX_BTMAP_END + TOTAL_FIX_BTMAPS - 1,
++
++	/*
++	 * Used for kernel page table creation, so unmapped memory may be used
++	 * for tables.
++	 */
++	FIX_PTE,
++	FIX_PMD,
++	FIX_PUD,
++	FIX_PGD,
++
+ 	__end_of_fixed_addresses
+ };
+ 
+diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
+index 774854e3..50341f0 100644
+--- a/arch/arm64/include/asm/pgtable.h
++++ b/arch/arm64/include/asm/pgtable.h
+@@ -59,6 +59,7 @@
+ 
+ #ifndef __ASSEMBLY__
+ 
++#include <asm/fixmap.h>
+ #include <linux/mmdebug.h>
+ 
+ extern void __pte_error(const char *file, int line, unsigned long val);
+@@ -431,6 +432,10 @@ static inline phys_addr_t pmd_page_paddr(pmd_t pmd)
+ #define pte_unmap(pte)			do { } while (0)
+ #define pte_unmap_nested(pte)		do { } while (0)
+ 
++#define pte_set_fixmap(addr)		((pte_t *)set_fixmap_offset(FIX_PTE, addr))
++#define pte_set_fixmap_offset(pmd, addr)	pte_set_fixmap(pte_offset_phys(pmd, addr))
++#define pte_clear_fixmap()		clear_fixmap(FIX_PTE)
++
+ #define pmd_page(pmd)		pfn_to_page(__phys_to_pfn(pmd_val(pmd) & PHYS_MASK))
+ 
+ /*
+@@ -470,12 +475,21 @@ static inline phys_addr_t pud_page_paddr(pud_t pud)
+ #define pmd_offset_phys(dir, addr)	(pud_page_paddr(*(dir)) + pmd_index(addr) * sizeof(pmd_t))
+ #define pmd_offset(dir, addr)		((pmd_t *)__va(pmd_offset_phys((dir), (addr))))
+ 
++#define pmd_set_fixmap(addr)		((pmd_t *)set_fixmap_offset(FIX_PMD, addr))
++#define pmd_set_fixmap_offset(pud, addr)	pmd_set_fixmap(pmd_offset_phys(pud, addr))
++#define pmd_clear_fixmap()		clear_fixmap(FIX_PMD)
++
+ #define pud_page(pud)		pfn_to_page(__phys_to_pfn(pud_val(pud) & PHYS_MASK))
+ 
+ #else
+ 
+ #define pud_page_paddr(pud)	({ BUILD_BUG(); 0; })
+ 
++/* Match pmd_offset folding in <asm/generic/pgtable-nopmd.h> */
++#define pmd_set_fixmap(addr)		NULL
++#define pmd_set_fixmap_offset(pudp, addr)	((pmd_t *)pudp)
++#define pmd_clear_fixmap()
++
+ #endif	/* CONFIG_PGTABLE_LEVELS > 2 */
+ 
+ #if CONFIG_PGTABLE_LEVELS > 3
+@@ -508,12 +522,21 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
+ #define pud_offset_phys(dir, addr)	(pgd_page_paddr(*(dir)) + pud_index(addr) * sizeof(pud_t))
+ #define pud_offset(dir, addr)		((pud_t *)__va(pud_offset_phys((dir), (addr))))
+ 
++#define pud_set_fixmap(addr)		((pud_t *)set_fixmap_offset(FIX_PUD, addr))
++#define pud_set_fixmap_offset(pgd, addr)	pud_set_fixmap(pud_offset_phys(pgd, addr))
++#define pud_clear_fixmap()		clear_fixmap(FIX_PUD)
++
+ #define pgd_page(pgd)		pfn_to_page(__phys_to_pfn(pgd_val(pgd) & PHYS_MASK))
+ 
+ #else
+ 
+ #define pgd_page_paddr(pgd)	({ BUILD_BUG(); 0;})
+ 
++/* Match pud_offset folding in <asm/generic/pgtable-nopud.h> */
++#define pud_set_fixmap(addr)		NULL
++#define pud_set_fixmap_offset(pgdp, addr)	((pud_t *)pgdp)
++#define pud_clear_fixmap()
++
+ #endif  /* CONFIG_PGTABLE_LEVELS > 3 */
+ 
+ #define pgd_ERROR(pgd)		__pgd_error(__FILE__, __LINE__, pgd_val(pgd))
+@@ -528,6 +551,9 @@ static inline phys_addr_t pgd_page_paddr(pgd_t pgd)
+ /* to find an entry in a kernel page-table-directory */
+ #define pgd_offset_k(addr)	pgd_offset(&init_mm, addr)
+ 
++#define pgd_set_fixmap(addr)	((pgd_t *)set_fixmap_offset(FIX_PGD, addr))
++#define pgd_clear_fixmap()	clear_fixmap(FIX_PGD)
++
+ static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+ {
+ 	const pteval_t mask = PTE_USER | PTE_PXN | PTE_UXN | PTE_RDONLY |
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0083-arm64-mm-use-fixmap-when-creating-page-tables.patch b/tools/kdump/0083-arm64-mm-use-fixmap-when-creating-page-tables.patch
new file mode 100644
index 0000000..79898f5
--- /dev/null
+++ b/tools/kdump/0083-arm64-mm-use-fixmap-when-creating-page-tables.patch
@@ -0,0 +1,205 @@
+From 5648523071cb92c9d3243c12bef258195e65905a Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:08 +0000
+Subject: [PATCH 083/123] arm64: mm: use fixmap when creating page tables
+
+As a preparatory step to allow us to allocate early page tables from
+unmapped memory using memblock_alloc, modify the __create_mapping
+callees to map and unmap the tables they modify using fixmap entries.
+
+All but the top-level pgd initialisation is performed via the fixmap.
+Subsequent patches will inject the pgd physical address, and migrate to
+using the FIX_PGD slot.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit f4710445458c0a1bd1c3c014ada2e7d7dc7b882f)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 61 +++++++++++++++++++++++++++++++++++------------------
+ 1 file changed, 41 insertions(+), 20 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index e1240cf..36203c2 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -63,19 +63,30 @@ pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
+ }
+ EXPORT_SYMBOL(phys_mem_access_prot);
+ 
+-static void __init *early_pgtable_alloc(void)
++static phys_addr_t __init early_pgtable_alloc(void)
+ {
+ 	phys_addr_t phys;
+ 	void *ptr;
+ 
+ 	phys = memblock_alloc(PAGE_SIZE, PAGE_SIZE);
+ 	BUG_ON(!phys);
+-	ptr = __va(phys);
++
++	/*
++	 * The FIX_{PGD,PUD,PMD} slots may be in active use, but the FIX_PTE
++	 * slot will be free, so we can (ab)use the FIX_PTE slot to initialise
++	 * any level of table.
++	 */
++	ptr = pte_set_fixmap(phys);
++
+ 	memset(ptr, 0, PAGE_SIZE);
+ 
+-	/* Ensure the zeroed page is visible to the page table walker */
+-	dsb(ishst);
+-	return ptr;
++	/*
++	 * Implicit barriers also ensure the zeroed page is visible to the page
++	 * table walker
++	 */
++	pte_clear_fixmap();
++
++	return phys;
+ }
+ 
+ /*
+@@ -99,24 +110,28 @@ static void split_pmd(pmd_t *pmd, pte_t *pte)
+ static void alloc_init_pte(pmd_t *pmd, unsigned long addr,
+ 				  unsigned long end, unsigned long pfn,
+ 				  pgprot_t prot,
+-				  void *(*pgtable_alloc)(void))
++				  phys_addr_t (*pgtable_alloc)(void))
+ {
+ 	pte_t *pte;
+ 
+ 	if (pmd_none(*pmd) || pmd_sect(*pmd)) {
+-		pte = pgtable_alloc();
++		phys_addr_t pte_phys = pgtable_alloc();
++		pte = pte_set_fixmap(pte_phys);
+ 		if (pmd_sect(*pmd))
+ 			split_pmd(pmd, pte);
+-		__pmd_populate(pmd, __pa(pte), PMD_TYPE_TABLE);
++		__pmd_populate(pmd, pte_phys, PMD_TYPE_TABLE);
+ 		flush_tlb_all();
++		pte_clear_fixmap();
+ 	}
+ 	BUG_ON(pmd_bad(*pmd));
+ 
+-	pte = pte_offset_kernel(pmd, addr);
++	pte = pte_set_fixmap_offset(pmd, addr);
+ 	do {
+ 		set_pte(pte, pfn_pte(pfn, prot));
+ 		pfn++;
+ 	} while (pte++, addr += PAGE_SIZE, addr != end);
++
++	pte_clear_fixmap();
+ }
+ 
+ static void split_pud(pud_t *old_pud, pmd_t *pmd)
+@@ -134,7 +149,7 @@ static void split_pud(pud_t *old_pud, pmd_t *pmd)
+ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 				  unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+-				  void *(*pgtable_alloc)(void))
++				  phys_addr_t (*pgtable_alloc)(void))
+ {
+ 	pmd_t *pmd;
+ 	unsigned long next;
+@@ -143,7 +158,8 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 	 * Check for initial section mappings in the pgd/pud and remove them.
+ 	 */
+ 	if (pud_none(*pud) || pud_sect(*pud)) {
+-		pmd = pgtable_alloc();
++		phys_addr_t pmd_phys = pgtable_alloc();
++		pmd = pmd_set_fixmap(pmd_phys);
+ 		if (pud_sect(*pud)) {
+ 			/*
+ 			 * need to have the 1G of mappings continue to be
+@@ -151,12 +167,13 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 			 */
+ 			split_pud(pud, pmd);
+ 		}
+-		pud_populate(mm, pud, pmd);
++		__pud_populate(pud, pmd_phys, PUD_TYPE_TABLE);
+ 		flush_tlb_all();
++		pmd_clear_fixmap();
+ 	}
+ 	BUG_ON(pud_bad(*pud));
+ 
+-	pmd = pmd_offset(pud, addr);
++	pmd = pmd_set_fixmap_offset(pud, addr);
+ 	do {
+ 		next = pmd_addr_end(addr, end);
+ 		/* try section mapping first */
+@@ -182,6 +199,8 @@ static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+ 		}
+ 		phys += next - addr;
+ 	} while (pmd++, addr = next, addr != end);
++
++	pmd_clear_fixmap();
+ }
+ 
+ static inline bool use_1G_block(unsigned long addr, unsigned long next,
+@@ -199,18 +218,18 @@ static inline bool use_1G_block(unsigned long addr, unsigned long next,
+ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ 				  unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+-				  void *(*pgtable_alloc)(void))
++				  phys_addr_t (*pgtable_alloc)(void))
+ {
+ 	pud_t *pud;
+ 	unsigned long next;
+ 
+ 	if (pgd_none(*pgd)) {
+-		pud = pgtable_alloc();
+-		pgd_populate(mm, pgd, pud);
++		phys_addr_t pud_phys = pgtable_alloc();
++		__pgd_populate(pgd, pud_phys, PUD_TYPE_TABLE);
+ 	}
+ 	BUG_ON(pgd_bad(*pgd));
+ 
+-	pud = pud_offset(pgd, addr);
++	pud = pud_set_fixmap_offset(pgd, addr);
+ 	do {
+ 		next = pud_addr_end(addr, end);
+ 
+@@ -243,6 +262,8 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ 		}
+ 		phys += next - addr;
+ 	} while (pud++, addr = next, addr != end);
++
++	pud_clear_fixmap();
+ }
+ 
+ /*
+@@ -252,7 +273,7 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ static void  __create_mapping(struct mm_struct *mm, pgd_t *pgd,
+ 				    phys_addr_t phys, unsigned long virt,
+ 				    phys_addr_t size, pgprot_t prot,
+-				    void *(*pgtable_alloc)(void))
++				    phys_addr_t (*pgtable_alloc)(void))
+ {
+ 	unsigned long addr, length, end, next;
+ 
+@@ -267,14 +288,14 @@ static void  __create_mapping(struct mm_struct *mm, pgd_t *pgd,
+ 	} while (pgd++, addr = next, addr != end);
+ }
+ 
+-static void *late_pgtable_alloc(void)
++static phys_addr_t late_pgtable_alloc(void)
+ {
+ 	void *ptr = (void *)__get_free_page(PGALLOC_GFP);
+ 	BUG_ON(!ptr);
+ 
+ 	/* Ensure the zeroed page is visible to the page table walker */
+ 	dsb(ishst);
+-	return ptr;
++	return __pa(ptr);
+ }
+ 
+ static void __init create_mapping(phys_addr_t phys, unsigned long virt,
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0084-arm64-mm-allocate-pagetables-anywhere.patch b/tools/kdump/0084-arm64-mm-allocate-pagetables-anywhere.patch
new file mode 100644
index 0000000..1382936
--- /dev/null
+++ b/tools/kdump/0084-arm64-mm-allocate-pagetables-anywhere.patch
@@ -0,0 +1,89 @@
+From 2c9ec1754e50a1f8131dd8f37cb12fbc6f6859a6 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:09 +0000
+Subject: [PATCH 084/123] arm64: mm: allocate pagetables anywhere
+
+Now that create_mapping uses fixmap slots to modify pte, pmd, and pud
+entries, we can access page tables anywhere in physical memory,
+regardless of the extent of the linear mapping.
+
+Given that, we no longer need to limit memblock allocations during page
+table creation, and can leave the limit as its default
+MEMBLOCK_ALLOC_ANYWHERE.
+
+We never add memory which will fall outside of the linear map range
+given phys_offset and MAX_MEMBLOCK_ADDR are configured appropriately, so
+any tables we create will fall in the linear map of the final tables.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit cdef5f6e9e0e5ee397759b664a9f875ff59ccf01)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 35 -----------------------------------
+ 1 file changed, 35 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 36203c2..dc6665f 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -376,20 +376,6 @@ static void __init __map_memblock(phys_addr_t start, phys_addr_t end)
+ static void __init map_mem(void)
+ {
+ 	struct memblock_region *reg;
+-	phys_addr_t limit;
+-
+-	/*
+-	 * Temporarily limit the memblock range. We need to do this as
+-	 * create_mapping requires puds, pmds and ptes to be allocated from
+-	 * memory addressable from the initial direct kernel mapping.
+-	 *
+-	 * The initial direct kernel mapping, located at swapper_pg_dir, gives
+-	 * us PUD_SIZE (with SECTION maps) or PMD_SIZE (without SECTION maps,
+-	 * memory starting from PHYS_OFFSET (which must be aligned to 2MB as
+-	 * per Documentation/arm64/booting.txt).
+-	 */
+-	limit = PHYS_OFFSET + SWAPPER_INIT_MAP_SIZE;
+-	memblock_set_current_limit(limit);
+ 
+ 	/* map all the memory banks */
+ 	for_each_memblock(memory, reg) {
+@@ -399,29 +385,8 @@ static void __init map_mem(void)
+ 		if (start >= end)
+ 			break;
+ 
+-		if (ARM64_SWAPPER_USES_SECTION_MAPS) {
+-			/*
+-			 * For the first memory bank align the start address and
+-			 * current memblock limit to prevent create_mapping() from
+-			 * allocating pte page tables from unmapped memory. With
+-			 * the section maps, if the first block doesn't end on section
+-			 * size boundary, create_mapping() will try to allocate a pte
+-			 * page, which may be returned from an unmapped area.
+-			 * When section maps are not used, the pte page table for the
+-			 * current limit is already present in swapper_pg_dir.
+-			 */
+-			if (start < limit)
+-				start = ALIGN(start, SECTION_SIZE);
+-			if (end < limit) {
+-				limit = end & SECTION_MASK;
+-				memblock_set_current_limit(limit);
+-			}
+-		}
+ 		__map_memblock(start, end);
+ 	}
+-
+-	/* Limit no longer required. */
+-	memblock_set_current_limit(MEMBLOCK_ALLOC_ANYWHERE);
+ }
+ 
+ static void __init fixup_executable(void)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0085-arm64-mm-allow-passing-a-pgdir-to-alloc_init_.patch b/tools/kdump/0085-arm64-mm-allow-passing-a-pgdir-to-alloc_init_.patch
new file mode 100644
index 0000000..63233b3
--- /dev/null
+++ b/tools/kdump/0085-arm64-mm-allow-passing-a-pgdir-to-alloc_init_.patch
@@ -0,0 +1,131 @@
+From eafe89143983974574fd45207cc42c3f784494ae Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:10 +0000
+Subject: [PATCH 085/123] arm64: mm: allow passing a pgdir to alloc_init_*
+
+To allow us to initialise pgdirs which are fixmapped, allow explicitly
+passing a pgdir rather than an mm. A new __create_pgd_mapping function
+is added for this, with existing __create_mapping callers migrated to
+this.
+
+The mm argument was previously only used at the top level. Now that it
+is redundant at all levels, it is removed. To indicate its new found
+similarity to alloc_init_{pud,pmd,pte}, __create_mapping is renamed to
+init_pgd.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 11509a306bb6ea595878b2d246d2d56b1783e040)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 33 +++++++++++++++++++--------------
+ 1 file changed, 19 insertions(+), 14 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index dc6665f..fd4c95b 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -146,8 +146,7 @@ static void split_pud(pud_t *old_pud, pmd_t *pmd)
+ 	} while (pmd++, i++, i < PTRS_PER_PMD);
+ }
+ 
+-static void alloc_init_pmd(struct mm_struct *mm, pud_t *pud,
+-				  unsigned long addr, unsigned long end,
++static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+ 				  phys_addr_t (*pgtable_alloc)(void))
+ {
+@@ -215,8 +214,7 @@ static inline bool use_1G_block(unsigned long addr, unsigned long next,
+ 	return true;
+ }
+ 
+-static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+-				  unsigned long addr, unsigned long end,
++static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+ 				  phys_addr_t (*pgtable_alloc)(void))
+ {
+@@ -257,7 +255,7 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+ 				}
+ 			}
+ 		} else {
+-			alloc_init_pmd(mm, pud, addr, next, phys, prot,
++			alloc_init_pmd(pud, addr, next, phys, prot,
+ 				       pgtable_alloc);
+ 		}
+ 		phys += next - addr;
+@@ -270,8 +268,7 @@ static void alloc_init_pud(struct mm_struct *mm, pgd_t *pgd,
+  * Create the page directory entries and any necessary page tables for the
+  * mapping specified by 'md'.
+  */
+-static void  __create_mapping(struct mm_struct *mm, pgd_t *pgd,
+-				    phys_addr_t phys, unsigned long virt,
++static void init_pgd(pgd_t *pgd, phys_addr_t phys, unsigned long virt,
+ 				    phys_addr_t size, pgprot_t prot,
+ 				    phys_addr_t (*pgtable_alloc)(void))
+ {
+@@ -283,7 +280,7 @@ static void  __create_mapping(struct mm_struct *mm, pgd_t *pgd,
+ 	end = addr + length;
+ 	do {
+ 		next = pgd_addr_end(addr, end);
+-		alloc_init_pud(mm, pgd, addr, next, phys, prot, pgtable_alloc);
++		alloc_init_pud(pgd, addr, next, phys, prot, pgtable_alloc);
+ 		phys += next - addr;
+ 	} while (pgd++, addr = next, addr != end);
+ }
+@@ -298,6 +295,14 @@ static phys_addr_t late_pgtable_alloc(void)
+ 	return __pa(ptr);
+ }
+ 
++static void __create_pgd_mapping(pgd_t *pgdir, phys_addr_t phys,
++				 unsigned long virt, phys_addr_t size,
++				 pgprot_t prot,
++				 phys_addr_t (*alloc)(void))
++{
++	init_pgd(pgd_offset_raw(pgdir, virt), phys, virt, size, prot, alloc);
++}
++
+ static void __init create_mapping(phys_addr_t phys, unsigned long virt,
+ 				  phys_addr_t size, pgprot_t prot)
+ {
+@@ -306,16 +311,16 @@ static void __init create_mapping(phys_addr_t phys, unsigned long virt,
+ 			&phys, virt);
+ 		return;
+ 	}
+-	__create_mapping(&init_mm, pgd_offset_k(virt), phys, virt,
+-			 size, prot, early_pgtable_alloc);
++	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot,
++			     early_pgtable_alloc);
+ }
+ 
+ void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
+ 			       unsigned long virt, phys_addr_t size,
+ 			       pgprot_t prot)
+ {
+-	__create_mapping(mm, pgd_offset(mm, virt), phys, virt, size, prot,
+-				late_pgtable_alloc);
++	__create_pgd_mapping(mm->pgd, phys, virt, size, prot,
++			     late_pgtable_alloc);
+ }
+ 
+ static void create_mapping_late(phys_addr_t phys, unsigned long virt,
+@@ -327,8 +332,8 @@ static void create_mapping_late(phys_addr_t phys, unsigned long virt,
+ 		return;
+ 	}
+ 
+-	return __create_mapping(&init_mm, pgd_offset_k(virt),
+-				phys, virt, size, prot, late_pgtable_alloc);
++	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot,
++			     late_pgtable_alloc);
+ }
+ 
+ #ifdef CONFIG_DEBUG_RODATA
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0086-arm64-ensure-_stext-and-_etext-are-page-aligned.patch b/tools/kdump/0086-arm64-ensure-_stext-and-_etext-are-page-aligned.patch
new file mode 100644
index 0000000..d14e57c
--- /dev/null
+++ b/tools/kdump/0086-arm64-ensure-_stext-and-_etext-are-page-aligned.patch
@@ -0,0 +1,59 @@
+From 2a67376001bd871f5a5b91a10dd4c3735cd16a61 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:11 +0000
+Subject: [PATCH 086/123] arm64: ensure _stext and _etext are page-aligned
+
+Currently we have separate ALIGN_DEBUG_RO{,_MIN} directives to align
+_etext and __init_begin. While we ensure that __init_begin is
+page-aligned, we do not provide the same guarantee for _etext. This is
+not problematic currently as the alignment of __init_begin is sufficient
+to prevent issues when we modify permissions.
+
+Subsequent patches will assume page alignment of segments of the kernel
+we wish to map with different permissions. To ensure this, move _etext
+after the ALIGN_DEBUG_RO_MIN for the init section. This renders the
+prior ALIGN_DEBUG_RO irrelevant, and hence it is removed. Likewise,
+upgrade to ALIGN_DEBUG_RO_MIN(PAGE_SIZE) for _stext.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit fca082bfb543ccaaff864fc0892379ccaa1711cd)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kernel/vmlinux.lds.S | 5 ++---
+ 1 file changed, 2 insertions(+), 3 deletions(-)
+
+diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
+index e2d63ed..fa2fc07 100644
+--- a/arch/arm64/kernel/vmlinux.lds.S
++++ b/arch/arm64/kernel/vmlinux.lds.S
+@@ -105,7 +105,7 @@ SECTIONS
+ 		_text = .;
+ 		HEAD_TEXT
+ 	}
+-	ALIGN_DEBUG_RO
++	ALIGN_DEBUG_RO_MIN(PAGE_SIZE)
+ 	.text : {			/* Real text segment		*/
+ 		_stext = .;		/* Text and read-only data	*/
+ 			__exception_text_start = .;
+@@ -130,10 +130,9 @@ SECTIONS
+ 	RO_DATA(PAGE_SIZE)
+ 	EXCEPTION_TABLE(8)
+ 	NOTES
+-	ALIGN_DEBUG_RO
+-	_etext = .;			/* End of text and rodata section */
+ 
+ 	ALIGN_DEBUG_RO_MIN(PAGE_SIZE)
++	_etext = .;			/* End of text and rodata section */
+ 	__init_begin = .;
+ 
+ 	INIT_TEXT_SECTION(8)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0087-arm64-mm-create-new-fine-grained-mappings-at-boot.patch b/tools/kdump/0087-arm64-mm-create-new-fine-grained-mappings-at-boot.patch
new file mode 100644
index 0000000..78820dd
--- /dev/null
+++ b/tools/kdump/0087-arm64-mm-create-new-fine-grained-mappings-at-boot.patch
@@ -0,0 +1,304 @@
+From ec49d6fc8b9c865916e1b5cab3b38a1b7bdf7dc0 Mon Sep 17 00:00:00 2001
+From: Mark Rutland <mark.rutland@arm.com>
+Date: Mon, 25 Jan 2016 11:45:12 +0000
+Subject: [PATCH 087/123] arm64: mm: create new fine-grained mappings at boot
+
+At boot we may change the granularity of the tables mapping the kernel
+(by splitting or making sections). This may happen when we create the
+linear mapping (in __map_memblock), or at any point we try to apply
+fine-grained permissions to the kernel (e.g. fixup_executable,
+mark_rodata_ro, fixup_init).
+
+Changing the active page tables in this manner may result in multiple
+entries for the same address being allocated into TLBs, risking problems
+such as TLB conflict aborts or issues derived from the amalgamation of
+TLB entries. Generally, a break-before-make (BBM) approach is necessary
+to avoid conflicts, but we cannot do this for the kernel tables as it
+risks unmapping text or data being used to do so.
+
+Instead, we can create a new set of tables from scratch in the safety of
+the existing mappings, and subsequently migrate over to these using the
+new cpu_replace_ttbr1 helper, which avoids the two sets of tables being
+active simultaneously.
+
+To avoid issues when we later modify permissions of the page tables
+(e.g. in fixup_init), we must create the page tables at a granularity
+such that later modification does not result in splitting of tables.
+
+This patch applies this strategy, creating a new set of fine-grained
+page tables from scratch, and safely migrating to them. The existing
+fixmap and kasan shadow page tables are reused in the new fine-grained
+tables.
+
+Signed-off-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+Tested-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Tested-by: Jeremy Linton <jeremy.linton@arm.com>
+Cc: Laura Abbott <labbott@fedoraproject.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 068a17a5805dfbca4bbf03e664ca6b19709cc7a8)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/kasan.h |   3 +
+ arch/arm64/mm/kasan_init.c     |  15 ++++
+ arch/arm64/mm/mmu.c            | 153 ++++++++++++++++++++++++-----------------
+ 3 files changed, 109 insertions(+), 62 deletions(-)
+
+diff --git a/arch/arm64/include/asm/kasan.h b/arch/arm64/include/asm/kasan.h
+index 2774fa3..de0d212 100644
+--- a/arch/arm64/include/asm/kasan.h
++++ b/arch/arm64/include/asm/kasan.h
+@@ -7,6 +7,7 @@
+ 
+ #include <linux/linkage.h>
+ #include <asm/memory.h>
++#include <asm/pgtable-types.h>
+ 
+ /*
+  * KASAN_SHADOW_START: beginning of the kernel virtual addresses.
+@@ -28,10 +29,12 @@
+ #define KASAN_SHADOW_OFFSET     (KASAN_SHADOW_END - (1ULL << (64 - 3)))
+ 
+ void kasan_init(void);
++void kasan_copy_shadow(pgd_t *pgdir);
+ asmlinkage void kasan_early_init(void);
+ 
+ #else
+ static inline void kasan_init(void) { }
++static inline void kasan_copy_shadow(pgd_t *pgdir) { }
+ #endif
+ 
+ #endif
+diff --git a/arch/arm64/mm/kasan_init.c b/arch/arm64/mm/kasan_init.c
+index cf038c7..a365dfa 100644
+--- a/arch/arm64/mm/kasan_init.c
++++ b/arch/arm64/mm/kasan_init.c
+@@ -96,6 +96,21 @@ asmlinkage void __init kasan_early_init(void)
+ 	kasan_map_early_shadow();
+ }
+ 
++/*
++ * Copy the current shadow region into a new pgdir.
++ */
++void __init kasan_copy_shadow(pgd_t *pgdir)
++{
++	pgd_t *pgd, *pgd_new, *pgd_end;
++
++	pgd = pgd_offset_k(KASAN_SHADOW_START);
++	pgd_end = pgd_offset_k(KASAN_SHADOW_END);
++	pgd_new = pgd_offset_raw(pgdir, KASAN_SHADOW_START);
++	do {
++		set_pgd(pgd_new, *pgd);
++	} while (pgd++, pgd_new++, pgd != pgd_end);
++}
++
+ static void __init clear_pgds(unsigned long start,
+ 			unsigned long end)
+ {
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index fd4c95b..9c5b5dd 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -33,6 +33,7 @@
+ #include <asm/barrier.h>
+ #include <asm/cputype.h>
+ #include <asm/fixmap.h>
++#include <asm/kasan.h>
+ #include <asm/kernel-pgtable.h>
+ #include <asm/sections.h>
+ #include <asm/setup.h>
+@@ -336,49 +337,42 @@ static void create_mapping_late(phys_addr_t phys, unsigned long virt,
+ 			     late_pgtable_alloc);
+ }
+ 
+-#ifdef CONFIG_DEBUG_RODATA
+-static void __init __map_memblock(phys_addr_t start, phys_addr_t end)
++static void __init __map_memblock(pgd_t *pgd, phys_addr_t start, phys_addr_t end)
+ {
++
++	unsigned long kernel_start = __pa(_stext);
++	unsigned long kernel_end = __pa(_end);
++
+ 	/*
+-	 * Set up the executable regions using the existing section mappings
+-	 * for now. This will get more fine grained later once all memory
+-	 * is mapped
++	 * The kernel itself is mapped at page granularity. Map all other
++	 * memory, making sure we don't overwrite the existing kernel mappings.
+ 	 */
+-	unsigned long kernel_x_start = round_down(__pa(_stext), SWAPPER_BLOCK_SIZE);
+-	unsigned long kernel_x_end = round_up(__pa(__init_end), SWAPPER_BLOCK_SIZE);
+-
+-	if (end < kernel_x_start) {
+-		create_mapping(start, __phys_to_virt(start),
+-			end - start, PAGE_KERNEL);
+-	} else if (start >= kernel_x_end) {
+-		create_mapping(start, __phys_to_virt(start),
+-			end - start, PAGE_KERNEL);
+-	} else {
+-		if (start < kernel_x_start)
+-			create_mapping(start, __phys_to_virt(start),
+-				kernel_x_start - start,
+-				PAGE_KERNEL);
+-		create_mapping(kernel_x_start,
+-				__phys_to_virt(kernel_x_start),
+-				kernel_x_end - kernel_x_start,
+-				PAGE_KERNEL_EXEC);
+-		if (kernel_x_end < end)
+-			create_mapping(kernel_x_end,
+-				__phys_to_virt(kernel_x_end),
+-				end - kernel_x_end,
+-				PAGE_KERNEL);
++
++	/* No overlap with the kernel. */
++	if (end < kernel_start || start >= kernel_end) {
++		__create_pgd_mapping(pgd, start, __phys_to_virt(start),
++				     end - start, PAGE_KERNEL,
++				     early_pgtable_alloc);
++		return;
+ 	}
+ 
++	/*
++	 * This block overlaps the kernel mapping. Map the portion(s) which
++	 * don't overlap.
++	 */
++	if (start < kernel_start)
++		__create_pgd_mapping(pgd, start,
++				     __phys_to_virt(start),
++				     kernel_start - start, PAGE_KERNEL,
++				     early_pgtable_alloc);
++	if (kernel_end < end)
++		__create_pgd_mapping(pgd, kernel_end,
++				     __phys_to_virt(kernel_end),
++				     end - kernel_end, PAGE_KERNEL,
++				     early_pgtable_alloc);
+ }
+-#else
+-static void __init __map_memblock(phys_addr_t start, phys_addr_t end)
+-{
+-	create_mapping(start, __phys_to_virt(start), end - start,
+-			PAGE_KERNEL_EXEC);
+-}
+-#endif
+ 
+-static void __init map_mem(void)
++static void __init map_mem(pgd_t *pgd)
+ {
+ 	struct memblock_region *reg;
+ 
+@@ -390,33 +384,10 @@ static void __init map_mem(void)
+ 		if (start >= end)
+ 			break;
+ 
+-		__map_memblock(start, end);
++		__map_memblock(pgd, start, end);
+ 	}
+ }
+ 
+-static void __init fixup_executable(void)
+-{
+-#ifdef CONFIG_DEBUG_RODATA
+-	/* now that we are actually fully mapped, make the start/end more fine grained */
+-	if (!IS_ALIGNED((unsigned long)_stext, SWAPPER_BLOCK_SIZE)) {
+-		unsigned long aligned_start = round_down(__pa(_stext),
+-							 SWAPPER_BLOCK_SIZE);
+-
+-		create_mapping(aligned_start, __phys_to_virt(aligned_start),
+-				__pa(_stext) - aligned_start,
+-				PAGE_KERNEL);
+-	}
+-
+-	if (!IS_ALIGNED((unsigned long)__init_end, SWAPPER_BLOCK_SIZE)) {
+-		unsigned long aligned_end = round_up(__pa(__init_end),
+-							  SWAPPER_BLOCK_SIZE);
+-		create_mapping(__pa(__init_end), (unsigned long)__init_end,
+-				aligned_end - __pa(__init_end),
+-				PAGE_KERNEL);
+-	}
+-#endif
+-}
+-
+ #ifdef CONFIG_DEBUG_RODATA
+ void mark_rodata_ro(void)
+ {
+@@ -434,14 +405,72 @@ void fixup_init(void)
+ 			PAGE_KERNEL);
+ }
+ 
++static void __init map_kernel_chunk(pgd_t *pgd, void *va_start, void *va_end,
++				    pgprot_t prot)
++{
++	phys_addr_t pa_start = __pa(va_start);
++	unsigned long size = va_end - va_start;
++
++	BUG_ON(!PAGE_ALIGNED(pa_start));
++	BUG_ON(!PAGE_ALIGNED(size));
++
++	__create_pgd_mapping(pgd, pa_start, (unsigned long)va_start, size, prot,
++			     early_pgtable_alloc);
++}
++
++/*
++ * Create fine-grained mappings for the kernel.
++ */
++static void __init map_kernel(pgd_t *pgd)
++{
++
++	map_kernel_chunk(pgd, _stext, _etext, PAGE_KERNEL_EXEC);
++	map_kernel_chunk(pgd, __init_begin, __init_end, PAGE_KERNEL_EXEC);
++	map_kernel_chunk(pgd, _data, _end, PAGE_KERNEL);
++
++	/*
++	 * The fixmap falls in a separate pgd to the kernel, and doesn't live
++	 * in the carveout for the swapper_pg_dir. We can simply re-use the
++	 * existing dir for the fixmap.
++	 */
++	set_pgd(pgd_offset_raw(pgd, FIXADDR_START), *pgd_offset_k(FIXADDR_START));
++
++	kasan_copy_shadow(pgd);
++}
++
+ /*
+  * paging_init() sets up the page tables, initialises the zone memory
+  * maps and sets up the zero page.
+  */
+ void __init paging_init(void)
+ {
+-	map_mem();
+-	fixup_executable();
++	phys_addr_t pgd_phys = early_pgtable_alloc();
++	pgd_t *pgd = pgd_set_fixmap(pgd_phys);
++
++	map_kernel(pgd);
++	map_mem(pgd);
++
++	/*
++	 * We want to reuse the original swapper_pg_dir so we don't have to
++	 * communicate the new address to non-coherent secondaries in
++	 * secondary_entry, and so cpu_switch_mm can generate the address with
++	 * adrp+add rather than a load from some global variable.
++	 *
++	 * To do this we need to go via a temporary pgd.
++	 */
++	cpu_replace_ttbr1(__va(pgd_phys));
++	memcpy(swapper_pg_dir, pgd, PAGE_SIZE);
++	cpu_replace_ttbr1(swapper_pg_dir);
++
++	pgd_clear_fixmap();
++	memblock_free(pgd_phys, PAGE_SIZE);
++
++	/*
++	 * We only reuse the PGD from the swapper_pg_dir, not the pud + pmd
++	 * allocated with it.
++	 */
++	memblock_free(__pa(swapper_pg_dir) + PAGE_SIZE,
++		      SWAPPER_DIR_SIZE - PAGE_SIZE);
+ 
+ 	bootmem_init();
+ }
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0088-arm64-kernel-implement-ACPI-parking-protocol.patch b/tools/kdump/0088-arm64-kernel-implement-ACPI-parking-protocol.patch
new file mode 100644
index 0000000..20e96a1
--- /dev/null
+++ b/tools/kdump/0088-arm64-kernel-implement-ACPI-parking-protocol.patch
@@ -0,0 +1,443 @@
+From 4c733b34c722493adbfb0f5bf546d20c91f08cf1 Mon Sep 17 00:00:00 2001
+From: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Date: Tue, 26 Jan 2016 11:10:38 +0000
+Subject: [PATCH 088/123] arm64: kernel: implement ACPI parking protocol
+
+The SBBR and ACPI specifications allow ACPI based systems that do not
+implement PSCI (eg systems with no EL3) to boot through the ACPI parking
+protocol specification[1].
+
+This patch implements the ACPI parking protocol CPU operations, and adds
+code that eases parsing the parking protocol data structures to the
+ARM64 SMP initializion carried out at the same time as cpus enumeration.
+
+To wake-up the CPUs from the parked state, this patch implements a
+wakeup IPI for ARM64 (ie arch_send_wakeup_ipi_mask()) that mirrors the
+ARM one, so that a specific IPI is sent for wake-up purpose in order
+to distinguish it from other IPI sources.
+
+Given the current ACPI MADT parsing API, the patch implements a glue
+layer that helps passing MADT GICC data structure from SMP initialization
+code to the parking protocol implementation somewhat overriding the CPU
+operations interfaces. This to avoid creating a completely trasparent
+DT/ACPI CPU operations layer that would require creating opaque
+structure handling for CPUs data (DT represents CPU through DT nodes, ACPI
+through static MADT table entries), which seems overkill given that ACPI
+on ARM64 mandates only two booting protocols (PSCI and parking protocol),
+so there is no need for further protocol additions.
+
+Based on the original work by Mark Salter <msalter@redhat.com>
+
+[1] https://acpica.org/sites/acpica/files/MP%20Startup%20for%20ARM%20platforms.docx
+
+Signed-off-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+Tested-by: Loc Ho <lho@apm.com>
+Cc: Will Deacon <will.deacon@arm.com>
+Cc: Hanjun Guo <hanjun.guo@linaro.org>
+Cc: Sudeep Holla <sudeep.holla@arm.com>
+Cc: Mark Rutland <mark.rutland@arm.com>
+Cc: Mark Salter <msalter@redhat.com>
+Cc: Al Stone <ahs3@redhat.com>
+[catalin.marinas@arm.com: Added WARN_ONCE(!acpi_parking_protocol_valid() on the IPI]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+
+(cherry picked from commit 5e89c55e4ed81d7abb1ce8828db35fa389dc0e90)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	have probes/ now in arch/arm64/kernel/Makefile
+---
+ arch/arm64/Kconfig                        |   9 ++
+ arch/arm64/include/asm/acpi.h             |  19 +++-
+ arch/arm64/include/asm/hardirq.h          |   2 +-
+ arch/arm64/include/asm/smp.h              |   9 ++
+ arch/arm64/kernel/Makefile                |   2 +
+ arch/arm64/kernel/acpi_parking_protocol.c | 153 ++++++++++++++++++++++++++++++
+ arch/arm64/kernel/cpu_ops.c               |  27 +++++-
+ arch/arm64/kernel/smp.c                   |  28 ++++++
+ 8 files changed, 243 insertions(+), 6 deletions(-)
+ create mode 100644 arch/arm64/kernel/acpi_parking_protocol.c
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index b42e3a9..81113b0 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -738,6 +738,15 @@ endmenu
+ 
+ menu "Boot options"
+ 
++config ARM64_ACPI_PARKING_PROTOCOL
++	bool "Enable support for the ARM64 ACPI parking protocol"
++	depends on ACPI
++	help
++	  Enable support for the ARM64 ACPI parking protocol. If disabled
++	  the kernel will not allow booting through the ARM64 ACPI parking
++	  protocol even if the corresponding data is present in the ACPI
++	  MADT table.
++
+ config CMDLINE
+ 	string "Default kernel command string"
+ 	default ""
+diff --git a/arch/arm64/include/asm/acpi.h b/arch/arm64/include/asm/acpi.h
+index caafd63..aee323b 100644
+--- a/arch/arm64/include/asm/acpi.h
++++ b/arch/arm64/include/asm/acpi.h
+@@ -87,9 +87,26 @@ void __init acpi_init_cpus(void);
+ static inline void acpi_init_cpus(void) { }
+ #endif /* CONFIG_ACPI */
+ 
++#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
++bool acpi_parking_protocol_valid(int cpu);
++void __init
++acpi_set_mailbox_entry(int cpu, struct acpi_madt_generic_interrupt *processor);
++#else
++static inline bool acpi_parking_protocol_valid(int cpu) { return false; }
++static inline void
++acpi_set_mailbox_entry(int cpu, struct acpi_madt_generic_interrupt *processor)
++{}
++#endif
++
+ static inline const char *acpi_get_enable_method(int cpu)
+ {
+-	return acpi_psci_present() ? "psci" : NULL;
++	if (acpi_psci_present())
++		return "psci";
++
++	if (acpi_parking_protocol_valid(cpu))
++		return "parking-protocol";
++
++	return NULL;
+ }
+ 
+ #ifdef	CONFIG_ACPI_APEI
+diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
+index a57601f..8740297 100644
+--- a/arch/arm64/include/asm/hardirq.h
++++ b/arch/arm64/include/asm/hardirq.h
+@@ -20,7 +20,7 @@
+ #include <linux/threads.h>
+ #include <asm/irq.h>
+ 
+-#define NR_IPI	5
++#define NR_IPI	6
+ 
+ typedef struct {
+ 	unsigned int __softirq_pending;
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index d9c3d6a..2013a4d 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -64,6 +64,15 @@ extern void secondary_entry(void);
+ extern void arch_send_call_function_single_ipi(int cpu);
+ extern void arch_send_call_function_ipi_mask(const struct cpumask *mask);
+ 
++#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
++extern void arch_send_wakeup_ipi_mask(const struct cpumask *mask);
++#else
++static inline void arch_send_wakeup_ipi_mask(const struct cpumask *mask)
++{
++	BUILD_BUG();
++}
++#endif
++
+ extern int __cpu_disable(void);
+ 
+ extern void __cpu_die(unsigned int cpu);
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index 8867ff7..ee2ffac 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -41,6 +41,8 @@ arm64-obj-$(CONFIG_PCI)			+= pci.o
+ arm64-obj-$(CONFIG_ARMV8_DEPRECATED)	+= armv8_deprecated.o
+ arm64-obj-$(CONFIG_ACPI)		+= acpi.o
+ arm64-obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
++arm64-obj-$(CONFIG_ARM64_ACPI_PARKING_PROTOCOL)	+= acpi_parking_protocol.o
++arm64-obj-$(CONFIG_PARAVIRT)		+= paravirt.o
+ 
+ obj-y					+= $(arm64-obj-y) vdso/ probes/
+ obj-m					+= $(arm64-obj-m)
+diff --git a/arch/arm64/kernel/acpi_parking_protocol.c b/arch/arm64/kernel/acpi_parking_protocol.c
+new file mode 100644
+index 0000000..4b1e5a7
+--- /dev/null
++++ b/arch/arm64/kernel/acpi_parking_protocol.c
+@@ -0,0 +1,153 @@
++/*
++ * ARM64 ACPI Parking Protocol implementation
++ *
++ * Authors: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
++ *	    Mark Salter <msalter@redhat.com>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ *
++ * This program is distributed in the hope that it will be useful,
++ * but WITHOUT ANY WARRANTY; without even the implied warranty of
++ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
++ * GNU General Public License for more details.
++ *
++ * You should have received a copy of the GNU General Public License
++ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
++ */
++#include <linux/acpi.h>
++#include <linux/types.h>
++
++#include <asm/cpu_ops.h>
++
++struct cpu_mailbox_entry {
++	phys_addr_t mailbox_addr;
++	u8 version;
++	u8 gic_cpu_id;
++};
++
++static struct cpu_mailbox_entry cpu_mailbox_entries[NR_CPUS];
++
++void __init acpi_set_mailbox_entry(int cpu,
++				   struct acpi_madt_generic_interrupt *p)
++{
++	struct cpu_mailbox_entry *cpu_entry = &cpu_mailbox_entries[cpu];
++
++	cpu_entry->mailbox_addr = p->parked_address;
++	cpu_entry->version = p->parking_version;
++	cpu_entry->gic_cpu_id = p->cpu_interface_number;
++}
++
++bool acpi_parking_protocol_valid(int cpu)
++{
++	struct cpu_mailbox_entry *cpu_entry = &cpu_mailbox_entries[cpu];
++
++	return cpu_entry->mailbox_addr && cpu_entry->version;
++}
++
++static int acpi_parking_protocol_cpu_init(unsigned int cpu)
++{
++	pr_debug("%s: ACPI parked addr=%llx\n", __func__,
++		  cpu_mailbox_entries[cpu].mailbox_addr);
++
++	return 0;
++}
++
++static int acpi_parking_protocol_cpu_prepare(unsigned int cpu)
++{
++	return 0;
++}
++
++struct parking_protocol_mailbox {
++	__le32 cpu_id;
++	__le32 reserved;
++	__le64 entry_point;
++};
++
++static int acpi_parking_protocol_cpu_boot(unsigned int cpu)
++{
++	struct cpu_mailbox_entry *cpu_entry = &cpu_mailbox_entries[cpu];
++	struct parking_protocol_mailbox __iomem *mailbox;
++	__le32 cpu_id;
++
++	/*
++	 * Map mailbox memory with attribute device nGnRE (ie ioremap -
++	 * this deviates from the parking protocol specifications since
++	 * the mailboxes are required to be mapped nGnRnE; the attribute
++	 * discrepancy is harmless insofar as the protocol specification
++	 * is concerned).
++	 * If the mailbox is mistakenly allocated in the linear mapping
++	 * by FW ioremap will fail since the mapping will be prevented
++	 * by the kernel (it clashes with the linear mapping attributes
++	 * specifications).
++	 */
++	mailbox = ioremap(cpu_entry->mailbox_addr, sizeof(*mailbox));
++	if (!mailbox)
++		return -EIO;
++
++	cpu_id = readl_relaxed(&mailbox->cpu_id);
++	/*
++	 * Check if firmware has set-up the mailbox entry properly
++	 * before kickstarting the respective cpu.
++	 */
++	if (cpu_id != ~0U) {
++		iounmap(mailbox);
++		return -ENXIO;
++	}
++
++	/*
++	 * We write the entry point and cpu id as LE regardless of the
++	 * native endianness of the kernel. Therefore, any boot-loaders
++	 * that read this address need to convert this address to the
++	 * Boot-Loader's endianness before jumping.
++	 */
++	writeq_relaxed(__pa(secondary_entry), &mailbox->entry_point);
++	writel_relaxed(cpu_entry->gic_cpu_id, &mailbox->cpu_id);
++
++	arch_send_wakeup_ipi_mask(cpumask_of(cpu));
++
++	iounmap(mailbox);
++
++	return 0;
++}
++
++static void acpi_parking_protocol_cpu_postboot(void)
++{
++	int cpu = smp_processor_id();
++	struct cpu_mailbox_entry *cpu_entry = &cpu_mailbox_entries[cpu];
++	struct parking_protocol_mailbox __iomem *mailbox;
++	__le64 entry_point;
++
++	/*
++	 * Map mailbox memory with attribute device nGnRE (ie ioremap -
++	 * this deviates from the parking protocol specifications since
++	 * the mailboxes are required to be mapped nGnRnE; the attribute
++	 * discrepancy is harmless insofar as the protocol specification
++	 * is concerned).
++	 * If the mailbox is mistakenly allocated in the linear mapping
++	 * by FW ioremap will fail since the mapping will be prevented
++	 * by the kernel (it clashes with the linear mapping attributes
++	 * specifications).
++	 */
++	mailbox = ioremap(cpu_entry->mailbox_addr, sizeof(*mailbox));
++	if (!mailbox)
++		return;
++
++	entry_point = readl_relaxed(&mailbox->entry_point);
++	/*
++	 * Check if firmware has cleared the entry_point as expected
++	 * by the protocol specification.
++	 */
++	WARN_ON(entry_point);
++
++	iounmap(mailbox);
++}
++
++const struct cpu_operations acpi_parking_protocol_ops = {
++	.name		= "parking-protocol",
++	.cpu_init	= acpi_parking_protocol_cpu_init,
++	.cpu_prepare	= acpi_parking_protocol_cpu_prepare,
++	.cpu_boot	= acpi_parking_protocol_cpu_boot,
++	.cpu_postboot	= acpi_parking_protocol_cpu_postboot
++};
+diff --git a/arch/arm64/kernel/cpu_ops.c b/arch/arm64/kernel/cpu_ops.c
+index b6bd7d4..c7cfb8f 100644
+--- a/arch/arm64/kernel/cpu_ops.c
++++ b/arch/arm64/kernel/cpu_ops.c
+@@ -25,19 +25,30 @@
+ #include <asm/smp_plat.h>
+ 
+ extern const struct cpu_operations smp_spin_table_ops;
++extern const struct cpu_operations acpi_parking_protocol_ops;
+ extern const struct cpu_operations cpu_psci_ops;
+ 
+ const struct cpu_operations *cpu_ops[NR_CPUS];
+ 
+-static const struct cpu_operations *supported_cpu_ops[] __initconst = {
++static const struct cpu_operations *dt_supported_cpu_ops[] __initconst = {
+ 	&smp_spin_table_ops,
+ 	&cpu_psci_ops,
+ 	NULL,
+ };
+ 
++static const struct cpu_operations *acpi_supported_cpu_ops[] __initconst = {
++#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
++	&acpi_parking_protocol_ops,
++#endif
++	&cpu_psci_ops,
++	NULL,
++};
++
+ static const struct cpu_operations * __init cpu_get_ops(const char *name)
+ {
+-	const struct cpu_operations **ops = supported_cpu_ops;
++	const struct cpu_operations **ops;
++
++	ops = acpi_disabled ? dt_supported_cpu_ops : acpi_supported_cpu_ops;
+ 
+ 	while (*ops) {
+ 		if (!strcmp(name, (*ops)->name))
+@@ -75,8 +86,16 @@ static const char *__init cpu_read_enable_method(int cpu)
+ 		}
+ 	} else {
+ 		enable_method = acpi_get_enable_method(cpu);
+-		if (!enable_method)
+-			pr_err("Unsupported ACPI enable-method\n");
++		if (!enable_method) {
++			/*
++			 * In ACPI systems the boot CPU does not require
++			 * checking the enable method since for some
++			 * boot protocol (ie parking protocol) it need not
++			 * be initialized. Don't warn spuriously.
++			 */
++			if (cpu != 0)
++				pr_err("Unsupported ACPI enable-method\n");
++		}
+ 	}
+ 
+ 	return enable_method;
+diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
+index 3df454f..a84623d 100644
+--- a/arch/arm64/kernel/smp.c
++++ b/arch/arm64/kernel/smp.c
+@@ -70,6 +70,7 @@ enum ipi_msg_type {
+ 	IPI_CPU_STOP,
+ 	IPI_TIMER,
+ 	IPI_IRQ_WORK,
++	IPI_WAKEUP
+ };
+ 
+ /*
+@@ -442,6 +443,17 @@ acpi_map_gic_cpu_interface(struct acpi_madt_generic_interrupt *processor)
+ 	/* map the logical cpu id to cpu MPIDR */
+ 	cpu_logical_map(cpu_count) = hwid;
+ 
++	/*
++	 * Set-up the ACPI parking protocol cpu entries
++	 * while initializing the cpu_logical_map to
++	 * avoid parsing MADT entries multiple times for
++	 * nothing (ie a valid cpu_logical_map entry should
++	 * contain a valid parking protocol data set to
++	 * initialize the cpu if the parking protocol is
++	 * the only available enable method).
++	 */
++	acpi_set_mailbox_entry(cpu_count, processor);
++
+ 	cpu_count++;
+ }
+ 
+@@ -624,6 +636,7 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
+ 	S(IPI_CPU_STOP, "CPU stop interrupts"),
+ 	S(IPI_TIMER, "Timer broadcast interrupts"),
+ 	S(IPI_IRQ_WORK, "IRQ work interrupts"),
++	S(IPI_WAKEUP, "CPU wake-up interrupts"),
+ };
+ 
+ static void smp_cross_call(const struct cpumask *target, unsigned int ipinr)
+@@ -667,6 +680,13 @@ void arch_send_call_function_single_ipi(int cpu)
+ 	smp_cross_call(cpumask_of(cpu), IPI_CALL_FUNC);
+ }
+ 
++#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
++void arch_send_wakeup_ipi_mask(const struct cpumask *mask)
++{
++	smp_cross_call(mask, IPI_WAKEUP);
++}
++#endif
++
+ #ifdef CONFIG_IRQ_WORK
+ void arch_irq_work_raise(void)
+ {
+@@ -744,6 +764,14 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
+ 		break;
+ #endif
+ 
++#ifdef CONFIG_ARM64_ACPI_PARKING_PROTOCOL
++	case IPI_WAKEUP:
++		WARN_ONCE(!acpi_parking_protocol_valid(cpu),
++			  "CPU%u: Wake-up IPI outside the ACPI parking protocol\n",
++			  cpu);
++		break;
++#endif
++
+ 	default:
+ 		pr_crit("CPU%u: Unknown IPI message 0x%x\n", cpu, ipinr);
+ 		break;
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0089-arm64-allow-vmalloc-regions-to-be-set-with-set_memor.patch b/tools/kdump/0089-arm64-allow-vmalloc-regions-to-be-set-with-set_memor.patch
new file mode 100644
index 0000000..79298ab
--- /dev/null
+++ b/tools/kdump/0089-arm64-allow-vmalloc-regions-to-be-set-with-set_memor.patch
@@ -0,0 +1,73 @@
+From a7779eb8064501469cce444bf0c3de5a9f25c97f Mon Sep 17 00:00:00 2001
+From: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Date: Wed, 27 Jan 2016 10:50:19 +0100
+Subject: [PATCH 089/123] arm64: allow vmalloc regions to be set with
+ set_memory_*
+
+The range of set_memory_* is currently restricted to the module address
+range because of difficulties in breaking down larger block sizes.
+vmalloc maps PAGE_SIZE pages so it is safe to use as well. Update the
+function ranges and add a comment explaining why the range is restricted
+the way it is.
+
+Suggested-by: Laura Abbott <labbott@fedoraproject.org>
+Acked-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 95f5c80050ad723163aa80dc8bffd48ef4afc6d5)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/pageattr.c | 23 +++++++++++++++++++----
+ 1 file changed, 19 insertions(+), 4 deletions(-)
+
+diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c
+index cf62407..0795c3a 100644
+--- a/arch/arm64/mm/pageattr.c
++++ b/arch/arm64/mm/pageattr.c
+@@ -14,6 +14,7 @@
+ #include <linux/mm.h>
+ #include <linux/module.h>
+ #include <linux/sched.h>
++#include <linux/vmalloc.h>
+ 
+ #include <asm/pgtable.h>
+ #include <asm/tlbflush.h>
+@@ -44,6 +45,7 @@ static int change_memory_common(unsigned long addr, int numpages,
+ 	unsigned long end = start + size;
+ 	int ret;
+ 	struct page_change_data data;
++	struct vm_struct *area;
+ 
+ 	if (!PAGE_ALIGNED(addr)) {
+ 		start &= PAGE_MASK;
+@@ -51,10 +53,23 @@ static int change_memory_common(unsigned long addr, int numpages,
+ 		WARN_ON_ONCE(1);
+ 	}
+ 
+-	if (start < MODULES_VADDR || start >= MODULES_END)
+-		return -EINVAL;
+-
+-	if (end < MODULES_VADDR || end >= MODULES_END)
++	/*
++	 * Kernel VA mappings are always live, and splitting live section
++	 * mappings into page mappings may cause TLB conflicts. This means
++	 * we have to ensure that changing the permission bits of the range
++	 * we are operating on does not result in such splitting.
++	 *
++	 * Let's restrict ourselves to mappings created by vmalloc (or vmap).
++	 * Those are guaranteed to consist entirely of page mappings, and
++	 * splitting is never needed.
++	 *
++	 * So check whether the [addr, addr + size) interval is entirely
++	 * covered by precisely one VM area that has the VM_ALLOC flag set.
++	 */
++	area = find_vm_area((void *)addr);
++	if (!area ||
++	    end > (unsigned long)area->addr + area->size ||
++	    !(area->flags & VM_ALLOC))
+ 		return -EINVAL;
+ 
+ 	if (!numpages)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0090-arm64-Drop-alloc-function-from-create_mapping.patch b/tools/kdump/0090-arm64-Drop-alloc-function-from-create_mapping.patch
new file mode 100644
index 0000000..de0c334
--- /dev/null
+++ b/tools/kdump/0090-arm64-Drop-alloc-function-from-create_mapping.patch
@@ -0,0 +1,114 @@
+From 410457a0b95c48130b08cb4f3ca48f41bf0cd4a7 Mon Sep 17 00:00:00 2001
+From: Laura Abbott <labbott@fedoraproject.org>
+Date: Fri, 5 Feb 2016 16:24:46 -0800
+Subject: [PATCH 090/123] arm64: Drop alloc function from create_mapping
+
+create_mapping is only used in fixmap_remap_fdt. All the create_mapping
+calls need to happen on existing translation table pages without
+additional allocations. Rather than have an alloc function be called
+and fail, just set it to NULL and catch its use. Also change
+the name to create_mapping_noalloc to better capture what exactly is
+going on.
+
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit 132233a759580f5ce9b1bfaac9073e47d03c460d)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/mmu.c | 29 ++++++++++++++++++++---------
+ 1 file changed, 20 insertions(+), 9 deletions(-)
+
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 9c5b5dd..2ea381a 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -116,7 +116,9 @@ static void alloc_init_pte(pmd_t *pmd, unsigned long addr,
+ 	pte_t *pte;
+ 
+ 	if (pmd_none(*pmd) || pmd_sect(*pmd)) {
+-		phys_addr_t pte_phys = pgtable_alloc();
++		phys_addr_t pte_phys;
++		BUG_ON(!pgtable_alloc);
++		pte_phys = pgtable_alloc();
+ 		pte = pte_set_fixmap(pte_phys);
+ 		if (pmd_sect(*pmd))
+ 			split_pmd(pmd, pte);
+@@ -158,7 +160,9 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,
+ 	 * Check for initial section mappings in the pgd/pud and remove them.
+ 	 */
+ 	if (pud_none(*pud) || pud_sect(*pud)) {
+-		phys_addr_t pmd_phys = pgtable_alloc();
++		phys_addr_t pmd_phys;
++		BUG_ON(!pgtable_alloc);
++		pmd_phys = pgtable_alloc();
+ 		pmd = pmd_set_fixmap(pmd_phys);
+ 		if (pud_sect(*pud)) {
+ 			/*
+@@ -223,7 +227,9 @@ static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,
+ 	unsigned long next;
+ 
+ 	if (pgd_none(*pgd)) {
+-		phys_addr_t pud_phys = pgtable_alloc();
++		phys_addr_t pud_phys;
++		BUG_ON(!pgtable_alloc);
++		pud_phys = pgtable_alloc();
+ 		__pgd_populate(pgd, pud_phys, PUD_TYPE_TABLE);
+ 	}
+ 	BUG_ON(pgd_bad(*pgd));
+@@ -304,7 +310,12 @@ static void __create_pgd_mapping(pgd_t *pgdir, phys_addr_t phys,
+ 	init_pgd(pgd_offset_raw(pgdir, virt), phys, virt, size, prot, alloc);
+ }
+ 
+-static void __init create_mapping(phys_addr_t phys, unsigned long virt,
++/*
++ * This function can only be used to modify existing table entries,
++ * without allocating new levels of table. Note that this permits the
++ * creation of new section or page entries.
++ */
++static void __init create_mapping_noalloc(phys_addr_t phys, unsigned long virt,
+ 				  phys_addr_t size, pgprot_t prot)
+ {
+ 	if (virt < VMALLOC_START) {
+@@ -313,7 +324,7 @@ static void __init create_mapping(phys_addr_t phys, unsigned long virt,
+ 		return;
+ 	}
+ 	__create_pgd_mapping(init_mm.pgd, phys, virt, size, prot,
+-			     early_pgtable_alloc);
++			     NULL);
+ }
+ 
+ void __init create_pgd_mapping(struct mm_struct *mm, phys_addr_t phys,
+@@ -670,7 +681,7 @@ void *__init fixmap_remap_fdt(phys_addr_t dt_phys)
+ 	/*
+ 	 * Make sure that the FDT region can be mapped without the need to
+ 	 * allocate additional translation table pages, so that it is safe
+-	 * to call create_mapping() this early.
++	 * to call create_mapping_noalloc() this early.
+ 	 *
+ 	 * On 64k pages, the FDT will be mapped using PTEs, so we need to
+ 	 * be in the same PMD as the rest of the fixmap.
+@@ -686,8 +697,8 @@ void *__init fixmap_remap_fdt(phys_addr_t dt_phys)
+ 	dt_virt = (void *)dt_virt_base + offset;
+ 
+ 	/* map the first chunk so we can read the size from the header */
+-	create_mapping(round_down(dt_phys, SWAPPER_BLOCK_SIZE), dt_virt_base,
+-		       SWAPPER_BLOCK_SIZE, prot);
++	create_mapping_noalloc(round_down(dt_phys, SWAPPER_BLOCK_SIZE),
++			dt_virt_base, SWAPPER_BLOCK_SIZE, prot);
+ 
+ 	if (fdt_magic(dt_virt) != FDT_MAGIC)
+ 		return NULL;
+@@ -697,7 +708,7 @@ void *__init fixmap_remap_fdt(phys_addr_t dt_phys)
+ 		return NULL;
+ 
+ 	if (offset + size > SWAPPER_BLOCK_SIZE)
+-		create_mapping(round_down(dt_phys, SWAPPER_BLOCK_SIZE), dt_virt_base,
++		create_mapping_noalloc(round_down(dt_phys, SWAPPER_BLOCK_SIZE), dt_virt_base,
+ 			       round_up(offset + size, SWAPPER_BLOCK_SIZE), prot);
+ 
+ 	memblock_reserve(dt_phys, size);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0091-arm64-Add-support-for-ARCH_SUPPORTS_DEBUG_PAGEALLOC.patch b/tools/kdump/0091-arm64-Add-support-for-ARCH_SUPPORTS_DEBUG_PAGEALLOC.patch
new file mode 100644
index 0000000..4b996f6
--- /dev/null
+++ b/tools/kdump/0091-arm64-Add-support-for-ARCH_SUPPORTS_DEBUG_PAGEALLOC.patch
@@ -0,0 +1,170 @@
+From 779809e26d2c9891125e215baf05dfbb92d85e74 Mon Sep 17 00:00:00 2001
+From: Laura Abbott <labbott@fedoraproject.org>
+Date: Fri, 5 Feb 2016 16:24:47 -0800
+Subject: [PATCH 091/123] arm64: Add support for ARCH_SUPPORTS_DEBUG_PAGEALLOC
+
+ARCH_SUPPORTS_DEBUG_PAGEALLOC provides a hook to map and unmap
+pages for debugging purposes. This requires memory be mapped
+with PAGE_SIZE mappings since breaking down larger mappings
+at runtime will lead to TLB conflicts. Check if debug_pagealloc
+is enabled at runtime and if so, map everyting with PAGE_SIZE
+pages. Implement the functions to actually map/unmap the
+pages at runtime.
+
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Tested-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
+[catalin.marinas@arm.com: static annotation block_mappings_allowed() and #ifdef]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+
+(cherry picked from commit 83863f25e4b8214e994ef8b5647aad614d74b45d)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/Kconfig       |  3 +++
+ arch/arm64/mm/mmu.c      | 26 ++++++++++++++++++++++++--
+ arch/arm64/mm/pageattr.c | 46 ++++++++++++++++++++++++++++++++++++----------
+ 3 files changed, 63 insertions(+), 12 deletions(-)
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index 81113b0..eda5d99 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -536,6 +536,9 @@ config HOTPLUG_CPU
+ source kernel/Kconfig.preempt
+ source kernel/Kconfig.hz
+ 
++config ARCH_SUPPORTS_DEBUG_PAGEALLOC
++	def_bool y
++
+ config ARCH_HAS_HOLES_MEMORYMODEL
+ 	def_bool y if SPARSEMEM
+ 
+diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
+index 2ea381a..ee2821c 100644
+--- a/arch/arm64/mm/mmu.c
++++ b/arch/arm64/mm/mmu.c
+@@ -149,6 +149,26 @@ static void split_pud(pud_t *old_pud, pmd_t *pmd)
+ 	} while (pmd++, i++, i < PTRS_PER_PMD);
+ }
+ 
++#ifdef CONFIG_DEBUG_PAGEALLOC
++static bool block_mappings_allowed(phys_addr_t (*pgtable_alloc)(void))
++{
++
++	/*
++	 * If debug_page_alloc is enabled we must map the linear map
++	 * using pages. However, other mappings created by
++	 * create_mapping_noalloc must use sections in some cases. Allow
++	 * sections to be used in those cases, where no pgtable_alloc
++	 * function is provided.
++	 */
++	return !pgtable_alloc || !debug_pagealloc_enabled();
++}
++#else
++static bool block_mappings_allowed(phys_addr_t (*pgtable_alloc)(void))
++{
++	return true;
++}
++#endif
++
+ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,
+ 				  phys_addr_t phys, pgprot_t prot,
+ 				  phys_addr_t (*pgtable_alloc)(void))
+@@ -181,7 +201,8 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,
+ 	do {
+ 		next = pmd_addr_end(addr, end);
+ 		/* try section mapping first */
+-		if (((addr | next | phys) & ~SECTION_MASK) == 0) {
++		if (((addr | next | phys) & ~SECTION_MASK) == 0 &&
++		      block_mappings_allowed(pgtable_alloc)) {
+ 			pmd_t old_pmd =*pmd;
+ 			set_pmd(pmd, __pmd(phys |
+ 					   pgprot_val(mk_sect_prot(prot))));
+@@ -241,7 +262,8 @@ static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,
+ 		/*
+ 		 * For 4K granule only, attempt to put down a 1GB block
+ 		 */
+-		if (use_1G_block(addr, next, phys)) {
++		if (use_1G_block(addr, next, phys) &&
++		    block_mappings_allowed(pgtable_alloc)) {
+ 			pud_t old_pud = *pud;
+ 			set_pud(pud, __pud(phys |
+ 					   pgprot_val(mk_sect_prot(prot))));
+diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c
+index 0795c3a..ca6d268 100644
+--- a/arch/arm64/mm/pageattr.c
++++ b/arch/arm64/mm/pageattr.c
+@@ -37,14 +37,31 @@ static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,
+ 	return 0;
+ }
+ 
++/*
++ * This function assumes that the range is mapped with PAGE_SIZE pages.
++ */
++static int __change_memory_common(unsigned long start, unsigned long size,
++				pgprot_t set_mask, pgprot_t clear_mask)
++{
++	struct page_change_data data;
++	int ret;
++
++	data.set_mask = set_mask;
++	data.clear_mask = clear_mask;
++
++	ret = apply_to_page_range(&init_mm, start, size, change_page_range,
++					&data);
++
++	flush_tlb_kernel_range(start, start + size);
++	return ret;
++}
++
+ static int change_memory_common(unsigned long addr, int numpages,
+ 				pgprot_t set_mask, pgprot_t clear_mask)
+ {
+ 	unsigned long start = addr;
+ 	unsigned long size = PAGE_SIZE*numpages;
+ 	unsigned long end = start + size;
+-	int ret;
+-	struct page_change_data data;
+ 	struct vm_struct *area;
+ 
+ 	if (!PAGE_ALIGNED(addr)) {
+@@ -75,14 +92,7 @@ static int change_memory_common(unsigned long addr, int numpages,
+ 	if (!numpages)
+ 		return 0;
+ 
+-	data.set_mask = set_mask;
+-	data.clear_mask = clear_mask;
+-
+-	ret = apply_to_page_range(&init_mm, start, size, change_page_range,
+-					&data);
+-
+-	flush_tlb_kernel_range(start, end);
+-	return ret;
++	return __change_memory_common(start, size, set_mask, clear_mask);
+ }
+ 
+ int set_memory_ro(unsigned long addr, int numpages)
+@@ -114,3 +124,19 @@ int set_memory_x(unsigned long addr, int numpages)
+ 					__pgprot(PTE_PXN));
+ }
+ EXPORT_SYMBOL_GPL(set_memory_x);
++
++#ifdef CONFIG_DEBUG_PAGEALLOC
++void __kernel_map_pages(struct page *page, int numpages, int enable)
++{
++	unsigned long addr = (unsigned long) page_address(page);
++
++	if (enable)
++		__change_memory_common(addr, PAGE_SIZE * numpages,
++					__pgprot(PTE_VALID),
++					__pgprot(0));
++	else
++		__change_memory_common(addr, PAGE_SIZE * numpages,
++					__pgprot(0),
++					__pgprot(PTE_VALID));
++}
++#endif
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0092-arm64-ptdump-Indicate-whether-memory-should-be-fault.patch b/tools/kdump/0092-arm64-ptdump-Indicate-whether-memory-should-be-fault.patch
new file mode 100644
index 0000000..bcdb684
--- /dev/null
+++ b/tools/kdump/0092-arm64-ptdump-Indicate-whether-memory-should-be-fault.patch
@@ -0,0 +1,41 @@
+From 1a8ccc494af8f0726fb82db78a9f42ce3e0b65e4 Mon Sep 17 00:00:00 2001
+From: Laura Abbott <labbott@fedoraproject.org>
+Date: Fri, 5 Feb 2016 16:24:48 -0800
+Subject: [PATCH 092/123] arm64: ptdump: Indicate whether memory should be
+ faulting
+
+With CONFIG_DEBUG_PAGEALLOC, pages do not have the valid bit
+set when free in the buddy allocator. Add an indiciation to
+the page table dumping code that the valid bit is not set,
+'F' for fault, to make this easier to understand.
+
+Reviewed-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Tested-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Laura Abbott <labbott@fedoraproject.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit d7e9d59494a9a5d83274f5af2148b82ca22dff3f)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/mm/dump.c | 5 +++++
+ 1 file changed, 5 insertions(+)
+
+diff --git a/arch/arm64/mm/dump.c b/arch/arm64/mm/dump.c
+index 5a22a11..f381ac9 100644
+--- a/arch/arm64/mm/dump.c
++++ b/arch/arm64/mm/dump.c
+@@ -90,6 +90,11 @@ struct prot_bits {
+ 
+ static const struct prot_bits pte_bits[] = {
+ 	{
++		.mask	= PTE_VALID,
++		.val	= PTE_VALID,
++		.set	= " ",
++		.clear	= "F",
++	}, {
+ 		.mask	= PTE_USER,
+ 		.val	= PTE_USER,
+ 		.set	= "USR",
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0093-arm64-introduce-KIMAGE_VADDR-as-the-virtual-base-of-.patch b/tools/kdump/0093-arm64-introduce-KIMAGE_VADDR-as-the-virtual-base-of-.patch
new file mode 100644
index 0000000..1ee9244
--- /dev/null
+++ b/tools/kdump/0093-arm64-introduce-KIMAGE_VADDR-as-the-virtual-base-of-.patch
@@ -0,0 +1,87 @@
+From 296fe0a67f0153994d68ef7303b35568aaf8c59e Mon Sep 17 00:00:00 2001
+From: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Date: Tue, 16 Feb 2016 13:52:36 +0100
+Subject: [PATCH 093/123] arm64: introduce KIMAGE_VADDR as the virtual base of
+ the kernel region
+
+This introduces the preprocessor symbol KIMAGE_VADDR which will serve as
+the symbolic virtual base of the kernel region, i.e., the kernel's virtual
+offset will be KIMAGE_VADDR + TEXT_OFFSET. For now, we define it as being
+equal to PAGE_OFFSET, but in the future, it will be moved below it once
+we move the kernel virtual mapping out of the linear mapping.
+
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit ab893fb9f1b17f02139bce547bb4b69e96b9ae16)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/memory.h | 10 ++++++++--
+ arch/arm64/kernel/head.S        |  2 +-
+ arch/arm64/kernel/vmlinux.lds.S |  4 ++--
+ 3 files changed, 11 insertions(+), 5 deletions(-)
+
+diff --git a/arch/arm64/include/asm/memory.h b/arch/arm64/include/asm/memory.h
+index 5773a66..2e979ec 100644
+--- a/arch/arm64/include/asm/memory.h
++++ b/arch/arm64/include/asm/memory.h
+@@ -51,7 +51,8 @@
+ #define VA_BITS			(CONFIG_ARM64_VA_BITS)
+ #define VA_START		(UL(0xffffffffffffffff) << VA_BITS)
+ #define PAGE_OFFSET		(UL(0xffffffffffffffff) << (VA_BITS - 1))
+-#define MODULES_END		(PAGE_OFFSET)
++#define KIMAGE_VADDR		(PAGE_OFFSET)
++#define MODULES_END		(KIMAGE_VADDR)
+ #define MODULES_VADDR		(MODULES_END - SZ_64M)
+ #define PCI_IO_END		(MODULES_VADDR - SZ_2M)
+ #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
+@@ -78,8 +79,13 @@
+  * private definitions which should NOT be used outside memory.h
+  * files.  Use virt_to_phys/phys_to_virt/__pa/__va instead.
+  */
+-#define __virt_to_phys(x)	(((phys_addr_t)(x) - PAGE_OFFSET + PHYS_OFFSET))
++#define __virt_to_phys(x) ({						\
++	phys_addr_t __x = (phys_addr_t)(x);				\
++	__x >= PAGE_OFFSET ? (__x - PAGE_OFFSET + PHYS_OFFSET) :	\
++			     (__x - KIMAGE_VADDR + PHYS_OFFSET); })
++
+ #define __phys_to_virt(x)	((unsigned long)((x) - PHYS_OFFSET + PAGE_OFFSET))
++#define __phys_to_kimg(x)	((unsigned long)((x) - PHYS_OFFSET + KIMAGE_VADDR))
+ 
+ /*
+  * Convert a page to/from a physical address
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index 0d84e73..2f38749 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -386,7 +386,7 @@ __create_page_tables:
+ 	 * Map the kernel image (starting with PHYS_OFFSET).
+ 	 */
+ 	mov	x0, x26				// swapper_pg_dir
+-	mov	x5, #PAGE_OFFSET
++	ldr	x5, =KIMAGE_VADDR
+ 	create_pgd_entry x0, x5, x3, x6
+ 	ldr	x6, =KERNEL_END			// __va(KERNEL_END)
+ 	mov	x3, x24				// phys offset
+diff --git a/arch/arm64/kernel/vmlinux.lds.S b/arch/arm64/kernel/vmlinux.lds.S
+index fa2fc07..9e570d4 100644
+--- a/arch/arm64/kernel/vmlinux.lds.S
++++ b/arch/arm64/kernel/vmlinux.lds.S
+@@ -99,7 +99,7 @@ SECTIONS
+ 		*(.discard.*)
+ 	}
+ 
+-	. = PAGE_OFFSET + TEXT_OFFSET;
++	. = KIMAGE_VADDR + TEXT_OFFSET;
+ 
+ 	.head.text : {
+ 		_text = .;
+@@ -205,4 +205,4 @@ ASSERT(__hibernate_exit_text_end - (__hibernate_exit_text_start & ~(SZ_4K - 1))
+ /*
+  * If padding is applied before .head.text, virt<->phys conversions will fail.
+  */
+-ASSERT(_text == (PAGE_OFFSET + TEXT_OFFSET), "HEAD is misaligned")
++ASSERT(_text == (KIMAGE_VADDR + TEXT_OFFSET), "HEAD is misaligned")
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0094-arm64-kvm-deal-with-kernel-symbols-outside-of-linear.patch b/tools/kdump/0094-arm64-kvm-deal-with-kernel-symbols-outside-of-linear.patch
new file mode 100644
index 0000000..1a9b233
--- /dev/null
+++ b/tools/kdump/0094-arm64-kvm-deal-with-kernel-symbols-outside-of-linear.patch
@@ -0,0 +1,164 @@
+From 47a90102ae6b7d2892bcd6fa18750f71ad7a21be Mon Sep 17 00:00:00 2001
+From: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Date: Tue, 16 Feb 2016 13:52:39 +0100
+Subject: [PATCH 094/123] arm64: kvm: deal with kernel symbols outside of
+ linear mapping
+
+KVM on arm64 uses a fixed offset between the linear mapping at EL1 and
+the HYP mapping at EL2. Before we can move the kernel virtual mapping
+out of the linear mapping, we have to make sure that references to kernel
+symbols that are accessed via the HYP mapping are translated to their
+linear equivalent.
+
+Reviewed-by: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit a0bf9776cd0be4490d4675d4108e13379849fc7f)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/kvm/hyp.S
+---
+ arch/arm/include/asm/kvm_asm.h    |  2 ++
+ arch/arm/kvm/arm.c                |  8 +++++---
+ arch/arm64/include/asm/kvm_asm.h  | 17 +++++++++++++++++
+ arch/arm64/include/asm/kvm_host.h |  8 +++++---
+ arch/arm64/kvm/hyp.S              |  6 +++---
+ 5 files changed, 32 insertions(+), 9 deletions(-)
+
+diff --git a/arch/arm/include/asm/kvm_asm.h b/arch/arm/include/asm/kvm_asm.h
+index 194c91b..c35c349 100644
+--- a/arch/arm/include/asm/kvm_asm.h
++++ b/arch/arm/include/asm/kvm_asm.h
+@@ -79,6 +79,8 @@
+ #define rr_lo_hi(a1, a2) a1, a2
+ #endif
+ 
++#define kvm_ksym_ref(kva)	(kva)
++
+ #ifndef __ASSEMBLY__
+ struct kvm;
+ struct kvm_vcpu;
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index 30c9f7b..7a81332 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -979,7 +979,7 @@ static void cpu_init_hyp_mode(void *dummy)
+ 	pgd_ptr = kvm_mmu_get_httbr();
+ 	stack_page = __this_cpu_read(kvm_arm_hyp_stack_page);
+ 	hyp_stack_ptr = stack_page + PAGE_SIZE;
+-	vector_ptr = (unsigned long)__kvm_hyp_vector;
++	vector_ptr = (unsigned long)kvm_ksym_ref(__kvm_hyp_vector);
+ 
+ 	__cpu_init_hyp_mode(boot_pgd_ptr, pgd_ptr, hyp_stack_ptr, vector_ptr);
+ 	__cpu_init_stage2();
+@@ -1072,13 +1072,15 @@ static int init_hyp_mode(void)
+ 	/*
+ 	 * Map the Hyp-code called directly from the host
+ 	 */
+-	err = create_hyp_mappings(__kvm_hyp_code_start, __kvm_hyp_code_end);
++	err = create_hyp_mappings(kvm_ksym_ref(__kvm_hyp_code_start),
++				  kvm_ksym_ref(__kvm_hyp_code_end));
+ 	if (err) {
+ 		kvm_err("Cannot map world-switch code\n");
+ 		goto out_free_mappings;
+ 	}
+ 
+-	err = create_hyp_mappings(__start_rodata, __end_rodata);
++	err = create_hyp_mappings(kvm_ksym_ref(__start_rodata),
++				  kvm_ksym_ref(__end_rodata));
+ 	if (err) {
+ 		kvm_err("Cannot map rodata section\n");
+ 		goto out_free_mappings;
+diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
+index 52b777b..31b5600 100644
+--- a/arch/arm64/include/asm/kvm_asm.h
++++ b/arch/arm64/include/asm/kvm_asm.h
+@@ -26,7 +26,24 @@
+ #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
+ #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
+ 
++#define kvm_ksym_ref(sym)		((void *)&sym + kvm_ksym_shift)
++
+ #ifndef __ASSEMBLY__
++#if __GNUC__ > 4
++#define kvm_ksym_shift			(PAGE_OFFSET - KIMAGE_VADDR)
++#else
++/*
++ * GCC versions 4.9 and older will fold the constant below into the addend of
++ * the reference to 'sym' above if kvm_ksym_shift is declared static or if the
++ * constant is used directly. However, since we use the small code model for
++ * the core kernel, the reference to 'sym' will be emitted as a adrp/add pair,
++ * with a +/- 4 GB range, resulting in linker relocation errors if the shift
++ * is sufficiently large. So prevent the compiler from folding the shift into
++ * the addend, by making the shift a variable with external linkage.
++ */
++__weak u64 kvm_ksym_shift = PAGE_OFFSET - KIMAGE_VADDR;
++#endif
++
+ struct kvm;
+ struct kvm_vcpu;
+ 
+diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
+index 1b37b5d..bbdaa56 100644
+--- a/arch/arm64/include/asm/kvm_host.h
++++ b/arch/arm64/include/asm/kvm_host.h
+@@ -301,7 +301,7 @@ static inline void kvm_arch_mmu_notifier_invalidate_page(struct kvm *kvm,
+ struct kvm_vcpu *kvm_arm_get_running_vcpu(void);
+ struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void);
+ 
+-u64 kvm_call_hyp(void *hypfn, ...);
++u64 __kvm_call_hyp(void *hypfn, ...);
+ void force_vm_exit(const cpumask_t *mask);
+ void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);
+ 
+@@ -322,8 +322,8 @@ static inline void __cpu_init_hyp_mode(phys_addr_t boot_pgd_ptr,
+ 	 * Call initialization code, and switch to the full blown
+ 	 * HYP code.
+ 	 */
+-	kvm_call_hyp((void *)boot_pgd_ptr, pgd_ptr,
+-		     hyp_stack_ptr, vector_ptr);
++	__kvm_call_hyp((void *)boot_pgd_ptr, pgd_ptr,
++		       hyp_stack_ptr, vector_ptr);
+ }
+ 
+ static inline void __cpu_init_stage2(void)
+@@ -341,4 +341,6 @@ void kvm_arm_setup_debug(struct kvm_vcpu *vcpu);
+ void kvm_arm_clear_debug(struct kvm_vcpu *vcpu);
+ void kvm_arm_reset_debug_ptr(struct kvm_vcpu *vcpu);
+ 
++#define kvm_call_hyp(f, ...) __kvm_call_hyp(kvm_ksym_ref(f), ##__VA_ARGS__)
++
+ #endif /* __ARM64_KVM_HOST_H__ */
+diff --git a/arch/arm64/kvm/hyp.S b/arch/arm64/kvm/hyp.S
+index 0689a74..48f19a3 100644
+--- a/arch/arm64/kvm/hyp.S
++++ b/arch/arm64/kvm/hyp.S
+@@ -22,7 +22,7 @@
+ #include <asm/cpufeature.h>
+ 
+ /*
+- * u64 kvm_call_hyp(void *hypfn, ...);
++ * u64 __kvm_call_hyp(void *hypfn, ...);
+  *
+  * This is not really a variadic function in the classic C-way and care must
+  * be taken when calling this to ensure parameters are passed in registers
+@@ -39,7 +39,7 @@
+  * used to implement __hyp_get_vectors in the same way as in
+  * arch/arm64/kernel/hyp_stub.S.
+  */
+-ENTRY(kvm_call_hyp)
++ENTRY(__kvm_call_hyp)
+ alternative_if_not ARM64_HAS_VIRT_HOST_EXTN	
+ 	hvc	#0
+ 	ret
+@@ -47,4 +47,4 @@ alternative_else
+ 	b	__vhe_hyp_call
+ 	nop
+ alternative_endif
+-ENDPROC(kvm_call_hyp)
++ENDPROC(__kvm_call_hyp)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0095-arm64-kvm-Move-lr-save-restore-from-do_el2_call-into.patch b/tools/kdump/0095-arm64-kvm-Move-lr-save-restore-from-do_el2_call-into.patch
new file mode 100644
index 0000000..b7f53ff
--- /dev/null
+++ b/tools/kdump/0095-arm64-kvm-Move-lr-save-restore-from-do_el2_call-into.patch
@@ -0,0 +1,100 @@
+From ca748b3834788c363eb004b3b30ed4282bf70738 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:02 +0100
+Subject: [PATCH 095/123] arm64: kvm: Move lr save/restore from do_el2_call
+ into EL1
+
+Today the 'hvc' calling KVM or the hyp-stub is expected to preserve all
+registers. KVM saves/restores the registers it needs on the EL2 stack using
+do_el2_call(). The hyp-stub has no stack, later patches need to be able to
+be able to clobber the link register.
+
+Move the link register save/restore to the the call sites.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 00a44cdaba0900c63a003e0c431f506f49376a90)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/kernel/hyp-stub.S   | 10 ++++++++--
+ arch/arm64/kvm/hyp.S           |  7 ++++++-
+ arch/arm64/kvm/hyp/hyp-entry.S |  6 ++----
+ 3 files changed, 16 insertions(+), 7 deletions(-)
+
+diff --git a/arch/arm64/kernel/hyp-stub.S b/arch/arm64/kernel/hyp-stub.S
+index a272f33..7eab8ac 100644
+--- a/arch/arm64/kernel/hyp-stub.S
++++ b/arch/arm64/kernel/hyp-stub.S
+@@ -101,10 +101,16 @@ ENDPROC(\label)
+  */
+ 
+ ENTRY(__hyp_get_vectors)
++	str	lr, [sp, #-16]!
+ 	mov	x0, xzr
+-	// fall through
+-ENTRY(__hyp_set_vectors)
+ 	hvc	#0
++	ldr	lr, [sp], #16
+ 	ret
+ ENDPROC(__hyp_get_vectors)
++
++ENTRY(__hyp_set_vectors)
++	str	lr, [sp, #-16]!
++	hvc	#0
++	ldr	lr, [sp], #16
++	ret
+ ENDPROC(__hyp_set_vectors)
+diff --git a/arch/arm64/kvm/hyp.S b/arch/arm64/kvm/hyp.S
+index 48f19a3..4ee5612 100644
+--- a/arch/arm64/kvm/hyp.S
++++ b/arch/arm64/kvm/hyp.S
+@@ -38,13 +38,18 @@
+  * A function pointer with a value of 0 has a special meaning, and is
+  * used to implement __hyp_get_vectors in the same way as in
+  * arch/arm64/kernel/hyp_stub.S.
++ * HVC behaves as a 'bl' call and will clobber lr.
+  */
+ ENTRY(__kvm_call_hyp)
+-alternative_if_not ARM64_HAS_VIRT_HOST_EXTN	
++alternative_if_not ARM64_HAS_VIRT_HOST_EXTN
++	str     lr, [sp, #-16]!
+ 	hvc	#0
++	ldr     lr, [sp], #16
+ 	ret
+ alternative_else
+ 	b	__vhe_hyp_call
+ 	nop
++	nop
++	nop
+ alternative_endif
+ ENDPROC(__kvm_call_hyp)
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+index 1bdeee7..ca8a8ea 100644
+--- a/arch/arm64/kvm/hyp/hyp-entry.S
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -43,19 +43,17 @@
+ 	 * Shuffle the parameters before calling the function
+ 	 * pointed to in x0. Assumes parameters in x[1,2,3].
+ 	 */
+-	sub	sp, sp, #16
+-	str	lr, [sp]
+ 	mov	lr, x0
+ 	mov	x0, x1
+ 	mov	x1, x2
+ 	mov	x2, x3
+ 	blr	lr
+-	ldr	lr, [sp]
+-	add	sp, sp, #16
+ .endm
+ 
+ ENTRY(__vhe_hyp_call)
++	str	lr, [sp, #-16]!
+ 	do_el2_call
++	ldr	lr, [sp], #16
+ 	/*
+ 	 * We used to rely on having an exception return to get
+ 	 * an implicit isb. In the E2H case, we don't have it anymore.
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0096-arm64-hyp-kvm-Make-hyp-stub-extensible.patch b/tools/kdump/0096-arm64-hyp-kvm-Make-hyp-stub-extensible.patch
new file mode 100644
index 0000000..b1f4443
--- /dev/null
+++ b/tools/kdump/0096-arm64-hyp-kvm-Make-hyp-stub-extensible.patch
@@ -0,0 +1,166 @@
+From 20cf833b5db0c4ced37549ba748e2bd6f4211894 Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Wed, 27 Apr 2016 17:47:03 +0100
+Subject: [PATCH 096/123] arm64: hyp/kvm: Make hyp-stub extensible
+
+The existing arm64 hcall implementations are limited in that they only
+allow for two distinct hcalls; with the x0 register either zero or not
+zero.  Also, the API of the hyp-stub exception vector routines and the
+KVM exception vector routines differ; hyp-stub uses a non-zero value in
+x0 to implement __hyp_set_vectors, whereas KVM uses it to implement
+kvm_call_hyp.
+
+To allow for additional hcalls to be defined and to make the arm64 hcall
+API more consistent across exception vector routines, change the hcall
+implementations to reserve all x0 values below 0xfff for hcalls such
+as {s,g}et_vectors().
+
+Define two new preprocessor macros HVC_GET_VECTORS, and HVC_SET_VECTORS
+to be used as hcall type specifiers and convert the existing
+__hyp_get_vectors() and __hyp_set_vectors() routines to use these new
+macros when executing an HVC call.  Also, change the corresponding
+hyp-stub and KVM el1_sync exception vector routines to use these new
+macros.
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+[Merged two hcall patches, moved immediate value from esr to x0, use lr
+ as a scratch register, changed limit to 0xfff]
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+
+(cherry picked from commit ad72e59ff2bad55f6b9e7ac1fe5d824831ea2550)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/virt.h  | 16 ++++++++++++++++
+ arch/arm64/kernel/hyp-stub.S   | 34 ++++++++++++++++++++++++----------
+ arch/arm64/kvm/hyp.S           |  4 ++--
+ arch/arm64/kvm/hyp/hyp-entry.S |  4 ++--
+ 4 files changed, 44 insertions(+), 14 deletions(-)
+
+diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
+index 9f22dd6..06e6a523 100644
+--- a/arch/arm64/include/asm/virt.h
++++ b/arch/arm64/include/asm/virt.h
+@@ -18,6 +18,22 @@
+ #ifndef __ASM__VIRT_H
+ #define __ASM__VIRT_H
+ 
++/*
++ * The arm64 hcall implementation uses x0 to specify the hcall type. A value
++ * less than 0xfff indicates a special hcall, such as get/set vector.
++ * Any other value is used as a pointer to the function to call.
++ */
++
++/* HVC_GET_VECTORS - Return the value of the vbar_el2 register. */
++#define HVC_GET_VECTORS 0
++
++/*
++ * HVC_SET_VECTORS - Set the value of the vbar_el2 register.
++ *
++ * @x1: Physical address of the new vector table.
++ */
++#define HVC_SET_VECTORS 1
++
+ #define BOOT_CPU_MODE_EL1	(0xe11)
+ #define BOOT_CPU_MODE_EL2	(0xe12)
+ 
+diff --git a/arch/arm64/kernel/hyp-stub.S b/arch/arm64/kernel/hyp-stub.S
+index 7eab8ac..894fb40 100644
+--- a/arch/arm64/kernel/hyp-stub.S
++++ b/arch/arm64/kernel/hyp-stub.S
+@@ -22,6 +22,7 @@
+ #include <linux/irqchip/arm-gic-v3.h>
+ 
+ #include <asm/assembler.h>
++#include <asm/kvm_arm.h>
+ #include <asm/ptrace.h>
+ #include <asm/virt.h>
+ 
+@@ -53,15 +54,26 @@ ENDPROC(__hyp_stub_vectors)
+ 	.align 11
+ 
+ el1_sync:
+-	mrs	x1, esr_el2
+-	lsr	x1, x1, #26
+-	cmp	x1, #0x16
+-	b.ne	2f				// Not an HVC trap
+-	cbz	x0, 1f
+-	msr	vbar_el2, x0			// Set vbar_el2
+-	b	2f
+-1:	mrs	x0, vbar_el2			// Return vbar_el2
+-2:	eret
++	mrs	x30, esr_el2
++	lsr	x30, x30, #ESR_ELx_EC_SHIFT
++
++	cmp	x30, #ESR_ELx_EC_HVC64
++	b.ne	9f				// Not an HVC trap
++
++	cmp	x0, #HVC_GET_VECTORS
++	b.ne	1f
++	mrs	x0, vbar_el2
++	b	9f
++
++1:	cmp	x0, #HVC_SET_VECTORS
++	b.ne	2f
++	msr	vbar_el2, x1
++	b	9f
++
++	/* Unrecognised call type */
++2:	mov     x0, xzr
++
++9:	eret
+ ENDPROC(el1_sync)
+ 
+ .macro invalid_vector	label
+@@ -102,7 +114,7 @@ ENDPROC(\label)
+ 
+ ENTRY(__hyp_get_vectors)
+ 	str	lr, [sp, #-16]!
+-	mov	x0, xzr
++	mov	x0, #HVC_GET_VECTORS
+ 	hvc	#0
+ 	ldr	lr, [sp], #16
+ 	ret
+@@ -110,6 +122,8 @@ ENDPROC(__hyp_get_vectors)
+ 
+ ENTRY(__hyp_set_vectors)
+ 	str	lr, [sp, #-16]!
++	mov	x1, x0
++	mov	x0, #HVC_SET_VECTORS
+ 	hvc	#0
+ 	ldr	lr, [sp], #16
+ 	ret
+diff --git a/arch/arm64/kvm/hyp.S b/arch/arm64/kvm/hyp.S
+index 4ee5612..7ce9315 100644
+--- a/arch/arm64/kvm/hyp.S
++++ b/arch/arm64/kvm/hyp.S
+@@ -35,8 +35,8 @@
+  * in Hyp mode (see init_hyp_mode in arch/arm/kvm/arm.c).  Return values are
+  * passed in x0.
+  *
+- * A function pointer with a value of 0 has a special meaning, and is
+- * used to implement __hyp_get_vectors in the same way as in
++ * A function pointer with a value less than 0xfff has a special meaning,
++ * and is used to implement __hyp_get_vectors in the same way as in
+  * arch/arm64/kernel/hyp_stub.S.
+  * HVC behaves as a 'bl' call and will clobber lr.
+  */
+diff --git a/arch/arm64/kvm/hyp/hyp-entry.S b/arch/arm64/kvm/hyp/hyp-entry.S
+index ca8a8ea..44c79fd 100644
+--- a/arch/arm64/kvm/hyp/hyp-entry.S
++++ b/arch/arm64/kvm/hyp/hyp-entry.S
+@@ -79,8 +79,8 @@ el1_sync:				// Guest trapped into EL2
+ 	/* Here, we're pretty sure the host called HVC. */
+ 	restore_x0_to_x3
+ 
+-	/* Check for __hyp_get_vectors */
+-	cbnz	x0, 1f
++	cmp	x0, #HVC_GET_VECTORS
++	b.ne	1f
+ 	mrs	x0, vbar_el2
+ 	b	2f
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0097-arm64-hyp-kvm-Make-hyp-stub-reject-kvm_call_hyp.patch b/tools/kdump/0097-arm64-hyp-kvm-Make-hyp-stub-reject-kvm_call_hyp.patch
new file mode 100644
index 0000000..5737cf2
--- /dev/null
+++ b/tools/kdump/0097-arm64-hyp-kvm-Make-hyp-stub-reject-kvm_call_hyp.patch
@@ -0,0 +1,85 @@
+From 31f22f216d1e6291395bf397e8a6f6b97a195644 Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 27 Apr 2016 17:47:04 +0100
+Subject: [PATCH 097/123] arm64: hyp/kvm: Make hyp-stub reject kvm_call_hyp()
+
+A later patch implements kvm_arch_hardware_disable(), to remove kvm
+from el2, and re-instate the hyp-stub.
+
+This can happen while guests are running, particularly when kvm_reboot()
+calls kvm_arch_hardware_disable() on each cpu. This can interrupt a guest,
+remove kvm, then allow the guest to be scheduled again. This causes
+kvm_call_hyp() to be run against the hyp-stub.
+
+Change the hyp-stub to return a new exception type when this happens,
+and add code to kvm's handle_exit() to tell userspace we failed to
+enter the guest.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit c94b0cf28281d483c8b43b4874fcb7ab14ade1b1)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm64/include/asm/kvm_asm.h | 2 ++
+ arch/arm64/kernel/hyp-stub.S     | 5 +++--
+ arch/arm64/kvm/handle_exit.c     | 7 +++++++
+ 3 files changed, 12 insertions(+), 2 deletions(-)
+
+diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
+index 31b5600..edb51b8 100644
+--- a/arch/arm64/include/asm/kvm_asm.h
++++ b/arch/arm64/include/asm/kvm_asm.h
+@@ -22,6 +22,8 @@
+ 
+ #define ARM_EXCEPTION_IRQ	  0
+ #define ARM_EXCEPTION_TRAP	  1
++/* The hyp-stub will return this for any kvm_call_hyp() call */
++#define ARM_EXCEPTION_HYP_GONE	  2
+ 
+ #define KVM_ARM64_DEBUG_DIRTY_SHIFT	0
+ #define KVM_ARM64_DEBUG_DIRTY		(1 << KVM_ARM64_DEBUG_DIRTY_SHIFT)
+diff --git a/arch/arm64/kernel/hyp-stub.S b/arch/arm64/kernel/hyp-stub.S
+index 894fb40..8727f44 100644
+--- a/arch/arm64/kernel/hyp-stub.S
++++ b/arch/arm64/kernel/hyp-stub.S
+@@ -23,6 +23,7 @@
+ 
+ #include <asm/assembler.h>
+ #include <asm/kvm_arm.h>
++#include <asm/kvm_asm.h>
+ #include <asm/ptrace.h>
+ #include <asm/virt.h>
+ 
+@@ -70,8 +71,8 @@ el1_sync:
+ 	msr	vbar_el2, x1
+ 	b	9f
+ 
+-	/* Unrecognised call type */
+-2:	mov     x0, xzr
++	/* Someone called kvm_call_hyp() against the hyp-stub... */
++2:	mov     x0, #ARM_EXCEPTION_HYP_GONE
+ 
+ 9:	eret
+ ENDPROC(el1_sync)
+diff --git a/arch/arm64/kvm/handle_exit.c b/arch/arm64/kvm/handle_exit.c
+index 198cf10..25006a7 100644
+--- a/arch/arm64/kvm/handle_exit.c
++++ b/arch/arm64/kvm/handle_exit.c
+@@ -183,6 +183,13 @@ int handle_exit(struct kvm_vcpu *vcpu, struct kvm_run *run,
+ 		exit_handler = kvm_get_exit_handler(vcpu);
+ 
+ 		return exit_handler(vcpu, run);
++	case ARM_EXCEPTION_HYP_GONE:
++		/*
++		 * EL2 has been reset to the hyp-stub. This happens when a guest
++		 * is pre-empted by kvm_reboot()'s shutdown call.
++		 */
++		run->exit_reason = KVM_EXIT_FAIL_ENTRY;
++		return 0;
+ 	default:
+ 		kvm_pr_unimpl("Unsupported exception type: %d",
+ 			      exception_index);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0098-arm64-KVM-Skip-HYP-setup-when-already-running-in-HYP.patch b/tools/kdump/0098-arm64-KVM-Skip-HYP-setup-when-already-running-in-HYP.patch
new file mode 100644
index 0000000..1bc2102
--- /dev/null
+++ b/tools/kdump/0098-arm64-KVM-Skip-HYP-setup-when-already-running-in-HYP.patch
@@ -0,0 +1,318 @@
+From 48920dcd581e588d6c25171c4cf14b9e43ee8b34 Mon Sep 17 00:00:00 2001
+From: Marc Zyngier <marc.zyngier@arm.com>
+Date: Thu, 29 Jan 2015 11:59:54 +0000
+Subject: [PATCH 098/123] arm64: KVM: Skip HYP setup when already running in
+ HYP
+
+With the kernel running at EL2, there is no point trying to
+configure page tables for HYP, as the kernel is already mapped.
+
+Take this opportunity to refactor the whole init a bit, allowing
+the various parts of the hypervisor bringup to be split across
+multiple functions.
+
+Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
+Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>
+(cherry picked from commit 1e947bad0b63b351cbdd9ad55ea5bf7e31c76036)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/kvm/arm.c | 173 +++++++++++++++++++++++++++++++++++------------------
+ arch/arm/kvm/mmu.c |   7 +++
+ 2 files changed, 121 insertions(+), 59 deletions(-)
+
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index 7a81332..aed9348 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -964,6 +964,11 @@ long kvm_arch_vm_ioctl(struct file *filp,
+ 	}
+ }
+ 
++static void cpu_init_stage2(void *dummy)
++{
++	__cpu_init_stage2();
++}
++
+ static void cpu_init_hyp_mode(void *dummy)
+ {
+ 	phys_addr_t boot_pgd_ptr;
+@@ -1033,6 +1038,82 @@ static inline void hyp_cpu_pm_init(void)
+ }
+ #endif
+ 
++static void teardown_common_resources(void)
++{
++	free_percpu(kvm_host_cpu_state);
++}
++
++static int init_common_resources(void)
++{
++	kvm_host_cpu_state = alloc_percpu(kvm_cpu_context_t);
++	if (!kvm_host_cpu_state) {
++		kvm_err("Cannot allocate host CPU state\n");
++		return -ENOMEM;
++	}
++
++	return 0;
++}
++
++static int init_subsystems(void)
++{
++	int err;
++
++	/*
++	 * Init HYP view of VGIC
++	 */
++	err = kvm_vgic_hyp_init();
++	switch (err) {
++	case 0:
++		vgic_present = true;
++		break;
++	case -ENODEV:
++	case -ENXIO:
++		vgic_present = false;
++		break;
++	default:
++		return err;
++	}
++
++	/*
++	 * Init HYP architected timer support
++	 */
++	err = kvm_timer_hyp_init();
++	if (err)
++		return err;
++
++	kvm_perf_init();
++	kvm_coproc_table_init();
++
++	return 0;
++}
++
++static void teardown_hyp_mode(void)
++{
++	int cpu;
++
++	if (is_kernel_in_hyp_mode())
++		return;
++
++	free_hyp_pgds();
++	for_each_possible_cpu(cpu)
++		free_page(per_cpu(kvm_arm_hyp_stack_page, cpu));
++}
++
++static int init_vhe_mode(void)
++{
++	/*
++	 * Execute the init code on each CPU.
++	 */
++	on_each_cpu(cpu_init_stage2, NULL, 1);
++
++	/* set size of VMID supported by CPU */
++	kvm_vmid_bits = kvm_get_vmid_bits();
++	kvm_info("%d-bit VMID\n", kvm_vmid_bits);
++
++	kvm_info("VHE mode initialized successfully\n");
++	return 0;
++}
++
+ /**
+  * Inits Hyp-mode on all online CPUs
+  */
+@@ -1063,7 +1144,7 @@ static int init_hyp_mode(void)
+ 		stack_page = __get_free_page(GFP_KERNEL);
+ 		if (!stack_page) {
+ 			err = -ENOMEM;
+-			goto out_free_stack_pages;
++			goto out_err;
+ 		}
+ 
+ 		per_cpu(kvm_arm_hyp_stack_page, cpu) = stack_page;
+@@ -1076,14 +1157,14 @@ static int init_hyp_mode(void)
+ 				  kvm_ksym_ref(__kvm_hyp_code_end));
+ 	if (err) {
+ 		kvm_err("Cannot map world-switch code\n");
+-		goto out_free_mappings;
++		goto out_err;
+ 	}
+ 
+ 	err = create_hyp_mappings(kvm_ksym_ref(__start_rodata),
+ 				  kvm_ksym_ref(__end_rodata));
+ 	if (err) {
+ 		kvm_err("Cannot map rodata section\n");
+-		goto out_free_mappings;
++		goto out_err;
+ 	}
+ 
+ 	/*
+@@ -1095,20 +1176,10 @@ static int init_hyp_mode(void)
+ 
+ 		if (err) {
+ 			kvm_err("Cannot map hyp stack\n");
+-			goto out_free_mappings;
++			goto out_err;
+ 		}
+ 	}
+ 
+-	/*
+-	 * Map the host CPU structures
+-	 */
+-	kvm_host_cpu_state = alloc_percpu(kvm_cpu_context_t);
+-	if (!kvm_host_cpu_state) {
+-		err = -ENOMEM;
+-		kvm_err("Cannot allocate host CPU state\n");
+-		goto out_free_mappings;
+-	}
+-
+ 	for_each_possible_cpu(cpu) {
+ 		kvm_cpu_context_t *cpu_ctxt;
+ 
+@@ -1117,7 +1188,7 @@ static int init_hyp_mode(void)
+ 
+ 		if (err) {
+ 			kvm_err("Cannot map host CPU state: %d\n", err);
+-			goto out_free_context;
++			goto out_err;
+ 		}
+ 	}
+ 
+@@ -1126,34 +1197,22 @@ static int init_hyp_mode(void)
+ 	 */
+ 	on_each_cpu(cpu_init_hyp_mode, NULL, 1);
+ 
+-	/*
+-	 * Init HYP view of VGIC
+-	 */
+-	err = kvm_vgic_hyp_init();
+-	switch (err) {
+-	case 0:
+-		vgic_present = true;
+-		break;
+-	case -ENODEV:
+-	case -ENXIO:
+-		vgic_present = false;
+-		break;
+-	default:
+-		goto out_free_context;
+-	}
+-
+-	/*
+-	 * Init HYP architected timer support
+-	 */
+-	err = kvm_timer_hyp_init();
+-	if (err)
+-		goto out_free_context;
+-
+ #ifndef CONFIG_HOTPLUG_CPU
+ 	free_boot_hyp_pgd();
+ #endif
+ 
+-	kvm_perf_init();
++	cpu_notifier_register_begin();
++
++	err = __register_cpu_notifier(&hyp_init_cpu_nb);
++
++	cpu_notifier_register_done();
++
++	if (err) {
++		kvm_err("Cannot register HYP init CPU notifier (%d)\n", err);
++		goto out_err;
++	}
++
++	hyp_cpu_pm_init();
+ 
+ 	/* set size of VMID supported by CPU */
+ 	kvm_vmid_bits = kvm_get_vmid_bits();
+@@ -1162,14 +1221,9 @@ static int init_hyp_mode(void)
+ 	kvm_info("Hyp mode initialized successfully\n");
+ 
+ 	return 0;
+-out_free_context:
+-	free_percpu(kvm_host_cpu_state);
+-out_free_mappings:
+-	free_hyp_pgds();
+-out_free_stack_pages:
+-	for_each_possible_cpu(cpu)
+-		free_page(per_cpu(kvm_arm_hyp_stack_page, cpu));
++
+ out_err:
++	teardown_hyp_mode();
+ 	kvm_err("error initializing Hyp mode: %d\n", err);
+ 	return err;
+ }
+@@ -1213,26 +1267,27 @@ int kvm_arch_init(void *opaque)
+ 		}
+ 	}
+ 
+-	cpu_notifier_register_begin();
+-
+-	err = init_hyp_mode();
++	err = init_common_resources();
+ 	if (err)
+-		goto out_err;
++		return err;
+ 
+-	err = __register_cpu_notifier(&hyp_init_cpu_nb);
+-	if (err) {
+-		kvm_err("Cannot register HYP init CPU notifier (%d)\n", err);
++	if (is_kernel_in_hyp_mode())
++		err = init_vhe_mode();
++	else
++		err = init_hyp_mode();
++	if (err)
+ 		goto out_err;
+-	}
+-
+-	cpu_notifier_register_done();
+ 
+-	hyp_cpu_pm_init();
++	err = init_subsystems();
++	if (err)
++		goto out_hyp;
+ 
+-	kvm_coproc_table_init();
+ 	return 0;
++
++out_hyp:
++	teardown_hyp_mode();
+ out_err:
+-	cpu_notifier_register_done();
++	teardown_common_resources();
+ 	return err;
+ }
+ 
+diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c
+index e2b6801..bf3697c 100644
+--- a/arch/arm/kvm/mmu.c
++++ b/arch/arm/kvm/mmu.c
+@@ -28,6 +28,7 @@
+ #include <asm/kvm_mmio.h>
+ #include <asm/kvm_asm.h>
+ #include <asm/kvm_emulate.h>
++#include <asm/virt.h>
+ 
+ #include "trace.h"
+ 
+@@ -598,6 +599,9 @@ int create_hyp_mappings(void *from, void *to)
+ 	unsigned long start = KERN_TO_HYP((unsigned long)from);
+ 	unsigned long end = KERN_TO_HYP((unsigned long)to);
+ 
++	if (is_kernel_in_hyp_mode())
++		return 0;
++
+ 	start = start & PAGE_MASK;
+ 	end = PAGE_ALIGN(end);
+ 
+@@ -630,6 +634,9 @@ int create_hyp_io_mappings(void *from, void *to, phys_addr_t phys_addr)
+ 	unsigned long start = KERN_TO_HYP((unsigned long)from);
+ 	unsigned long end = KERN_TO_HYP((unsigned long)to);
+ 
++	if (is_kernel_in_hyp_mode())
++		return 0;
++
+ 	/* Check for a valid kernel IO mapping */
+ 	if (!is_vmalloc_addr(from) || !is_vmalloc_addr(to - 1))
+ 		return -EINVAL;
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0099-arm64-KVM-Register-CPU-notifiers-when-the-kernel-run.patch b/tools/kdump/0099-arm64-KVM-Register-CPU-notifiers-when-the-kernel-run.patch
new file mode 100644
index 0000000..a0806af
--- /dev/null
+++ b/tools/kdump/0099-arm64-KVM-Register-CPU-notifiers-when-the-kernel-run.patch
@@ -0,0 +1,119 @@
+From 1e8b48038e9e3533646e6ff2d7e8cfe3fc2792ca Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 30 Mar 2016 18:33:04 +0100
+Subject: [PATCH 099/123] arm64: KVM: Register CPU notifiers when the kernel
+ runs at HYP
+
+When the kernel is running at EL2, it doesn't need init_hyp_mode() to
+configure page tables for HYP. This function also registers the CPU
+hotplug and lower power notifiers that cause HYP to be re-initialised
+after the CPU has been reset.
+
+To avoid losing the register state that controls stage2 translation, move
+the registering of these notifiers into init_subsystems(), and add a
+is_kernel_in_hyp_mode() path to each callback.
+
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
+Fixes: 1e947bad0b6 ("arm64: KVM: Skip HYP setup when already running in HYP")
+Signed-off-by: James Morse <james.morse@arm.com>
+Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
+(cherry picked from commit 5f5560b1c5f3a80e91c6babb2da34a51943bbdec)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+---
+ arch/arm/kvm/arm.c | 52 +++++++++++++++++++++++++++++++++-------------------
+ 1 file changed, 33 insertions(+), 19 deletions(-)
+
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index aed9348..99b2703 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -992,15 +992,27 @@ static void cpu_init_hyp_mode(void *dummy)
+ 	kvm_arm_init_debug();
+ }
+ 
++static void cpu_hyp_reinit(void)
++{
++	if (is_kernel_in_hyp_mode()) {
++		/*
++		 * cpu_init_stage2() is safe to call even if the PM
++		 * event was cancelled before the CPU was reset.
++		 */
++		cpu_init_stage2(NULL);
++	} else {
++		if (__hyp_get_vectors() == hyp_default_vectors)
++			cpu_init_hyp_mode(NULL);
++	}
++}
++
+ static int hyp_init_cpu_notify(struct notifier_block *self,
+ 			       unsigned long action, void *cpu)
+ {
+ 	switch (action) {
+ 	case CPU_STARTING:
+ 	case CPU_STARTING_FROZEN:
+-		if (__hyp_get_vectors() == hyp_default_vectors)
+-			cpu_init_hyp_mode(NULL);
+-		break;
++		cpu_hyp_reinit();
+ 	}
+ 
+ 	return NOTIFY_OK;
+@@ -1015,9 +1027,8 @@ static int hyp_init_cpu_pm_notifier(struct notifier_block *self,
+ 				    unsigned long cmd,
+ 				    void *v)
+ {
+-	if (cmd == CPU_PM_EXIT &&
+-	    __hyp_get_vectors() == hyp_default_vectors) {
+-		cpu_init_hyp_mode(NULL);
++	if (cmd == CPU_PM_EXIT) {
++		cpu_hyp_reinit();
+ 		return NOTIFY_OK;
+ 	}
+ 
+@@ -1059,6 +1070,22 @@ static int init_subsystems(void)
+ 	int err;
+ 
+ 	/*
++	 * Register CPU Hotplug notifier
++	 */
++	cpu_notifier_register_begin();
++	err = __register_cpu_notifier(&hyp_init_cpu_nb);
++	cpu_notifier_register_done();
++	if (err) {
++		kvm_err("Cannot register KVM init CPU notifier (%d)\n", err);
++		return err;
++	}
++
++	/*
++	 * Register CPU lower-power notifier
++	 */
++	hyp_cpu_pm_init();
++
++	/*
+ 	 * Init HYP view of VGIC
+ 	 */
+ 	err = kvm_vgic_hyp_init();
+@@ -1201,19 +1228,6 @@ static int init_hyp_mode(void)
+ 	free_boot_hyp_pgd();
+ #endif
+ 
+-	cpu_notifier_register_begin();
+-
+-	err = __register_cpu_notifier(&hyp_init_cpu_nb);
+-
+-	cpu_notifier_register_done();
+-
+-	if (err) {
+-		kvm_err("Cannot register HYP init CPU notifier (%d)\n", err);
+-		goto out_err;
+-	}
+-
+-	hyp_cpu_pm_init();
+-
+ 	/* set size of VMID supported by CPU */
+ 	kvm_vmid_bits = kvm_get_vmid_bits();
+ 	kvm_info("%d-bit VMID\n", kvm_vmid_bits);
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0100-arm64-kvm-allows-kvm-cpu-hotplug.patch b/tools/kdump/0100-arm64-kvm-allows-kvm-cpu-hotplug.patch
new file mode 100644
index 0000000..b11904f
--- /dev/null
+++ b/tools/kdump/0100-arm64-kvm-allows-kvm-cpu-hotplug.patch
@@ -0,0 +1,461 @@
+From f7c5e57e8f87ea9911556b1ed8a0567a82a475c0 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Wed, 27 Apr 2016 17:47:05 +0100
+Subject: [PATCH 100/123] arm64: kvm: allows kvm cpu hotplug
+
+The current kvm implementation on arm64 does cpu-specific initialization
+at system boot, and has no way to gracefully shutdown a core in terms of
+kvm. This prevents kexec from rebooting the system at EL2.
+
+This patch adds a cpu tear-down function and also puts an existing cpu-init
+code into a separate function, kvm_arch_hardware_disable() and
+kvm_arch_hardware_enable() respectively.
+We don't need the arm64 specific cpu hotplug hook any more.
+
+Since this patch modifies common code between arm and arm64, one stub
+definition, __cpu_reset_hyp_mode(), is added on arm side to avoid
+compilation errors.
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+[Rebase, added separate VHE init/exit path, changed resets use of
+ kvm_call_hyp() to the __version, en/disabled hardware in init_subsystems(),
+ added icache maintenance to __kvm_hyp_reset() and removed lr restore, removed
+ guest-enter after teardown handling]
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+
+(cherry picked from commit 67f6919766620e7ea7aab11a6a3470dc7b451359)
+Signed-off-by: Alex Shi <alex.shi@linaro.org>
+
+Conflicts:
+	arch/arm64/include/asm/kvm_host.h
+---
+ arch/arm/include/asm/kvm_host.h   |  10 +++-
+ arch/arm/include/asm/kvm_mmu.h    |   1 +
+ arch/arm/kvm/arm.c                | 119 +++++++++++++++++++++++---------------
+ arch/arm/kvm/mmu.c                |   5 ++
+ arch/arm64/include/asm/kvm_asm.h  |   1 +
+ arch/arm64/include/asm/kvm_host.h |  13 ++++-
+ arch/arm64/include/asm/kvm_mmu.h  |   1 +
+ arch/arm64/kvm/hyp-init.S         |  38 ++++++++++++
+ arch/arm64/kvm/reset.c            |  14 +++++
+ 9 files changed, 152 insertions(+), 50 deletions(-)
+
+diff --git a/arch/arm/include/asm/kvm_host.h b/arch/arm/include/asm/kvm_host.h
+index 945bfa5..bedaf65 100644
+--- a/arch/arm/include/asm/kvm_host.h
++++ b/arch/arm/include/asm/kvm_host.h
+@@ -218,6 +218,15 @@ static inline void __cpu_init_stage2(void)
+ {
+ }
+ 
++static inline void __cpu_reset_hyp_mode(phys_addr_t boot_pgd_ptr,
++					phys_addr_t phys_idmap_start)
++{
++	/*
++	 * TODO
++	 * kvm_call_reset(boot_pgd_ptr, phys_idmap_start);
++	 */
++}
++
+ static inline int kvm_arch_dev_ioctl_check_extension(long ext)
+ {
+ 	return 0;
+@@ -230,7 +239,6 @@ void kvm_mmu_wp_memory_region(struct kvm *kvm, int slot);
+ 
+ struct kvm_vcpu *kvm_mpidr_to_vcpu(struct kvm *kvm, unsigned long mpidr);
+ 
+-static inline void kvm_arch_hardware_disable(void) {}
+ static inline void kvm_arch_hardware_unsetup(void) {}
+ static inline void kvm_arch_sync_events(struct kvm *kvm) {}
+ static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}
+diff --git a/arch/arm/include/asm/kvm_mmu.h b/arch/arm/include/asm/kvm_mmu.h
+index 9203c21..c7ba9a4 100644
+--- a/arch/arm/include/asm/kvm_mmu.h
++++ b/arch/arm/include/asm/kvm_mmu.h
+@@ -66,6 +66,7 @@ void kvm_mmu_free_memory_caches(struct kvm_vcpu *vcpu);
+ phys_addr_t kvm_mmu_get_httbr(void);
+ phys_addr_t kvm_mmu_get_boot_httbr(void);
+ phys_addr_t kvm_get_idmap_vector(void);
++phys_addr_t kvm_get_idmap_start(void);
+ int kvm_mmu_init(void);
+ void kvm_clear_hyp_idmap(void);
+ 
+diff --git a/arch/arm/kvm/arm.c b/arch/arm/kvm/arm.c
+index 99b2703..4cddf20 100644
+--- a/arch/arm/kvm/arm.c
++++ b/arch/arm/kvm/arm.c
+@@ -16,7 +16,6 @@
+  * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+  */
+ 
+-#include <linux/cpu.h>
+ #include <linux/cpu_pm.h>
+ #include <linux/errno.h>
+ #include <linux/err.h>
+@@ -65,6 +64,8 @@ static DEFINE_SPINLOCK(kvm_vmid_lock);
+ 
+ static bool vgic_present;
+ 
++static DEFINE_PER_CPU(unsigned char, kvm_arm_hardware_enabled);
++
+ static void kvm_arm_set_running_vcpu(struct kvm_vcpu *vcpu)
+ {
+ 	BUG_ON(preemptible());
+@@ -89,11 +90,6 @@ struct kvm_vcpu * __percpu *kvm_get_running_vcpus(void)
+ 	return &kvm_arm_running_vcpu;
+ }
+ 
+-int kvm_arch_hardware_enable(void)
+-{
+-	return 0;
+-}
+-
+ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
+ {
+ 	return kvm_vcpu_exiting_guest_mode(vcpu) == IN_GUEST_MODE;
+@@ -964,11 +960,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
+ 	}
+ }
+ 
+-static void cpu_init_stage2(void *dummy)
+-{
+-	__cpu_init_stage2();
+-}
+-
+ static void cpu_init_hyp_mode(void *dummy)
+ {
+ 	phys_addr_t boot_pgd_ptr;
+@@ -996,43 +987,87 @@ static void cpu_hyp_reinit(void)
+ {
+ 	if (is_kernel_in_hyp_mode()) {
+ 		/*
+-		 * cpu_init_stage2() is safe to call even if the PM
++		 * __cpu_init_stage2() is safe to call even if the PM
+ 		 * event was cancelled before the CPU was reset.
+ 		 */
+-		cpu_init_stage2(NULL);
++		__cpu_init_stage2();
+ 	} else {
+ 		if (__hyp_get_vectors() == hyp_default_vectors)
+ 			cpu_init_hyp_mode(NULL);
+ 	}
+ }
+ 
+-static int hyp_init_cpu_notify(struct notifier_block *self,
+-			       unsigned long action, void *cpu)
++static void cpu_hyp_reset(void)
++{
++	phys_addr_t boot_pgd_ptr;
++	phys_addr_t phys_idmap_start;
++
++	if (!is_kernel_in_hyp_mode()) {
++		boot_pgd_ptr = kvm_mmu_get_boot_httbr();
++		phys_idmap_start = kvm_get_idmap_start();
++
++		__cpu_reset_hyp_mode(boot_pgd_ptr, phys_idmap_start);
++	}
++}
++
++static void _kvm_arch_hardware_enable(void *discard)
+ {
+-	switch (action) {
+-	case CPU_STARTING:
+-	case CPU_STARTING_FROZEN:
++	if (!__this_cpu_read(kvm_arm_hardware_enabled)) {
+ 		cpu_hyp_reinit();
++		__this_cpu_write(kvm_arm_hardware_enabled, 1);
+ 	}
++}
++
++int kvm_arch_hardware_enable(void)
++{
++	_kvm_arch_hardware_enable(NULL);
++	return 0;
++}
+ 
+-	return NOTIFY_OK;
++static void _kvm_arch_hardware_disable(void *discard)
++{
++	if (__this_cpu_read(kvm_arm_hardware_enabled)) {
++		cpu_hyp_reset();
++		__this_cpu_write(kvm_arm_hardware_enabled, 0);
++	}
+ }
+ 
+-static struct notifier_block hyp_init_cpu_nb = {
+-	.notifier_call = hyp_init_cpu_notify,
+-};
++void kvm_arch_hardware_disable(void)
++{
++	_kvm_arch_hardware_disable(NULL);
++}
+ 
+ #ifdef CONFIG_CPU_PM
+ static int hyp_init_cpu_pm_notifier(struct notifier_block *self,
+ 				    unsigned long cmd,
+ 				    void *v)
+ {
+-	if (cmd == CPU_PM_EXIT) {
+-		cpu_hyp_reinit();
++	/*
++	 * kvm_arm_hardware_enabled is left with its old value over
++	 * PM_ENTER->PM_EXIT. It is used to indicate PM_EXIT should
++	 * re-enable hyp.
++	 */
++	switch (cmd) {
++	case CPU_PM_ENTER:
++		if (__this_cpu_read(kvm_arm_hardware_enabled))
++			/*
++			 * don't update kvm_arm_hardware_enabled here
++			 * so that the hardware will be re-enabled
++			 * when we resume. See below.
++			 */
++			cpu_hyp_reset();
++
++		return NOTIFY_OK;
++	case CPU_PM_EXIT:
++		if (__this_cpu_read(kvm_arm_hardware_enabled))
++			/* The hardware was enabled before suspend. */
++			cpu_hyp_reinit();
++
+ 		return NOTIFY_OK;
+-	}
+ 
+-	return NOTIFY_DONE;
++	default:
++		return NOTIFY_DONE;
++	}
+ }
+ 
+ static struct notifier_block hyp_init_cpu_pm_nb = {
+@@ -1067,18 +1102,12 @@ static int init_common_resources(void)
+ 
+ static int init_subsystems(void)
+ {
+-	int err;
++	int err = 0;
+ 
+ 	/*
+-	 * Register CPU Hotplug notifier
++	 * Enable hardware so that subsystem initialisation can access EL2.
+ 	 */
+-	cpu_notifier_register_begin();
+-	err = __register_cpu_notifier(&hyp_init_cpu_nb);
+-	cpu_notifier_register_done();
+-	if (err) {
+-		kvm_err("Cannot register KVM init CPU notifier (%d)\n", err);
+-		return err;
+-	}
++	on_each_cpu(_kvm_arch_hardware_enable, NULL, 1);
+ 
+ 	/*
+ 	 * Register CPU lower-power notifier
+@@ -1096,9 +1125,10 @@ static int init_subsystems(void)
+ 	case -ENODEV:
+ 	case -ENXIO:
+ 		vgic_present = false;
++		err = 0;
+ 		break;
+ 	default:
+-		return err;
++		goto out;
+ 	}
+ 
+ 	/*
+@@ -1106,12 +1136,15 @@ static int init_subsystems(void)
+ 	 */
+ 	err = kvm_timer_hyp_init();
+ 	if (err)
+-		return err;
++		goto out;
+ 
+ 	kvm_perf_init();
+ 	kvm_coproc_table_init();
+ 
+-	return 0;
++out:
++	on_each_cpu(_kvm_arch_hardware_disable, NULL, 1);
++
++	return err;
+ }
+ 
+ static void teardown_hyp_mode(void)
+@@ -1128,11 +1161,6 @@ static void teardown_hyp_mode(void)
+ 
+ static int init_vhe_mode(void)
+ {
+-	/*
+-	 * Execute the init code on each CPU.
+-	 */
+-	on_each_cpu(cpu_init_stage2, NULL, 1);
+-
+ 	/* set size of VMID supported by CPU */
+ 	kvm_vmid_bits = kvm_get_vmid_bits();
+ 	kvm_info("%d-bit VMID\n", kvm_vmid_bits);
+@@ -1219,11 +1247,6 @@ static int init_hyp_mode(void)
+ 		}
+ 	}
+ 
+-	/*
+-	 * Execute the init code on each CPU.
+-	 */
+-	on_each_cpu(cpu_init_hyp_mode, NULL, 1);
+-
+ #ifndef CONFIG_HOTPLUG_CPU
+ 	free_boot_hyp_pgd();
+ #endif
+diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c
+index bf3697c..7678724 100644
+--- a/arch/arm/kvm/mmu.c
++++ b/arch/arm/kvm/mmu.c
+@@ -1655,6 +1655,11 @@ phys_addr_t kvm_get_idmap_vector(void)
+ 	return hyp_idmap_vector;
+ }
+ 
++phys_addr_t kvm_get_idmap_start(void)
++{
++	return hyp_idmap_start;
++}
++
+ int kvm_mmu_init(void)
+ {
+ 	int err;
+diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h
+index edb51b8..fca5148 100644
+--- a/arch/arm64/include/asm/kvm_asm.h
++++ b/arch/arm64/include/asm/kvm_asm.h
+@@ -51,6 +51,7 @@ struct kvm_vcpu;
+ 
+ extern char __kvm_hyp_init[];
+ extern char __kvm_hyp_init_end[];
++extern char __kvm_hyp_reset[];
+ 
+ extern char __kvm_hyp_vector[];
+ 
+diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
+index bbdaa56..3be7a7b 100644
+--- a/arch/arm64/include/asm/kvm_host.h
++++ b/arch/arm64/include/asm/kvm_host.h
+@@ -44,6 +44,7 @@
+ int __attribute_const__ kvm_target_cpu(void);
+ int kvm_reset_vcpu(struct kvm_vcpu *vcpu);
+ int kvm_arch_dev_ioctl_check_extension(long ext);
++phys_addr_t kvm_hyp_reset_entry(void);
+ 
+ struct kvm_arch {
+ 	/* The VMID generation used for the virt. memory system */
+@@ -330,7 +331,17 @@ static inline void __cpu_init_stage2(void)
+ {
+ }
+ 
+-static inline void kvm_arch_hardware_disable(void) {}
++static inline void __cpu_reset_hyp_mode(phys_addr_t boot_pgd_ptr,
++					phys_addr_t phys_idmap_start)
++{
++	/*
++	 * Call reset code, and switch back to stub hyp vectors.
++	 * Uses __kvm_call_hyp() to avoid kaslr's kvm_ksym_ref() translation.
++	 */
++	__kvm_call_hyp((void *)kvm_hyp_reset_entry(),
++		       boot_pgd_ptr, phys_idmap_start);
++}
++
+ static inline void kvm_arch_hardware_unsetup(void) {}
+ static inline void kvm_arch_sync_events(struct kvm *kvm) {}
+ static inline void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu) {}
+diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h
+index 0bf8b43..342a5ac 100644
+--- a/arch/arm64/include/asm/kvm_mmu.h
++++ b/arch/arm64/include/asm/kvm_mmu.h
+@@ -99,6 +99,7 @@ void kvm_mmu_free_memory_caches(struct kvm_vcpu *vcpu);
+ phys_addr_t kvm_mmu_get_httbr(void);
+ phys_addr_t kvm_mmu_get_boot_httbr(void);
+ phys_addr_t kvm_get_idmap_vector(void);
++phys_addr_t kvm_get_idmap_start(void);
+ int kvm_mmu_init(void);
+ void kvm_clear_hyp_idmap(void);
+ 
+diff --git a/arch/arm64/kvm/hyp-init.S b/arch/arm64/kvm/hyp-init.S
+index 034d152c..d87635e 100644
+--- a/arch/arm64/kvm/hyp-init.S
++++ b/arch/arm64/kvm/hyp-init.S
+@@ -152,6 +152,44 @@ merged:
+ 	eret
+ ENDPROC(__kvm_hyp_init)
+ 
++	/*
++	 * x0: HYP boot pgd
++	 * x1: HYP phys_idmap_start
++	 */
++ENTRY(__kvm_hyp_reset)
++	/* We're in trampoline code in VA, switch back to boot page tables */
++	msr	ttbr0_el2, x0
++	isb
++
++	/* Ensure the PA branch doesn't find a stale tlb entry or stale code. */
++	ic	iallu
++	tlbi	alle2
++	dsb	sy
++	isb
++
++	/* Branch into PA space */
++	adr	x0, 1f
++	bfi	x1, x0, #0, #PAGE_SHIFT
++	br	x1
++
++	/* We're now in idmap, disable MMU */
++1:	mrs	x0, sctlr_el2
++	ldr	x1, =SCTLR_ELx_FLAGS
++	bic	x0, x0, x1		// Clear SCTL_M and etc
++	msr	sctlr_el2, x0
++	isb
++
++	/* Invalidate the old TLBs */
++	tlbi	alle2
++	dsb	sy
++
++	/* Install stub vectors */
++	adr_l	x0, __hyp_stub_vectors
++	msr	vbar_el2, x0
++
++	eret
++ENDPROC(__kvm_hyp_reset)
++
+ 	.ltorg
+ 
+ 	.popsection
+diff --git a/arch/arm64/kvm/reset.c b/arch/arm64/kvm/reset.c
+index f34745c..d6e155a 100644
+--- a/arch/arm64/kvm/reset.c
++++ b/arch/arm64/kvm/reset.c
+@@ -29,7 +29,9 @@
+ #include <asm/cputype.h>
+ #include <asm/ptrace.h>
+ #include <asm/kvm_arm.h>
++#include <asm/kvm_asm.h>
+ #include <asm/kvm_coproc.h>
++#include <asm/kvm_mmu.h>
+ 
+ /*
+  * ARMv8 Reset Values
+@@ -123,3 +125,15 @@ int kvm_reset_vcpu(struct kvm_vcpu *vcpu)
+ 	/* Reset timer */
+ 	return kvm_timer_vcpu_reset(vcpu, cpu_vtimer_irq);
+ }
++
++extern char __hyp_idmap_text_start[];
++
++phys_addr_t kvm_hyp_reset_entry(void)
++{
++	unsigned long offset;
++
++	offset = (unsigned long)__kvm_hyp_reset
++		 - ((unsigned long)__hyp_idmap_text_start & PAGE_MASK);
++
++	return TRAMPOLINE_VA + offset;
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0101-arm64-Add-a-helper-for-parking-CPUs-in-a-loop.patch b/tools/kdump/0101-arm64-Add-a-helper-for-parking-CPUs-in-a-loop.patch
new file mode 100644
index 0000000..7ee7578
--- /dev/null
+++ b/tools/kdump/0101-arm64-Add-a-helper-for-parking-CPUs-in-a-loop.patch
@@ -0,0 +1,54 @@
+From f6e7b201aedd14a1d3921f4ca9e7b76009444351 Mon Sep 17 00:00:00 2001
+From: Suzuki K Poulose <suzuki.poulose@arm.com>
+Date: Tue, 23 Feb 2016 10:31:39 +0000
+Subject: [PATCH 101/123] arm64: Add a helper for parking CPUs in a loop
+
+Adds a routine which can be used to park CPUs (spinning in kernel)
+when they can't be killed.
+
+Cc: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit c4bc34d20273db698c51951a1951dba0a722e162)
+---
+ arch/arm64/include/asm/smp.h   | 8 ++++++++
+ arch/arm64/kernel/cpufeature.c | 5 +----
+ 2 files changed, 9 insertions(+), 4 deletions(-)
+
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index 2013a4d..b93eb33 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -78,4 +78,12 @@ extern int __cpu_disable(void);
+ extern void __cpu_die(unsigned int cpu);
+ extern void cpu_die(void);
+ 
++static inline void cpu_park_loop(void)
++{
++	for (;;) {
++		wfe();
++		wfi();
++	}
++}
++
+ #endif /* ifndef __ASM_SMP_H */
+diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
+index b9c7176..a323cce 100644
+--- a/arch/arm64/kernel/cpufeature.c
++++ b/arch/arm64/kernel/cpufeature.c
+@@ -860,10 +860,7 @@ static void fail_incapable_cpu(char *cap_type,
+ 	/* Check if we can park ourselves */
+ 	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
+ 		cpu_ops[cpu]->cpu_die(cpu);
+-	asm(
+-	"1:	wfe\n"
+-	"	wfi\n"
+-	"	b	1b");
++	cpu_park_loop();
+ }
+ 
+ /*
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0102-arm64-Introduce-cpu_die_early.patch b/tools/kdump/0102-arm64-Introduce-cpu_die_early.patch
new file mode 100644
index 0000000..25e56f1
--- /dev/null
+++ b/tools/kdump/0102-arm64-Introduce-cpu_die_early.patch
@@ -0,0 +1,83 @@
+From dee4930b42dc6222714d2c2c69ed3003c4158d15 Mon Sep 17 00:00:00 2001
+From: Suzuki K Poulose <suzuki.poulose@arm.com>
+Date: Tue, 23 Feb 2016 10:31:40 +0000
+Subject: [PATCH 102/123] arm64: Introduce cpu_die_early
+
+Or in other words, make fail_incapable_cpu() reusable.
+
+We use fail_incapable_cpu() to kill a secondary CPU early during the
+bringup, which doesn't have the system advertised capabilities.
+This patch makes the routine more generic, to kill a secondary
+booting CPU, getting rid of the dependency on capability struct.
+This can be used by checks which are not necessarily attached to
+a capability struct (e.g, cpu ASIDBits).
+
+In that process, renames the function to cpu_die_early() to better
+match its functionality. This will be moved to arch/arm64/kernel/smp.c
+later.
+
+Cc: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit ee02a15919cf86c004142edaa05b43f7ff10edf0)
+---
+ arch/arm64/kernel/cpufeature.c | 24 +++++++++++++++---------
+ 1 file changed, 15 insertions(+), 9 deletions(-)
+
+diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
+index a323cce..0fab3b1 100644
+--- a/arch/arm64/kernel/cpufeature.c
++++ b/arch/arm64/kernel/cpufeature.c
+@@ -845,15 +845,15 @@ static u64 __raw_read_system_reg(u32 sys_id)
+ }
+ 
+ /*
+- * Park the CPU which doesn't have the capability as advertised
+- * by the system.
++ * Kill the calling secondary CPU, early in bringup before it is turned
++ * online.
+  */
+-static void fail_incapable_cpu(char *cap_type,
+-				 const struct arm64_cpu_capabilities *cap)
++void cpu_die_early(void)
+ {
+ 	int cpu = smp_processor_id();
+ 
+-	pr_crit("CPU%d: missing %s : %s\n", cpu, cap_type, cap->desc);
++	pr_crit("CPU%d: will not boot\n", cpu);
++
+ 	/* Mark this CPU absent */
+ 	set_cpu_present(cpu, 0);
+ 
+@@ -891,8 +891,11 @@ void verify_local_cpu_capabilities(void)
+ 		 * If the new CPU misses an advertised feature, we cannot proceed
+ 		 * further, park the cpu.
+ 		 */
+-		if (!feature_matches(__raw_read_system_reg(caps[i].sys_reg), &caps[i]))
+-			fail_incapable_cpu("arm64_features", &caps[i]);
++		if (!feature_matches(__raw_read_system_reg(caps[i].sys_reg), &caps[i])) {
++			pr_crit("CPU%d: missing feature: %s\n",
++					smp_processor_id(), caps[i].desc);
++			cpu_die_early();
++		}
+ 		if (caps[i].enable)
+ 			caps[i].enable(NULL);
+ 	}
+@@ -900,8 +903,11 @@ void verify_local_cpu_capabilities(void)
+ 	for (i = 0, caps = arm64_hwcaps; caps[i].desc; i++) {
+ 		if (!cpus_have_hwcap(&caps[i]))
+ 			continue;
+-		if (!feature_matches(__raw_read_system_reg(caps[i].sys_reg), &caps[i]))
+-			fail_incapable_cpu("arm64_hwcaps", &caps[i]);
++		if (!feature_matches(__raw_read_system_reg(caps[i].sys_reg), &caps[i])) {
++			pr_crit("CPU%d: missing HWCAP: %s\n",
++					smp_processor_id(), caps[i].desc);
++			cpu_die_early();
++		}
+ 	}
+ }
+ 
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0103-arm64-Move-cpu_die_early-to-smp.c.patch b/tools/kdump/0103-arm64-Move-cpu_die_early-to-smp.c.patch
new file mode 100644
index 0000000..09af886
--- /dev/null
+++ b/tools/kdump/0103-arm64-Move-cpu_die_early-to-smp.c.patch
@@ -0,0 +1,98 @@
+From a40abe4f0254ef23b2d1f5b325478ec7040869a4 Mon Sep 17 00:00:00 2001
+From: Suzuki K Poulose <suzuki.poulose@arm.com>
+Date: Tue, 23 Feb 2016 10:31:41 +0000
+Subject: [PATCH 103/123] arm64: Move cpu_die_early to smp.c
+
+This patch moves cpu_die_early to smp.c, where it fits better.
+No functional changes, except for adding the necessary checks
+for CONFIG_HOTPLUG_CPU.
+
+Cc: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit fce6361fe9b0caeba0c05b7d72ceda406f8780df)
+---
+ arch/arm64/include/asm/smp.h   |  1 +
+ arch/arm64/kernel/cpufeature.c | 19 -------------------
+ arch/arm64/kernel/smp.c        | 22 ++++++++++++++++++++++
+ 3 files changed, 23 insertions(+), 19 deletions(-)
+
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index b93eb33..51913be 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -77,6 +77,7 @@ extern int __cpu_disable(void);
+ 
+ extern void __cpu_die(unsigned int cpu);
+ extern void cpu_die(void);
++extern void cpu_die_early(void);
+ 
+ static inline void cpu_park_loop(void)
+ {
+diff --git a/arch/arm64/kernel/cpufeature.c b/arch/arm64/kernel/cpufeature.c
+index 0fab3b1..111e06f 100644
+--- a/arch/arm64/kernel/cpufeature.c
++++ b/arch/arm64/kernel/cpufeature.c
+@@ -845,25 +845,6 @@ static u64 __raw_read_system_reg(u32 sys_id)
+ }
+ 
+ /*
+- * Kill the calling secondary CPU, early in bringup before it is turned
+- * online.
+- */
+-void cpu_die_early(void)
+-{
+-	int cpu = smp_processor_id();
+-
+-	pr_crit("CPU%d: will not boot\n", cpu);
+-
+-	/* Mark this CPU absent */
+-	set_cpu_present(cpu, 0);
+-
+-	/* Check if we can park ourselves */
+-	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
+-		cpu_ops[cpu]->cpu_die(cpu);
+-	cpu_park_loop();
+-}
+-
+-/*
+  * Run through the enabled system capabilities and enable() it on this CPU.
+  * The capabilities were decided based on the available CPUs at the boot time.
+  * Any new CPU should match the system wide status of the capability. If the
+diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
+index a84623d..ee247d4 100644
+--- a/arch/arm64/kernel/smp.c
++++ b/arch/arm64/kernel/smp.c
+@@ -311,6 +311,28 @@ void cpu_die(void)
+ }
+ #endif
+ 
++/*
++ * Kill the calling secondary CPU, early in bringup before it is turned
++ * online.
++ */
++void cpu_die_early(void)
++{
++	int cpu = smp_processor_id();
++
++	pr_crit("CPU%d: will not boot\n", cpu);
++
++	/* Mark this CPU absent */
++	set_cpu_present(cpu, 0);
++
++#ifdef CONFIG_HOTPLUG_CPU
++	/* Check if we can park ourselves */
++	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
++		cpu_ops[cpu]->cpu_die(cpu);
++#endif
++
++	cpu_park_loop();
++}
++
+ static void __init hyp_mode_check(void)
+ {
+ 	if (is_hyp_mode_available())
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0104-arm64-Handle-early-CPU-boot-failures.patch b/tools/kdump/0104-arm64-Handle-early-CPU-boot-failures.patch
new file mode 100644
index 0000000..d2c8c04
--- /dev/null
+++ b/tools/kdump/0104-arm64-Handle-early-CPU-boot-failures.patch
@@ -0,0 +1,313 @@
+From 952e4ffc0dcbdf24b6e9cf32ef436fde32128c2b Mon Sep 17 00:00:00 2001
+From: Suzuki K Poulose <suzuki.poulose@arm.com>
+Date: Tue, 23 Feb 2016 10:31:42 +0000
+Subject: [PATCH 104/123] arm64: Handle early CPU boot failures
+
+A secondary CPU could fail to come online due to insufficient
+capabilities and could simply die or loop in the kernel.
+e.g, a CPU with no support for the selected kernel PAGE_SIZE
+loops in kernel with MMU turned off.
+or a hotplugged CPU which doesn't have one of the advertised
+system capability will die during the activation.
+
+There is no way to synchronise the status of the failing CPU
+back to the master. This patch solves the issue by adding a
+field to the secondary_data which can be updated by the failing
+CPU. If the secondary CPU fails even before turning the MMU on,
+it updates the status in a special variable reserved in the head.txt
+section to make sure that the update can be cache invalidated safely
+without possible sharing of cache write back granule.
+
+Here are the possible states :
+
+ -1. CPU_MMU_OFF - Initial value set by the master CPU, this value
+indicates that the CPU could not turn the MMU on, hence the status
+could not be reliably updated in the secondary_data. Instead, the
+CPU has updated the status @ __early_cpu_boot_status.
+
+ 0. CPU_BOOT_SUCCESS - CPU has booted successfully.
+
+ 1. CPU_KILL_ME - CPU has invoked cpu_ops->die, indicating the
+master CPU to synchronise by issuing a cpu_ops->cpu_kill.
+
+ 2. CPU_STUCK_IN_KERNEL - CPU couldn't invoke die(), instead is
+looping in the kernel. This information could be used by say,
+kexec to check if it is really safe to do a kexec reboot.
+
+ 3. CPU_PANIC_KERNEL - CPU detected some serious issues which
+requires kernel to crash immediately. The secondary CPU cannot
+call panic() until it has initialised the GIC. This flag can
+be used to instruct the master to do so.
+
+Cc: Mark Rutland <mark.rutland@arm.com>
+Acked-by: Will Deacon <will.deacon@arm.com>
+Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
+[catalin.marinas@arm.com: conflict resolution]
+[catalin.marinas@arm.com: converted "status" from int to long]
+[catalin.marinas@arm.com: updated update_early_cpu_boot_status to use str_l]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+
+(cherry picked from commit bb9052744f4b7ae11d0061ac9492dd2949981b49)
+---
+ arch/arm64/include/asm/smp.h    | 28 +++++++++++++++++++++++++
+ arch/arm64/kernel/asm-offsets.c |  2 ++
+ arch/arm64/kernel/head.S        | 43 ++++++++++++++++++++++++++++++++++++++-
+ arch/arm64/kernel/smp.c         | 45 +++++++++++++++++++++++++++++++++++++++++
+ 4 files changed, 117 insertions(+), 1 deletion(-)
+
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index 51913be..817a067 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -16,6 +16,19 @@
+ #ifndef __ASM_SMP_H
+ #define __ASM_SMP_H
+ 
++/* Values for secondary_data.status */
++
++#define CPU_MMU_OFF		(-1)
++#define CPU_BOOT_SUCCESS	(0)
++/* The cpu invoked ops->cpu_die, synchronise it with cpu_kill */
++#define CPU_KILL_ME		(1)
++/* The cpu couldn't die gracefully and is looping in the kernel */
++#define CPU_STUCK_IN_KERNEL	(2)
++/* Fatal system error detected by secondary CPU, crash the system */
++#define CPU_PANIC_KERNEL	(3)
++
++#ifndef __ASSEMBLY__
++
+ #include <linux/threads.h>
+ #include <linux/cpumask.h>
+ #include <linux/thread_info.h>
+@@ -54,11 +67,17 @@ asmlinkage void secondary_start_kernel(void);
+ 
+ /*
+  * Initial data for bringing up a secondary CPU.
++ * @stack  - sp for the secondary CPU
++ * @status - Result passed back from the secondary CPU to
++ *           indicate failure.
+  */
+ struct secondary_data {
+ 	void *stack;
++	long status;
+ };
++
+ extern struct secondary_data secondary_data;
++extern long __early_cpu_boot_status;
+ extern void secondary_entry(void);
+ 
+ extern void arch_send_call_function_single_ipi(int cpu);
+@@ -87,4 +106,13 @@ static inline void cpu_park_loop(void)
+ 	}
+ }
+ 
++static inline void update_cpu_boot_status(int val)
++{
++	WRITE_ONCE(secondary_data.status, val);
++	/* Ensure the visibility of the status update */
++	dsb(ishst);
++}
++
++#endif /* ifndef __ASSEMBLY__ */
++
+ #endif /* ifndef __ASM_SMP_H */
+diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
+index 2bb17bd..a506ddc 100644
+--- a/arch/arm64/kernel/asm-offsets.c
++++ b/arch/arm64/kernel/asm-offsets.c
+@@ -117,6 +117,8 @@ int main(void)
+   DEFINE(TZ_MINWEST,		offsetof(struct timezone, tz_minuteswest));
+   DEFINE(TZ_DSTTIME,		offsetof(struct timezone, tz_dsttime));
+   BLANK();
++  DEFINE(CPU_BOOT_STACK,	offsetof(struct secondary_data, stack));
++  BLANK();
+ #ifdef CONFIG_KVM_ARM_HOST
+   DEFINE(VCPU_CONTEXT,		offsetof(struct kvm_vcpu, arch.ctxt));
+   DEFINE(CPU_GP_REGS,		offsetof(struct kvm_cpu_context, gp_regs));
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index 2f38749..24c2506 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -34,6 +34,7 @@
+ #include <asm/pgtable-hwdef.h>
+ #include <asm/pgtable.h>
+ #include <asm/page.h>
++#include <asm/smp.h>
+ #include <asm/sysreg.h>
+ #include <asm/thread_info.h>
+ #include <asm/virt.h>
+@@ -607,13 +608,45 @@ ENTRY(secondary_startup)
+ ENDPROC(secondary_startup)
+ 
+ ENTRY(__secondary_switched)
++#if 1 /* FIXME */
+ 	ldr	x0, [x21]			// get secondary_data.stack
++#else
++	adr_l	x5, vectors
++	msr	vbar_el1, x5
++	isb
++
++	adr_l	x0, secondary_data
++	ldr	x0, [x0, #CPU_BOOT_STACK]	// get secondary_data.stack
++#endif
+ 	mov	sp, x0
+ 	mov	x29, #0
+ 	b	secondary_start_kernel
+ ENDPROC(__secondary_switched)
+ 
+ /*
++ * The booting CPU updates the failed status @__early_cpu_boot_status,
++ * with MMU turned off.
++ *
++ * update_early_cpu_boot_status tmp, status
++ *  - Corrupts tmp1, tmp2
++ *  - Writes 'status' to __early_cpu_boot_status and makes sure
++ *    it is committed to memory.
++ */
++
++	.macro	update_early_cpu_boot_status status, tmp1, tmp2
++	mov	\tmp2, #\status
++	str_l	\tmp2, __early_cpu_boot_status, \tmp1
++	dmb	sy
++	dc	ivac, \tmp1			// Invalidate potentially stale cache line
++	.endm
++
++	.pushsection	.data..cacheline_aligned
++	.align	L1_CACHE_SHIFT
++ENTRY(__early_cpu_boot_status)
++	.long 	0
++	.popsection
++
++/*
+  * Enable the MMU.
+  *
+  *  x0  = SCTLR_EL1 value for turning on the MMU.
+@@ -630,8 +663,12 @@ ENTRY(__enable_mmu)
+ 	ubfx	x2, x1, #ID_AA64MMFR0_TGRAN_SHIFT, 4
+ 	cmp	x2, #ID_AA64MMFR0_TGRAN_SUPPORTED
+ 	b.ne	__no_granule_support
++#if 1 /* FIXME */
+ 	ldr	x5, =vectors
+ 	msr	vbar_el1, x5
++#else
++	update_early_cpu_boot_status 0, x1, x2
++#endif
+ 	msr	ttbr0_el1, x25			// load TTBR0
+ 	msr	ttbr1_el1, x26			// load TTBR1
+ 	isb
+@@ -649,6 +686,10 @@ ENTRY(__enable_mmu)
+ ENDPROC(__enable_mmu)
+ 
+ __no_granule_support:
++	/* Indicate that this CPU can't boot and is stuck in the kernel */
++	update_early_cpu_boot_status CPU_STUCK_IN_KERNEL, x1, x2
++1:
+ 	wfe
+-	b __no_granule_support
++	wfi
++	b 1b
+ ENDPROC(__no_granule_support)
+diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
+index ee247d4..acabb9b 100644
+--- a/arch/arm64/kernel/smp.c
++++ b/arch/arm64/kernel/smp.c
+@@ -63,6 +63,8 @@
+  * where to place its SVC stack
+  */
+ struct secondary_data secondary_data;
++/* Number of CPUs which aren't online, but looping in kernel text. */
++int cpus_stuck_in_kernel;
+ 
+ enum ipi_msg_type {
+ 	IPI_RESCHEDULE,
+@@ -73,6 +75,16 @@ enum ipi_msg_type {
+ 	IPI_WAKEUP
+ };
+ 
++#ifdef CONFIG_HOTPLUG_CPU
++static int op_cpu_kill(unsigned int cpu);
++#else
++static inline int op_cpu_kill(unsigned int cpu)
++{
++	return -ENOSYS;
++}
++#endif
++
++
+ /*
+  * Boot a secondary CPU, and assign it the specified idle task.
+  * This also gives us the initial stack to use for this CPU.
+@@ -90,12 +102,14 @@ static DECLARE_COMPLETION(cpu_running);
+ int __cpu_up(unsigned int cpu, struct task_struct *idle)
+ {
+ 	int ret;
++	long status;
+ 
+ 	/*
+ 	 * We need to tell the secondary core where to find its stack and the
+ 	 * page tables.
+ 	 */
+ 	secondary_data.stack = task_stack_page(idle) + THREAD_START_SP;
++	update_cpu_boot_status(CPU_MMU_OFF);
+ 	__flush_dcache_area(&secondary_data, sizeof(secondary_data));
+ 
+ 	/*
+@@ -119,6 +133,32 @@ int __cpu_up(unsigned int cpu, struct task_struct *idle)
+ 	}
+ 
+ 	secondary_data.stack = NULL;
++	status = READ_ONCE(secondary_data.status);
++	if (ret && status) {
++
++		if (status == CPU_MMU_OFF)
++			status = READ_ONCE(__early_cpu_boot_status);
++
++		switch (status) {
++		default:
++			pr_err("CPU%u: failed in unknown state : 0x%lx\n",
++					cpu, status);
++			break;
++		case CPU_KILL_ME:
++			if (!op_cpu_kill(cpu)) {
++				pr_crit("CPU%u: died during early boot\n", cpu);
++				break;
++			}
++			/* Fall through */
++			pr_crit("CPU%u: may not have shut down cleanly\n", cpu);
++		case CPU_STUCK_IN_KERNEL:
++			pr_crit("CPU%u: is stuck in kernel\n", cpu);
++			cpus_stuck_in_kernel++;
++			break;
++		case CPU_PANIC_KERNEL:
++			panic("CPU%u detected unsupported configuration\n", cpu);
++		}
++	}
+ 
+ 	return ret;
+ }
+@@ -184,6 +224,9 @@ asmlinkage void secondary_start_kernel(void)
+ 	 */
+ 	pr_info("CPU%u: Booted secondary processor [%08x]\n",
+ 					 cpu, read_cpuid_id());
++	update_cpu_boot_status(CPU_BOOT_SUCCESS);
++	/* Make sure the status update is visible before we complete */
++	smp_wmb();
+ 	set_cpu_online(cpu, true);
+ 	complete(&cpu_running);
+ 
+@@ -325,10 +368,12 @@ void cpu_die_early(void)
+ 	set_cpu_present(cpu, 0);
+ 
+ #ifdef CONFIG_HOTPLUG_CPU
++	update_cpu_boot_status(CPU_KILL_ME);
+ 	/* Check if we can park ourselves */
+ 	if (cpu_ops[cpu] && cpu_ops[cpu]->cpu_die)
+ 		cpu_ops[cpu]->cpu_die(cpu);
+ #endif
++	update_cpu_boot_status(CPU_STUCK_IN_KERNEL);
+ 
+ 	cpu_park_loop();
+ }
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0105-arm64-Add-cpu_panic_kernel-helper.patch b/tools/kdump/0105-arm64-Add-cpu_panic_kernel-helper.patch
new file mode 100644
index 0000000..4f4d1f8
--- /dev/null
+++ b/tools/kdump/0105-arm64-Add-cpu_panic_kernel-helper.patch
@@ -0,0 +1,47 @@
+From 05e05be66299eeed95715bf97077207e75e7f342 Mon Sep 17 00:00:00 2001
+From: Suzuki K Poulose <suzuki.poulose@arm.com>
+Date: Tue, 12 Apr 2016 15:46:00 +0100
+Subject: [PATCH 105/123] arm64: Add cpu_panic_kernel helper
+
+During the activation of a secondary CPU, we could report serious
+configuration issues and hence request to crash the kernel. We do
+this for CPU ASID bit check now. We will need it also for handling
+mismatched exception levels for the CPUs with VHE. Hence, add a
+helper to do the same for reusability.
+
+Cc: Mark Rutland <mark.rutland@arm.com>
+Cc: Will Deacon <will.deacon@arm.com>
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Marc Zyngier <marc.zyngier@arm.com>
+Signed-off-by: Suzuki K Poulose <suzuki.poulose@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit 17eebd1a435c8616c47b715e3447f4a9c15b741f)
+---
+ arch/arm64/include/asm/smp.h | 11 +++++++++++
+ 1 file changed, 11 insertions(+)
+
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index 817a067..433e504 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -113,6 +113,17 @@ static inline void update_cpu_boot_status(int val)
+ 	dsb(ishst);
+ }
+ 
++/*
++ * The calling secondary CPU has detected serious configuration mismatch,
++ * which calls for a kernel panic. Update the boot status and park the calling
++ * CPU.
++ */
++static inline void cpu_panic_kernel(void)
++{
++	update_cpu_boot_status(CPU_PANIC_KERNEL);
++	cpu_park_loop();
++}
++
+ #endif /* ifndef __ASSEMBLY__ */
+ 
+ #endif /* ifndef __ASM_SMP_H */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0106-fixup-arm64-Handle-early-CPU-boot-failures.patch b/tools/kdump/0106-fixup-arm64-Handle-early-CPU-boot-failures.patch
new file mode 100644
index 0000000..538fcf6
--- /dev/null
+++ b/tools/kdump/0106-fixup-arm64-Handle-early-CPU-boot-failures.patch
@@ -0,0 +1,48 @@
+From 2a1d26bc88d5e2ee40ac671ee40f24e68f6abfcb Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Mon, 28 Nov 2016 14:20:38 +0900
+Subject: [PATCH 106/123] fixup! arm64: Handle early CPU boot failures
+
+---
+ arch/arm64/kernel/head.S | 16 ++--------------
+ 1 file changed, 2 insertions(+), 14 deletions(-)
+
+diff --git a/arch/arm64/kernel/head.S b/arch/arm64/kernel/head.S
+index 24c2506..aab630a 100644
+--- a/arch/arm64/kernel/head.S
++++ b/arch/arm64/kernel/head.S
+@@ -608,16 +608,7 @@ ENTRY(secondary_startup)
+ ENDPROC(secondary_startup)
+ 
+ ENTRY(__secondary_switched)
+-#if 1 /* FIXME */
+-	ldr	x0, [x21]			// get secondary_data.stack
+-#else
+-	adr_l	x5, vectors
+-	msr	vbar_el1, x5
+-	isb
+-
+-	adr_l	x0, secondary_data
+-	ldr	x0, [x0, #CPU_BOOT_STACK]	// get secondary_data.stack
+-#endif
++	ldr	x0, [x21, #CPU_BOOT_STACK]	// get secondary_data.stack
+ 	mov	sp, x0
+ 	mov	x29, #0
+ 	b	secondary_start_kernel
+@@ -663,12 +654,9 @@ ENTRY(__enable_mmu)
+ 	ubfx	x2, x1, #ID_AA64MMFR0_TGRAN_SHIFT, 4
+ 	cmp	x2, #ID_AA64MMFR0_TGRAN_SUPPORTED
+ 	b.ne	__no_granule_support
+-#if 1 /* FIXME */
++	update_early_cpu_boot_status 0, x1, x2
+ 	ldr	x5, =vectors
+ 	msr	vbar_el1, x5
+-#else
+-	update_early_cpu_boot_status 0, x1, x2
+-#endif
+ 	msr	ttbr0_el1, x25			// load TTBR0
+ 	msr	ttbr1_el1, x26			// load TTBR1
+ 	isb
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0107-arm64-smp-Add-function-to-determine-if-cpus-are-stuc.patch b/tools/kdump/0107-arm64-smp-Add-function-to-determine-if-cpus-are-stuc.patch
new file mode 100644
index 0000000..8138961
--- /dev/null
+++ b/tools/kdump/0107-arm64-smp-Add-function-to-determine-if-cpus-are-stuc.patch
@@ -0,0 +1,85 @@
+From 0e15fc9ed7e9ff922f4c1813a4bcc8b800d9651a Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Wed, 22 Jun 2016 10:06:12 +0100
+Subject: [PATCH 107/123] arm64: smp: Add function to determine if cpus are
+ stuck in the kernel
+
+kernel/smp.c has a fancy counter that keeps track of the number of CPUs
+it marked as not-present and left in cpu_park_loop(). If there are any
+CPUs spinning in here, features like kexec or hibernate may release them
+by overwriting this memory.
+
+This problem also occurs on machines using spin-tables to release
+secondary cores.
+After commit 44dbcc93ab67 ("arm64: Fix behavior of maxcpus=N")
+we bring all known cpus into the secondary holding pen, meaning this
+memory can't be re-used by kexec or hibernate.
+
+Add a function cpus_are_stuck_in_kernel() to determine if either of these
+cases have occurred.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+Acked-by: Mark Rutland <mark.rutland@arm.com>
+Reviewed-by: Suzuki K Poulose <suzuki.poulose@arm.com>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+[catalin.marinas@arm.com: cherry-picked from mainline for kexec dependency]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+
+(cherry picked from commit b69e0dc14ce3c4abbd11725ff98a885d4616f9fe)
+---
+ arch/arm64/include/asm/smp.h | 12 ++++++++++++
+ arch/arm64/kernel/smp.c      | 18 ++++++++++++++++++
+ 2 files changed, 30 insertions(+)
+
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index 433e504..0226447 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -124,6 +124,18 @@ static inline void cpu_panic_kernel(void)
+ 	cpu_park_loop();
+ }
+ 
++/*
++ * If a secondary CPU enters the kernel but fails to come online,
++ * (e.g. due to mismatched features), and cannot exit the kernel,
++ * we increment cpus_stuck_in_kernel and leave the CPU in a
++ * quiesecent loop within the kernel text. The memory containing
++ * this loop must not be re-used for anything else as the 'stuck'
++ * core is executing it.
++ *
++ * This function is used to inhibit features like kexec and hibernate.
++ */
++bool cpus_are_stuck_in_kernel(void);
++
+ #endif /* ifndef __ASSEMBLY__ */
+ 
+ #endif /* ifndef __ASM_SMP_H */
+diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
+index acabb9b..dc2eaf3 100644
+--- a/arch/arm64/kernel/smp.c
++++ b/arch/arm64/kernel/smp.c
+@@ -890,3 +890,21 @@ int setup_profiling_timer(unsigned int multiplier)
+ {
+ 	return -EINVAL;
+ }
++
++static bool have_cpu_die(void)
++{
++#ifdef CONFIG_HOTPLUG_CPU
++	int any_cpu = raw_smp_processor_id();
++
++	if (cpu_ops[any_cpu]->cpu_die)
++		return true;
++#endif
++	return false;
++}
++
++bool cpus_are_stuck_in_kernel(void)
++{
++	bool smp_spin_tables = (num_possible_cpus() > 1 && !have_cpu_die());
++
++	return !!cpus_stuck_in_kernel || smp_spin_tables;
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0108-arm64-Add-back-cpu-reset-routines.patch b/tools/kdump/0108-arm64-Add-back-cpu-reset-routines.patch
new file mode 100644
index 0000000..59d19f3
--- /dev/null
+++ b/tools/kdump/0108-arm64-Add-back-cpu-reset-routines.patch
@@ -0,0 +1,170 @@
+From 78ae07bf718a9af5c64790a0bbbd747dbe5aa5f6 Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Thu, 23 Jun 2016 17:54:48 +0000
+Subject: [PATCH 108/123] arm64: Add back cpu reset routines
+
+Commit 68234df4ea79 ("arm64: kill flush_cache_all()") removed the global
+arm64 routines cpu_reset() and cpu_soft_restart() needed by the arm64
+kexec and kdump support.  Add back a simplified version of
+cpu_soft_restart() with some changes needed for kexec in the new files
+cpu_reset.S, and cpu_reset.h.
+
+When a CPU is reset it needs to be put into the exception level it had when
+it entered the kernel. Update cpu_soft_restart() to accept an argument
+which signals if the reset address should be entered at EL1 or EL2, and
+add a new hypercall HVC_SOFT_RESTART which is used for the EL2 switch.
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+Reviewed-by: James Morse <james.morse@arm.com>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit f9076ecfb1216a478312b1c078d04792df6d4477)
+---
+ arch/arm64/include/asm/virt.h |  5 ++++
+ arch/arm64/kernel/cpu-reset.S | 54 +++++++++++++++++++++++++++++++++++++++++++
+ arch/arm64/kernel/cpu-reset.h | 34 +++++++++++++++++++++++++++
+ arch/arm64/kernel/hyp-stub.S  | 10 +++++++-
+ 4 files changed, 102 insertions(+), 1 deletion(-)
+ create mode 100644 arch/arm64/kernel/cpu-reset.S
+ create mode 100644 arch/arm64/kernel/cpu-reset.h
+
+diff --git a/arch/arm64/include/asm/virt.h b/arch/arm64/include/asm/virt.h
+index 06e6a523..e6c27b8 100644
+--- a/arch/arm64/include/asm/virt.h
++++ b/arch/arm64/include/asm/virt.h
+@@ -34,6 +34,11 @@
+  */
+ #define HVC_SET_VECTORS 1
+ 
++/*
++ * HVC_SOFT_RESTART - CPU soft reset, used by the cpu_soft_restart routine.
++ */
++#define HVC_SOFT_RESTART 2
++
+ #define BOOT_CPU_MODE_EL1	(0xe11)
+ #define BOOT_CPU_MODE_EL2	(0xe12)
+ 
+diff --git a/arch/arm64/kernel/cpu-reset.S b/arch/arm64/kernel/cpu-reset.S
+new file mode 100644
+index 0000000..65f42d2
+--- /dev/null
++++ b/arch/arm64/kernel/cpu-reset.S
+@@ -0,0 +1,54 @@
++/*
++ * CPU reset routines
++ *
++ * Copyright (C) 2001 Deep Blue Solutions Ltd.
++ * Copyright (C) 2012 ARM Ltd.
++ * Copyright (C) 2015 Huawei Futurewei Technologies.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ */
++
++#include <linux/linkage.h>
++#include <asm/assembler.h>
++#include <asm/sysreg.h>
++#include <asm/virt.h>
++
++.text
++.pushsection    .idmap.text, "ax"
++
++/*
++ * __cpu_soft_restart(el2_switch, entry, arg0, arg1, arg2) - Helper for
++ * cpu_soft_restart.
++ *
++ * @el2_switch: Flag to indicate a swich to EL2 is needed.
++ * @entry: Location to jump to for soft reset.
++ * arg0: First argument passed to @entry.
++ * arg1: Second argument passed to @entry.
++ * arg2: Third argument passed to @entry.
++ *
++ * Put the CPU into the same state as it would be if it had been reset, and
++ * branch to what would be the reset vector. It must be executed with the
++ * flat identity mapping.
++ */
++ENTRY(__cpu_soft_restart)
++	/* Clear sctlr_el1 flags. */
++	mrs	x12, sctlr_el1
++	ldr	x13, =SCTLR_ELx_FLAGS
++	bic	x12, x12, x13
++	msr	sctlr_el1, x12
++	isb
++
++	cbz	x0, 1f				// el2_switch?
++	mov	x0, #HVC_SOFT_RESTART
++	hvc	#0				// no return
++
++1:	mov	x18, x1				// entry
++	mov	x0, x2				// arg0
++	mov	x1, x3				// arg1
++	mov	x2, x4				// arg2
++	br	x18
++ENDPROC(__cpu_soft_restart)
++
++.popsection
+diff --git a/arch/arm64/kernel/cpu-reset.h b/arch/arm64/kernel/cpu-reset.h
+new file mode 100644
+index 0000000..d4e9ecb
+--- /dev/null
++++ b/arch/arm64/kernel/cpu-reset.h
+@@ -0,0 +1,34 @@
++/*
++ * CPU reset routines
++ *
++ * Copyright (C) 2015 Huawei Futurewei Technologies.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ */
++
++#ifndef _ARM64_CPU_RESET_H
++#define _ARM64_CPU_RESET_H
++
++#include <asm/virt.h>
++
++void __cpu_soft_restart(unsigned long el2_switch, unsigned long entry,
++	unsigned long arg0, unsigned long arg1, unsigned long arg2);
++
++static inline void __noreturn cpu_soft_restart(unsigned long el2_switch,
++	unsigned long entry, unsigned long arg0, unsigned long arg1,
++	unsigned long arg2)
++{
++	typeof(__cpu_soft_restart) *restart;
++
++	el2_switch = el2_switch && !is_kernel_in_hyp_mode() &&
++		is_hyp_mode_available();
++	restart = (void *)virt_to_phys(__cpu_soft_restart);
++
++	cpu_install_idmap();
++	restart(el2_switch, entry, arg0, arg1, arg2);
++	unreachable();
++}
++
++#endif
+diff --git a/arch/arm64/kernel/hyp-stub.S b/arch/arm64/kernel/hyp-stub.S
+index 8727f44..d3b5f75 100644
+--- a/arch/arm64/kernel/hyp-stub.S
++++ b/arch/arm64/kernel/hyp-stub.S
+@@ -71,8 +71,16 @@ el1_sync:
+ 	msr	vbar_el2, x1
+ 	b	9f
+ 
++2:	cmp	x0, #HVC_SOFT_RESTART
++	b.ne	3f
++	mov	x0, x2
++	mov	x2, x4
++	mov	x4, x1
++	mov	x1, x3
++	br	x4				// no return
++
+ 	/* Someone called kvm_call_hyp() against the hyp-stub... */
+-2:	mov     x0, #ARM_EXCEPTION_HYP_GONE
++3:	mov	x0, #ARM_EXCEPTION_HYP_GONE
+ 
+ 9:	eret
+ ENDPROC(el1_sync)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0109-arm64-kexec-Add-core-kexec-support.patch b/tools/kdump/0109-arm64-kexec-Add-core-kexec-support.patch
new file mode 100644
index 0000000..cf6af65
--- /dev/null
+++ b/tools/kdump/0109-arm64-kexec-Add-core-kexec-support.patch
@@ -0,0 +1,443 @@
+From 8e0f52aaf3c8acb4a4f7aa9a99c3da7845bdb426 Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Thu, 23 Jun 2016 17:54:48 +0000
+Subject: [PATCH 109/123] arm64/kexec: Add core kexec support
+
+Add three new files, kexec.h, machine_kexec.c and relocate_kernel.S to the
+arm64 architecture that add support for the kexec re-boot mechanism
+(CONFIG_KEXEC) on arm64 platforms.
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+Reviewed-by: James Morse <james.morse@arm.com>
+[catalin.marinas@arm.com: removed dead code following James Morse's comments]
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+
+(cherry picked from commit d28f6df1305a86715e4e7ea0f043ba01c0a0e8d9)
+---
+ arch/arm64/Kconfig                  |  10 +++
+ arch/arm64/include/asm/kexec.h      |  48 ++++++++++
+ arch/arm64/kernel/Makefile          |   3 +
+ arch/arm64/kernel/machine_kexec.c   | 170 ++++++++++++++++++++++++++++++++++++
+ arch/arm64/kernel/relocate_kernel.S | 130 +++++++++++++++++++++++++++
+ include/uapi/linux/kexec.h          |   1 +
+ 6 files changed, 362 insertions(+)
+ create mode 100644 arch/arm64/include/asm/kexec.h
+ create mode 100644 arch/arm64/kernel/machine_kexec.c
+ create mode 100644 arch/arm64/kernel/relocate_kernel.S
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index eda5d99..2f862c7 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -589,6 +589,16 @@ config SECCOMP
+ 	  and the task is only allowed to execute a few safe syscalls
+ 	  defined by each seccomp mode.
+ 
++config KEXEC
++	depends on PM_SLEEP_SMP
++	select KEXEC_CORE
++	bool "kexec system call"
++	---help---
++	  kexec is a system call that implements the ability to shutdown your
++	  current kernel, and to start another kernel.  It is like a reboot
++	  but it is independent of the system firmware.   And like a reboot
++	  you can start any kernel with it, not just Linux.
++
+ config XEN_DOM0
+ 	def_bool y
+ 	depends on XEN
+diff --git a/arch/arm64/include/asm/kexec.h b/arch/arm64/include/asm/kexec.h
+new file mode 100644
+index 0000000..04744dc
+--- /dev/null
++++ b/arch/arm64/include/asm/kexec.h
+@@ -0,0 +1,48 @@
++/*
++ * kexec for arm64
++ *
++ * Copyright (C) Linaro.
++ * Copyright (C) Huawei Futurewei Technologies.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ */
++
++#ifndef _ARM64_KEXEC_H
++#define _ARM64_KEXEC_H
++
++/* Maximum physical address we can use pages from */
++
++#define KEXEC_SOURCE_MEMORY_LIMIT (-1UL)
++
++/* Maximum address we can reach in physical address mode */
++
++#define KEXEC_DESTINATION_MEMORY_LIMIT (-1UL)
++
++/* Maximum address we can use for the control code buffer */
++
++#define KEXEC_CONTROL_MEMORY_LIMIT (-1UL)
++
++#define KEXEC_CONTROL_PAGE_SIZE 4096
++
++#define KEXEC_ARCH KEXEC_ARCH_AARCH64
++
++#ifndef __ASSEMBLY__
++
++/**
++ * crash_setup_regs() - save registers for the panic kernel
++ *
++ * @newregs: registers are saved here
++ * @oldregs: registers to be saved (may be %NULL)
++ */
++
++static inline void crash_setup_regs(struct pt_regs *newregs,
++				    struct pt_regs *oldregs)
++{
++	/* Empty routine needed to avoid build errors. */
++}
++
++#endif /* __ASSEMBLY__ */
++
++#endif
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index ee2ffac..2fdc6f8 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -43,6 +43,9 @@ arm64-obj-$(CONFIG_ACPI)		+= acpi.o
+ arm64-obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
+ arm64-obj-$(CONFIG_ARM64_ACPI_PARKING_PROTOCOL)	+= acpi_parking_protocol.o
+ arm64-obj-$(CONFIG_PARAVIRT)		+= paravirt.o
++arm64-obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
++arm64-obj-$(CONFIG_KEXEC)		+= machine_kexec.o relocate_kernel.o	\
++					   cpu-reset.o
+ 
+ obj-y					+= $(arm64-obj-y) vdso/ probes/
+ obj-m					+= $(arm64-obj-m)
+diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
+new file mode 100644
+index 0000000..c40e646
+--- /dev/null
++++ b/arch/arm64/kernel/machine_kexec.c
+@@ -0,0 +1,170 @@
++/*
++ * kexec for arm64
++ *
++ * Copyright (C) Linaro.
++ * Copyright (C) Huawei Futurewei Technologies.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ */
++
++#include <linux/kexec.h>
++#include <linux/smp.h>
++
++#include <asm/cacheflush.h>
++#include <asm/cpu_ops.h>
++#include <asm/mmu_context.h>
++
++#include "cpu-reset.h"
++
++/* Global variables for the arm64_relocate_new_kernel routine. */
++extern const unsigned char arm64_relocate_new_kernel[];
++extern const unsigned long arm64_relocate_new_kernel_size;
++
++static unsigned long kimage_start;
++
++void machine_kexec_cleanup(struct kimage *kimage)
++{
++	/* Empty routine needed to avoid build errors. */
++}
++
++/**
++ * machine_kexec_prepare - Prepare for a kexec reboot.
++ *
++ * Called from the core kexec code when a kernel image is loaded.
++ * Forbid loading a kexec kernel if we have no way of hotplugging cpus or cpus
++ * are stuck in the kernel. This avoids a panic once we hit machine_kexec().
++ */
++int machine_kexec_prepare(struct kimage *kimage)
++{
++	kimage_start = kimage->start;
++
++	if (kimage->type != KEXEC_TYPE_CRASH && cpus_are_stuck_in_kernel()) {
++		pr_err("Can't kexec: CPUs are stuck in the kernel.\n");
++		return -EBUSY;
++	}
++
++	return 0;
++}
++
++/**
++ * kexec_list_flush - Helper to flush the kimage list and source pages to PoC.
++ */
++static void kexec_list_flush(struct kimage *kimage)
++{
++	kimage_entry_t *entry;
++
++	for (entry = &kimage->head; ; entry++) {
++		unsigned int flag;
++		void *addr;
++
++		/* flush the list entries. */
++		__flush_dcache_area(entry, sizeof(kimage_entry_t));
++
++		flag = *entry & IND_FLAGS;
++		if (flag == IND_DONE)
++			break;
++
++		addr = phys_to_virt(*entry & PAGE_MASK);
++
++		switch (flag) {
++		case IND_INDIRECTION:
++			/* Set entry point just before the new list page. */
++			entry = (kimage_entry_t *)addr - 1;
++			break;
++		case IND_SOURCE:
++			/* flush the source pages. */
++			__flush_dcache_area(addr, PAGE_SIZE);
++			break;
++		case IND_DESTINATION:
++			break;
++		default:
++			BUG();
++		}
++	}
++}
++
++/**
++ * kexec_segment_flush - Helper to flush the kimage segments to PoC.
++ */
++static void kexec_segment_flush(const struct kimage *kimage)
++{
++	unsigned long i;
++
++	pr_debug("%s:\n", __func__);
++
++	for (i = 0; i < kimage->nr_segments; i++) {
++		pr_debug("  segment[%lu]: %016lx - %016lx, 0x%lx bytes, %lu pages\n",
++			i,
++			kimage->segment[i].mem,
++			kimage->segment[i].mem + kimage->segment[i].memsz,
++			kimage->segment[i].memsz,
++			kimage->segment[i].memsz /  PAGE_SIZE);
++
++		__flush_dcache_area(phys_to_virt(kimage->segment[i].mem),
++			kimage->segment[i].memsz);
++	}
++}
++
++/**
++ * machine_kexec - Do the kexec reboot.
++ *
++ * Called from the core kexec code for a sys_reboot with LINUX_REBOOT_CMD_KEXEC.
++ */
++void machine_kexec(struct kimage *kimage)
++{
++	phys_addr_t reboot_code_buffer_phys;
++	void *reboot_code_buffer;
++
++	/*
++	 * New cpus may have become stuck_in_kernel after we loaded the image.
++	 */
++	BUG_ON(cpus_are_stuck_in_kernel() || (num_online_cpus() > 1));
++
++	reboot_code_buffer_phys = page_to_phys(kimage->control_code_page);
++	reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys);
++
++	/*
++	 * Copy arm64_relocate_new_kernel to the reboot_code_buffer for use
++	 * after the kernel is shut down.
++	 */
++	memcpy(reboot_code_buffer, arm64_relocate_new_kernel,
++		arm64_relocate_new_kernel_size);
++
++	/* Flush the reboot_code_buffer in preparation for its execution. */
++	__flush_dcache_area(reboot_code_buffer, arm64_relocate_new_kernel_size);
++	flush_icache_range((uintptr_t)reboot_code_buffer,
++		arm64_relocate_new_kernel_size);
++
++	/* Flush the kimage list and its buffers. */
++	kexec_list_flush(kimage);
++
++	/* Flush the new image if already in place. */
++	if (kimage->head & IND_DONE)
++		kexec_segment_flush(kimage);
++
++	pr_info("Bye!\n");
++
++	/* Disable all DAIF exceptions. */
++	asm volatile ("msr daifset, #0xf" : : : "memory");
++
++	/*
++	 * cpu_soft_restart will shutdown the MMU, disable data caches, then
++	 * transfer control to the reboot_code_buffer which contains a copy of
++	 * the arm64_relocate_new_kernel routine.  arm64_relocate_new_kernel
++	 * uses physical addressing to relocate the new image to its final
++	 * position and transfers control to the image entry point when the
++	 * relocation is complete.
++	 */
++
++	cpu_soft_restart(1, reboot_code_buffer_phys, kimage->head,
++		kimage_start, 0);
++
++	BUG(); /* Should never get here. */
++}
++
++void machine_crash_shutdown(struct pt_regs *regs)
++{
++	/* Empty routine needed to avoid build errors. */
++}
+diff --git a/arch/arm64/kernel/relocate_kernel.S b/arch/arm64/kernel/relocate_kernel.S
+new file mode 100644
+index 0000000..51b73cd
+--- /dev/null
++++ b/arch/arm64/kernel/relocate_kernel.S
+@@ -0,0 +1,130 @@
++/*
++ * kexec for arm64
++ *
++ * Copyright (C) Linaro.
++ * Copyright (C) Huawei Futurewei Technologies.
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ */
++
++#include <linux/kexec.h>
++#include <linux/linkage.h>
++
++#include <asm/assembler.h>
++#include <asm/kexec.h>
++#include <asm/page.h>
++#include <asm/sysreg.h>
++
++/*
++ * arm64_relocate_new_kernel - Put a 2nd stage image in place and boot it.
++ *
++ * The memory that the old kernel occupies may be overwritten when coping the
++ * new image to its final location.  To assure that the
++ * arm64_relocate_new_kernel routine which does that copy is not overwritten,
++ * all code and data needed by arm64_relocate_new_kernel must be between the
++ * symbols arm64_relocate_new_kernel and arm64_relocate_new_kernel_end.  The
++ * machine_kexec() routine will copy arm64_relocate_new_kernel to the kexec
++ * control_code_page, a special page which has been set up to be preserved
++ * during the copy operation.
++ */
++ENTRY(arm64_relocate_new_kernel)
++
++	/* Setup the list loop variables. */
++	mov	x17, x1				/* x17 = kimage_start */
++	mov	x16, x0				/* x16 = kimage_head */
++	dcache_line_size x15, x0		/* x15 = dcache line size */
++	mov	x14, xzr			/* x14 = entry ptr */
++	mov	x13, xzr			/* x13 = copy dest */
++
++	/* Clear the sctlr_el2 flags. */
++	mrs	x0, CurrentEL
++	cmp	x0, #CurrentEL_EL2
++	b.ne	1f
++	mrs	x0, sctlr_el2
++	ldr	x1, =SCTLR_ELx_FLAGS
++	bic	x0, x0, x1
++	msr	sctlr_el2, x0
++	isb
++1:
++
++	/* Check if the new image needs relocation. */
++	tbnz	x16, IND_DONE_BIT, .Ldone
++
++.Lloop:
++	and	x12, x16, PAGE_MASK		/* x12 = addr */
++
++	/* Test the entry flags. */
++.Ltest_source:
++	tbz	x16, IND_SOURCE_BIT, .Ltest_indirection
++
++	/* Invalidate dest page to PoC. */
++	mov     x0, x13
++	add     x20, x0, #PAGE_SIZE
++	sub     x1, x15, #1
++	bic     x0, x0, x1
++2:	dc      ivac, x0
++	add     x0, x0, x15
++	cmp     x0, x20
++	b.lo    2b
++	dsb     sy
++
++	mov x20, x13
++	mov x21, x12
++	copy_page x20, x21, x0, x1, x2, x3, x4, x5, x6, x7
++
++	/* dest += PAGE_SIZE */
++	add	x13, x13, PAGE_SIZE
++	b	.Lnext
++
++.Ltest_indirection:
++	tbz	x16, IND_INDIRECTION_BIT, .Ltest_destination
++
++	/* ptr = addr */
++	mov	x14, x12
++	b	.Lnext
++
++.Ltest_destination:
++	tbz	x16, IND_DESTINATION_BIT, .Lnext
++
++	/* dest = addr */
++	mov	x13, x12
++
++.Lnext:
++	/* entry = *ptr++ */
++	ldr	x16, [x14], #8
++
++	/* while (!(entry & DONE)) */
++	tbz	x16, IND_DONE_BIT, .Lloop
++
++.Ldone:
++	/* wait for writes from copy_page to finish */
++	dsb	nsh
++	ic	iallu
++	dsb	nsh
++	isb
++
++	/* Start new image. */
++	mov	x0, xzr
++	mov	x1, xzr
++	mov	x2, xzr
++	mov	x3, xzr
++	br	x17
++
++ENDPROC(arm64_relocate_new_kernel)
++
++.ltorg
++
++.align 3	/* To keep the 64-bit values below naturally aligned. */
++
++.Lcopy_end:
++.org	KEXEC_CONTROL_PAGE_SIZE
++
++/*
++ * arm64_relocate_new_kernel_size - Number of bytes to copy to the
++ * control_code_page.
++ */
++.globl arm64_relocate_new_kernel_size
++arm64_relocate_new_kernel_size:
++	.quad	.Lcopy_end - arm64_relocate_new_kernel
+diff --git a/include/uapi/linux/kexec.h b/include/uapi/linux/kexec.h
+index 99048e5..aae5ebf 100644
+--- a/include/uapi/linux/kexec.h
++++ b/include/uapi/linux/kexec.h
+@@ -39,6 +39,7 @@
+ #define KEXEC_ARCH_SH      (42 << 16)
+ #define KEXEC_ARCH_MIPS_LE (10 << 16)
+ #define KEXEC_ARCH_MIPS    ( 8 << 16)
++#define KEXEC_ARCH_AARCH64 (183 << 16)
+ 
+ /* The artificial cap on the number of segments passed to kexec_load. */
+ #define KEXEC_SEGMENT_MAX 16
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0110-arm64-kexec-Enable-kexec-in-the-arm64-defconfig.patch b/tools/kdump/0110-arm64-kexec-Enable-kexec-in-the-arm64-defconfig.patch
new file mode 100644
index 0000000..b3b202b
--- /dev/null
+++ b/tools/kdump/0110-arm64-kexec-Enable-kexec-in-the-arm64-defconfig.patch
@@ -0,0 +1,27 @@
+From 6c210c24ec5d823f8db4c663248d14096a275efc Mon Sep 17 00:00:00 2001
+From: Geoff Levand <geoff@infradead.org>
+Date: Thu, 23 Jun 2016 17:54:48 +0000
+Subject: [PATCH 110/123] arm64/kexec: Enable kexec in the arm64 defconfig
+
+Signed-off-by: Geoff Levand <geoff@infradead.org>
+Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
+(cherry picked from commit b26a4ae39963faf6f9014f6ff72e3661c83753ce)
+---
+ arch/arm64/configs/defconfig | 1 +
+ 1 file changed, 1 insertion(+)
+
+diff --git a/arch/arm64/configs/defconfig b/arch/arm64/configs/defconfig
+index 6fddf58..c1d4dec 100644
+--- a/arch/arm64/configs/defconfig
++++ b/arch/arm64/configs/defconfig
+@@ -60,6 +60,7 @@ CONFIG_KSM=y
+ CONFIG_TRANSPARENT_HUGEPAGE=y
+ CONFIG_CMA=y
+ CONFIG_CMDLINE="console=ttyAMA0"
++CONFIG_KEXEC=y
+ # CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+ CONFIG_COMPAT=y
+ CONFIG_CPU_IDLE=y
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0111-mm-memblock-add-MEMBLOCK_NOMAP-attribute-to-memblock.patch b/tools/kdump/0111-mm-memblock-add-MEMBLOCK_NOMAP-attribute-to-memblock.patch
new file mode 100644
index 0000000..17d458e
--- /dev/null
+++ b/tools/kdump/0111-mm-memblock-add-MEMBLOCK_NOMAP-attribute-to-memblock.patch
@@ -0,0 +1,128 @@
+From cf91a8b446f2469caec70ee73b9c040fbc35df49 Mon Sep 17 00:00:00 2001
+From: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Date: Mon, 30 Nov 2015 13:28:15 +0100
+Subject: [PATCH 111/123] mm/memblock: add MEMBLOCK_NOMAP attribute to memblock
+ memory table
+
+This introduces the MEMBLOCK_NOMAP attribute and the required plumbing
+to make it usable as an indicator that some parts of normal memory
+should not be covered by the kernel direct mapping. It is up to the
+arch to actually honor the attribute when laying out this mapping,
+but the memblock code itself is modified to disregard these regions
+for allocations and other general use.
+
+Cc: linux-mm@kvack.org
+Cc: Alexander Kuleshov <kuleshovmail@gmail.com>
+Cc: Andrew Morton <akpm@linux-foundation.org>
+Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
+Signed-off-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Signed-off-by: Will Deacon <will.deacon@arm.com>
+(cherry picked from commit bf3d3cc580f9960883ebf9ea05868f336d9491c2)
+---
+ include/linux/memblock.h |  8 ++++++++
+ mm/memblock.c            | 28 ++++++++++++++++++++++++++++
+ 2 files changed, 36 insertions(+)
+
+diff --git a/include/linux/memblock.h b/include/linux/memblock.h
+index 24daf8f..fec66f8 100644
+--- a/include/linux/memblock.h
++++ b/include/linux/memblock.h
+@@ -25,6 +25,7 @@ enum {
+ 	MEMBLOCK_NONE		= 0x0,	/* No special request */
+ 	MEMBLOCK_HOTPLUG	= 0x1,	/* hotpluggable region */
+ 	MEMBLOCK_MIRROR		= 0x2,	/* mirrored region */
++	MEMBLOCK_NOMAP		= 0x4,	/* don't add to kernel direct mapping */
+ };
+ 
+ struct memblock_region {
+@@ -82,6 +83,7 @@ bool memblock_overlaps_region(struct memblock_type *type,
+ int memblock_mark_hotplug(phys_addr_t base, phys_addr_t size);
+ int memblock_clear_hotplug(phys_addr_t base, phys_addr_t size);
+ int memblock_mark_mirror(phys_addr_t base, phys_addr_t size);
++int memblock_mark_nomap(phys_addr_t base, phys_addr_t size);
+ ulong choose_memblock_flags(void);
+ 
+ /* Low level functions */
+@@ -184,6 +186,11 @@ static inline bool memblock_is_mirror(struct memblock_region *m)
+ 	return m->flags & MEMBLOCK_MIRROR;
+ }
+ 
++static inline bool memblock_is_nomap(struct memblock_region *m)
++{
++	return m->flags & MEMBLOCK_NOMAP;
++}
++
+ #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+ int memblock_search_pfn_nid(unsigned long pfn, unsigned long *start_pfn,
+ 			    unsigned long  *end_pfn);
+@@ -319,6 +326,7 @@ phys_addr_t memblock_start_of_DRAM(void);
+ phys_addr_t memblock_end_of_DRAM(void);
+ void memblock_enforce_memory_limit(phys_addr_t memory_limit);
+ int memblock_is_memory(phys_addr_t addr);
++int memblock_is_map_memory(phys_addr_t addr);
+ int memblock_is_region_memory(phys_addr_t base, phys_addr_t size);
+ int memblock_is_reserved(phys_addr_t addr);
+ bool memblock_is_region_reserved(phys_addr_t base, phys_addr_t size);
+diff --git a/mm/memblock.c b/mm/memblock.c
+index d300f13..07ff069 100644
+--- a/mm/memblock.c
++++ b/mm/memblock.c
+@@ -822,6 +822,17 @@ int __init_memblock memblock_mark_mirror(phys_addr_t base, phys_addr_t size)
+ 	return memblock_setclr_flag(base, size, 1, MEMBLOCK_MIRROR);
+ }
+ 
++/**
++ * memblock_mark_nomap - Mark a memory region with flag MEMBLOCK_NOMAP.
++ * @base: the base phys addr of the region
++ * @size: the size of the region
++ *
++ * Return 0 on success, -errno on failure.
++ */
++int __init_memblock memblock_mark_nomap(phys_addr_t base, phys_addr_t size)
++{
++	return memblock_setclr_flag(base, size, 1, MEMBLOCK_NOMAP);
++}
+ 
+ /**
+  * __next_reserved_mem_region - next function for for_each_reserved_region()
+@@ -913,6 +924,10 @@ void __init_memblock __next_mem_range(u64 *idx, int nid, ulong flags,
+ 		if ((flags & MEMBLOCK_MIRROR) && !memblock_is_mirror(m))
+ 			continue;
+ 
++		/* skip nomap memory unless we were asked for it explicitly */
++		if (!(flags & MEMBLOCK_NOMAP) && memblock_is_nomap(m))
++			continue;
++
+ 		if (!type_b) {
+ 			if (out_start)
+ 				*out_start = m_start;
+@@ -1022,6 +1037,10 @@ void __init_memblock __next_mem_range_rev(u64 *idx, int nid, ulong flags,
+ 		if ((flags & MEMBLOCK_MIRROR) && !memblock_is_mirror(m))
+ 			continue;
+ 
++		/* skip nomap memory unless we were asked for it explicitly */
++		if (!(flags & MEMBLOCK_NOMAP) && memblock_is_nomap(m))
++			continue;
++
+ 		if (!type_b) {
+ 			if (out_start)
+ 				*out_start = m_start;
+@@ -1519,6 +1538,15 @@ int __init_memblock memblock_is_memory(phys_addr_t addr)
+ 	return memblock_search(&memblock.memory, addr) != -1;
+ }
+ 
++int __init_memblock memblock_is_map_memory(phys_addr_t addr)
++{
++	int i = memblock_search(&memblock.memory, addr);
++
++	if (i == -1)
++		return false;
++	return !memblock_is_nomap(&memblock.memory.regions[i]);
++}
++
+ #ifdef CONFIG_HAVE_MEMBLOCK_NODE_MAP
+ int __init_memblock memblock_search_pfn_nid(unsigned long pfn,
+ 			 unsigned long *start_pfn, unsigned long *end_pfn)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0112-mm-memblock.c-memblock_is_memory-reserved-can-be-boo.patch b/tools/kdump/0112-mm-memblock.c-memblock_is_memory-reserved-can-be-boo.patch
new file mode 100644
index 0000000..4535145
--- /dev/null
+++ b/tools/kdump/0112-mm-memblock.c-memblock_is_memory-reserved-can-be-boo.patch
@@ -0,0 +1,61 @@
+From 403cf51f3343e654b9313a6d3f8cb8d7742b5933 Mon Sep 17 00:00:00 2001
+From: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
+Date: Thu, 14 Jan 2016 15:18:54 -0800
+Subject: [PATCH 112/123] mm/memblock.c: memblock_is_memory()/reserved() can be
+ boolean
+
+Make memblock_is_memory() and memblock_is_reserved return bool to
+improve readability due to these particular functions only using either
+one or zero as their return value.
+
+No functional change.
+
+Signed-off-by: Yaowei Bai <baiyaowei@cmss.chinamobile.com>
+Acked-by: Michal Hocko <mhocko@suse.com>
+Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
+Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
+(cherry picked from commit b4ad0c7e004a2cc0e52790eff72f5176b59ca386)
+---
+ include/linux/memblock.h | 4 ++--
+ mm/memblock.c            | 4 ++--
+ 2 files changed, 4 insertions(+), 4 deletions(-)
+
+diff --git a/include/linux/memblock.h b/include/linux/memblock.h
+index fec66f8..3a092fb 100644
+--- a/include/linux/memblock.h
++++ b/include/linux/memblock.h
+@@ -325,10 +325,10 @@ phys_addr_t memblock_mem_size(unsigned long limit_pfn);
+ phys_addr_t memblock_start_of_DRAM(void);
+ phys_addr_t memblock_end_of_DRAM(void);
+ void memblock_enforce_memory_limit(phys_addr_t memory_limit);
+-int memblock_is_memory(phys_addr_t addr);
++bool memblock_is_memory(phys_addr_t addr);
+ int memblock_is_map_memory(phys_addr_t addr);
+ int memblock_is_region_memory(phys_addr_t base, phys_addr_t size);
+-int memblock_is_reserved(phys_addr_t addr);
++bool memblock_is_reserved(phys_addr_t addr);
+ bool memblock_is_region_reserved(phys_addr_t base, phys_addr_t size);
+ 
+ extern void __memblock_dump_all(void);
+diff --git a/mm/memblock.c b/mm/memblock.c
+index 07ff069..9695398 100644
+--- a/mm/memblock.c
++++ b/mm/memblock.c
+@@ -1528,12 +1528,12 @@ static int __init_memblock memblock_search(struct memblock_type *type, phys_addr
+ 	return -1;
+ }
+ 
+-int __init memblock_is_reserved(phys_addr_t addr)
++bool __init memblock_is_reserved(phys_addr_t addr)
+ {
+ 	return memblock_search(&memblock.reserved, addr) != -1;
+ }
+ 
+-int __init_memblock memblock_is_memory(phys_addr_t addr)
++bool __init_memblock memblock_is_memory(phys_addr_t addr)
+ {
+ 	return memblock_search(&memblock.memory, addr) != -1;
+ }
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0113-mm-memblock.c-add-new-infrastructure-to-address-the-.patch b/tools/kdump/0113-mm-memblock.c-add-new-infrastructure-to-address-the-.patch
new file mode 100644
index 0000000..f165724
--- /dev/null
+++ b/tools/kdump/0113-mm-memblock.c-add-new-infrastructure-to-address-the-.patch
@@ -0,0 +1,145 @@
+From 815107425d6a53dc1f18de570e1b99e697482999 Mon Sep 17 00:00:00 2001
+From: Dennis Chen <dennis.chen@arm.com>
+Date: Thu, 28 Jul 2016 15:48:26 -0700
+Subject: [PATCH 113/123] mm/memblock.c: add new infrastructure to address the
+ mem limit issue
+
+In some cases, memblock is queried by kernel to determine whether a
+specified address is RAM or not.  For example, the ACPI core needs this
+information to determine which attributes to use when mapping ACPI
+regions(acpi_os_ioremap).  Use of incorrect memory types can result in
+faults, data corruption, or other issues.
+
+Removing memory with memblock_enforce_memory_limit() throws away this
+information, and so a kernel booted with 'mem=' may suffer from the
+issues described above.  To avoid this, we need to keep those NOMAP
+regions instead of removing all above the limit, which preserves the
+information we need while preventing other use of those regions.
+
+This patch adds new infrastructure to retain all NOMAP memblock regions
+while removing others, to cater for this.
+
+Link: http://lkml.kernel.org/r/1468475036-5852-2-git-send-email-dennis.chen@arm.com
+Signed-off-by: Dennis Chen <dennis.chen@arm.com>
+Acked-by: Steve Capper <steve.capper@arm.com>
+Cc: Catalin Marinas <catalin.marinas@arm.com>
+Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
+Cc: Pekka Enberg <penberg@kernel.org>
+Cc: Mel Gorman <mgorman@techsingularity.net>
+Cc: Tang Chen <tangchen@cn.fujitsu.com>
+Cc: Tony Luck <tony.luck@intel.com>
+Cc: Ingo Molnar <mingo@kernel.org>
+Cc: Rafael J. Wysocki <rafael@kernel.org>
+Cc: Will Deacon <will.deacon@arm.com>
+Cc: Mark Rutland <mark.rutland@arm.com>
+Cc: Matt Fleming <matt@codeblueprint.co.uk>
+Cc: Kaly Xin <kaly.xin@arm.com>
+Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
+Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
+(cherry picked from commit a571d4eb55d83ff538d98870fa8a8497b24d39bc)
+---
+ include/linux/memblock.h |  1 +
+ mm/memblock.c            | 57 +++++++++++++++++++++++++++++++++++++++++++-----
+ 2 files changed, 53 insertions(+), 5 deletions(-)
+
+diff --git a/include/linux/memblock.h b/include/linux/memblock.h
+index 3a092fb..86d411c 100644
+--- a/include/linux/memblock.h
++++ b/include/linux/memblock.h
+@@ -325,6 +325,7 @@ phys_addr_t memblock_mem_size(unsigned long limit_pfn);
+ phys_addr_t memblock_start_of_DRAM(void);
+ phys_addr_t memblock_end_of_DRAM(void);
+ void memblock_enforce_memory_limit(phys_addr_t memory_limit);
++void memblock_mem_limit_remove_map(phys_addr_t limit);
+ bool memblock_is_memory(phys_addr_t addr);
+ int memblock_is_map_memory(phys_addr_t addr);
+ int memblock_is_region_memory(phys_addr_t base, phys_addr_t size);
+diff --git a/mm/memblock.c b/mm/memblock.c
+index 9695398..40402dc 100644
+--- a/mm/memblock.c
++++ b/mm/memblock.c
+@@ -1486,15 +1486,16 @@ phys_addr_t __init_memblock memblock_end_of_DRAM(void)
+ 	return (memblock.memory.regions[idx].base + memblock.memory.regions[idx].size);
+ }
+ 
+-void __init memblock_enforce_memory_limit(phys_addr_t limit)
++static phys_addr_t __init_memblock __find_max_addr(phys_addr_t limit)
+ {
+ 	phys_addr_t max_addr = (phys_addr_t)ULLONG_MAX;
+ 	struct memblock_region *r;
+ 
+-	if (!limit)
+-		return;
+-
+-	/* find out max address */
++	/*
++	 * translate the memory @limit size into the max address within one of
++	 * the memory memblock regions, if the @limit exceeds the total size
++	 * of those regions, max_addr will keep original value ULLONG_MAX
++	 */
+ 	for_each_memblock(memory, r) {
+ 		if (limit <= r->size) {
+ 			max_addr = r->base + limit;
+@@ -1503,6 +1504,22 @@ void __init memblock_enforce_memory_limit(phys_addr_t limit)
+ 		limit -= r->size;
+ 	}
+ 
++	return max_addr;
++}
++
++void __init memblock_enforce_memory_limit(phys_addr_t limit)
++{
++	phys_addr_t max_addr = (phys_addr_t)ULLONG_MAX;
++
++	if (!limit)
++		return;
++
++	max_addr = __find_max_addr(limit);
++
++	/* @limit exceeds the total size of the memory, do nothing */
++	if (max_addr == (phys_addr_t)ULLONG_MAX)
++		return;
++
+ 	/* truncate both memory and reserved regions */
+ 	memblock_remove_range(&memblock.memory, max_addr,
+ 			      (phys_addr_t)ULLONG_MAX);
+@@ -1510,6 +1527,36 @@ void __init memblock_enforce_memory_limit(phys_addr_t limit)
+ 			      (phys_addr_t)ULLONG_MAX);
+ }
+ 
++void __init memblock_mem_limit_remove_map(phys_addr_t limit)
++{
++	struct memblock_type *type = &memblock.memory;
++	phys_addr_t max_addr;
++	int i, ret, start_rgn, end_rgn;
++
++	if (!limit)
++		return;
++
++	max_addr = __find_max_addr(limit);
++
++	/* @limit exceeds the total size of the memory, do nothing */
++	if (max_addr == (phys_addr_t)ULLONG_MAX)
++		return;
++
++	ret = memblock_isolate_range(type, max_addr, (phys_addr_t)ULLONG_MAX,
++				&start_rgn, &end_rgn);
++	if (ret)
++		return;
++
++	/* remove all the MAP regions above the limit */
++	for (i = end_rgn - 1; i >= start_rgn; i--) {
++		if (!memblock_is_nomap(&type->regions[i]))
++			memblock_remove_region(type, i);
++	}
++	/* truncate the reserved regions */
++	memblock_remove_range(&memblock.reserved, max_addr,
++			      (phys_addr_t)ULLONG_MAX);
++}
++
+ static int __init_memblock memblock_search(struct memblock_type *type, phys_addr_t addr)
+ {
+ 	unsigned int left = 0, right = type->cnt;
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0114-memblock-add-memblock_cap_memory_range.patch b/tools/kdump/0114-memblock-add-memblock_cap_memory_range.patch
new file mode 100644
index 0000000..10ff34a
--- /dev/null
+++ b/tools/kdump/0114-memblock-add-memblock_cap_memory_range.patch
@@ -0,0 +1,111 @@
+From b746dd903931306fe99b0b5d02e02a4866bed3a8 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Mon, 25 Jul 2016 15:43:32 +0900
+Subject: [PATCH 114/123] memblock: add memblock_cap_memory_range()
+
+Add memblock_cap_memory_range() which will remove all the memblock regions
+except the memory range specified in the arguments. In addition, rework is
+done on memblock_mem_limit_remove_map() to re-implement it using
+memblock_cap_memory_range().
+
+This function, like memblock_mem_limit_remove_map(), will not remove
+memblocks with MEMMAP_NOMAP attribute as they may be mapped and accessed
+later as "device memory."
+See the commit a571d4eb55d8 ("mm/memblock.c: add new infrastructure to
+address the mem limit issue").
+
+This function is used, in a succeeding patch in the series of arm64 kdump
+suuport, to limit the range of usable memory, or System RAM, on crash dump
+kernel.
+(Please note that "mem=" parameter is of little use for this purpose.)
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Reviewed-by: Will Deacon <will.deacon@arm.com>
+Cc: linux-mm@kvack.org
+Cc: Andrew Morton <akpm@linux-foundation.org>
+(cherry picked from commit 092ee06a2acb2c0af9cece58f7c2fbe6a72cef52)
+---
+ include/linux/memblock.h |  1 +
+ mm/memblock.c            | 44 +++++++++++++++++++++++++++++---------------
+ 2 files changed, 30 insertions(+), 15 deletions(-)
+
+diff --git a/include/linux/memblock.h b/include/linux/memblock.h
+index 86d411c..682b23d 100644
+--- a/include/linux/memblock.h
++++ b/include/linux/memblock.h
+@@ -325,6 +325,7 @@ phys_addr_t memblock_mem_size(unsigned long limit_pfn);
+ phys_addr_t memblock_start_of_DRAM(void);
+ phys_addr_t memblock_end_of_DRAM(void);
+ void memblock_enforce_memory_limit(phys_addr_t memory_limit);
++void memblock_cap_memory_range(phys_addr_t base, phys_addr_t size);
+ void memblock_mem_limit_remove_map(phys_addr_t limit);
+ bool memblock_is_memory(phys_addr_t addr);
+ int memblock_is_map_memory(phys_addr_t addr);
+diff --git a/mm/memblock.c b/mm/memblock.c
+index 40402dc..06c9b74 100644
+--- a/mm/memblock.c
++++ b/mm/memblock.c
+@@ -1527,11 +1527,37 @@ void __init memblock_enforce_memory_limit(phys_addr_t limit)
+ 			      (phys_addr_t)ULLONG_MAX);
+ }
+ 
++void __init memblock_cap_memory_range(phys_addr_t base, phys_addr_t size)
++{
++	int start_rgn, end_rgn;
++	int i, ret;
++
++	if (!size)
++		return;
++
++	ret = memblock_isolate_range(&memblock.memory, base, size,
++						&start_rgn, &end_rgn);
++	if (ret)
++		return;
++
++	/* remove all the MAP regions */
++	for (i = memblock.memory.cnt - 1; i >= end_rgn; i--)
++		if (!memblock_is_nomap(&memblock.memory.regions[i]))
++			memblock_remove_region(&memblock.memory, i);
++
++	for (i = start_rgn - 1; i >= 0; i--)
++		if (!memblock_is_nomap(&memblock.memory.regions[i]))
++			memblock_remove_region(&memblock.memory, i);
++
++	/* truncate the reserved regions */
++	memblock_remove_range(&memblock.reserved, 0, base);
++	memblock_remove_range(&memblock.reserved,
++			base + size, (phys_addr_t)ULLONG_MAX);
++}
++
+ void __init memblock_mem_limit_remove_map(phys_addr_t limit)
+ {
+-	struct memblock_type *type = &memblock.memory;
+ 	phys_addr_t max_addr;
+-	int i, ret, start_rgn, end_rgn;
+ 
+ 	if (!limit)
+ 		return;
+@@ -1542,19 +1568,7 @@ void __init memblock_mem_limit_remove_map(phys_addr_t limit)
+ 	if (max_addr == (phys_addr_t)ULLONG_MAX)
+ 		return;
+ 
+-	ret = memblock_isolate_range(type, max_addr, (phys_addr_t)ULLONG_MAX,
+-				&start_rgn, &end_rgn);
+-	if (ret)
+-		return;
+-
+-	/* remove all the MAP regions above the limit */
+-	for (i = end_rgn - 1; i >= start_rgn; i--) {
+-		if (!memblock_is_nomap(&type->regions[i]))
+-			memblock_remove_region(type, i);
+-	}
+-	/* truncate the reserved regions */
+-	memblock_remove_range(&memblock.reserved, max_addr,
+-			      (phys_addr_t)ULLONG_MAX);
++	memblock_cap_memory_range(0, max_addr);
+ }
+ 
+ static int __init_memblock memblock_search(struct memblock_type *type, phys_addr_t addr)
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0115-arm64-limit-memory-regions-based-on-DT-property-usab.patch b/tools/kdump/0115-arm64-limit-memory-regions-based-on-DT-property-usab.patch
new file mode 100644
index 0000000..9005fb5
--- /dev/null
+++ b/tools/kdump/0115-arm64-limit-memory-regions-based-on-DT-property-usab.patch
@@ -0,0 +1,74 @@
+From 96af576852d969328982e8fd4f9843ab486f1071 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Tue, 26 Jan 2016 15:29:29 +0900
+Subject: [PATCH 115/123] arm64: limit memory regions based on DT property,
+ usable-memory-range
+
+Crash dump kernel utilizes only a subset of available memory as System RAM.
+On arm64 kdump, This memory range is advertized to crash dump kernel via
+a device-tree property under /chosen,
+   linux,usable-memory-range = <BASE SIZE>
+
+Crash dump kernel reads this property at boot time and calls
+memblock_cap_memory_range() to limit usable memory ranges which are
+described as entries in UEFI memory map table or "memory" nodes in
+a device tree blob.
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Reviewed-by: Geoff Levand <geoff@infradead.org>
+(cherry picked from commit 730c31632a7c31afcd704635990ca11d15d4d247)
+---
+ arch/arm64/mm/init.c | 35 +++++++++++++++++++++++++++++++++++
+ 1 file changed, 35 insertions(+)
+
+diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
+index b37e259..007ee97 100644
+--- a/arch/arm64/mm/init.c
++++ b/arch/arm64/mm/init.c
+@@ -157,8 +157,43 @@ static int __init early_mem(char *p)
+ }
+ early_param("mem", early_mem);
+ 
++static int __init early_init_dt_scan_usablemem(unsigned long node,
++		const char *uname, int depth, void *data)
++{
++	struct memblock_region *usablemem = (struct memblock_region *)data;
++	const __be32 *reg;
++	int len;
++
++	usablemem->size = 0;
++
++	if (depth != 1 || strcmp(uname, "chosen") != 0)
++		return 0;
++
++	reg = of_get_flat_dt_prop(node, "linux,usable-memory-range", &len);
++	if (!reg || (len < (dt_root_addr_cells + dt_root_size_cells)))
++		return 1;
++
++	usablemem->base = dt_mem_next_cell(dt_root_addr_cells, &reg);
++	usablemem->size = dt_mem_next_cell(dt_root_size_cells, &reg);
++
++	return 1;
++}
++
++static void __init fdt_enforce_memory_region(void)
++{
++	struct memblock_region reg;
++
++	of_scan_flat_dt(early_init_dt_scan_usablemem, &reg);
++
++	if (reg.size)
++		memblock_cap_memory_range(reg.base, reg.size);
++}
++
+ void __init arm64_memblock_init(void)
+ {
++	/* Handle linux,usable-memory-range property */
++	fdt_enforce_memory_region();
++
+ 	memblock_enforce_memory_limit(memory_limit);
+ 
+ 	/*
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0116-arm64-kdump-reserve-memory-for-crash-dump-kernel.patch b/tools/kdump/0116-arm64-kdump-reserve-memory-for-crash-dump-kernel.patch
new file mode 100644
index 0000000..33d5236
--- /dev/null
+++ b/tools/kdump/0116-arm64-kdump-reserve-memory-for-crash-dump-kernel.patch
@@ -0,0 +1,193 @@
+From 43cfbb60b5b043957939633c9dea195c310140f2 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Wed, 19 Aug 2015 12:07:21 +0900
+Subject: [PATCH 116/123] arm64: kdump: reserve memory for crash dump kernel
+
+"crashkernel=" kernel parameter specifies the size (and optionally
+the start address) of the system ram used by crash dump kernel.
+reserve_crashkernel() will allocate and reserve the memory at the startup
+of primary kernel.
+
+This memory range will be exported to userspace via:
+	- an entry named "Crash kernel" in /proc/iomem, and
+	- "linux,crashkernel-base" and "linux,crashkernel-size" under
+	  /sys/firmware/devicetree/base/chosen
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Signed-off-by: Mark Salter <msalter@redhat.com>
+Signed-off-by: Pratyush Anand <panand@redhat.com>
+Reviewed-by: James Morse <james.morse@arm.com>
+(cherry picked from commit 68eb94cc6f71c8dea69b000986c33bc4884c1c27)
+---
+ arch/arm64/kernel/setup.c |   7 ++-
+ arch/arm64/mm/init.c      | 110 ++++++++++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 116 insertions(+), 1 deletion(-)
+
+diff --git a/arch/arm64/kernel/setup.c b/arch/arm64/kernel/setup.c
+index ad29060..e641a52 100644
+--- a/arch/arm64/kernel/setup.c
++++ b/arch/arm64/kernel/setup.c
+@@ -31,7 +31,6 @@
+ #include <linux/screen_info.h>
+ #include <linux/init.h>
+ #include <linux/kexec.h>
+-#include <linux/crash_dump.h>
+ #include <linux/root_dev.h>
+ #include <linux/cpu.h>
+ #include <linux/interrupt.h>
+@@ -221,6 +220,12 @@ static void __init request_standard_resources(void)
+ 		    kernel_data.end <= res->end)
+ 			request_resource(res, &kernel_data);
+ 	}
++
++#ifdef CONFIG_KEXEC_CORE
++	/* User space tools will find "Crash kernel" region in /proc/iomem. */
++	if (crashk_res.end)
++		insert_resource(&iomem_resource, &crashk_res);
++#endif
+ }
+ 
+ #ifdef CONFIG_BLK_DEV_INITRD
+diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
+index 007ee97..38d3a56 100644
+--- a/arch/arm64/mm/init.c
++++ b/arch/arm64/mm/init.c
+@@ -29,11 +29,13 @@
+ #include <linux/gfp.h>
+ #include <linux/memblock.h>
+ #include <linux/sort.h>
++#include <linux/of.h>
+ #include <linux/of_fdt.h>
+ #include <linux/dma-mapping.h>
+ #include <linux/dma-contiguous.h>
+ #include <linux/efi.h>
+ #include <linux/swiotlb.h>
++#include <linux/kexec.h>
+ 
+ #include <asm/fixmap.h>
+ #include <asm/memory.h>
+@@ -66,6 +68,111 @@ static int __init early_initrd(char *p)
+ early_param("initrd", early_initrd);
+ #endif
+ 
++#ifdef CONFIG_KEXEC_CORE
++static unsigned long long crash_size, crash_base;
++static struct property crash_base_prop = {
++	.name = "linux,crashkernel-base",
++	.length = sizeof(u64),
++	.value = &crash_base
++};
++static struct property crash_size_prop = {
++	.name = "linux,crashkernel-size",
++	.length = sizeof(u64),
++	.value = &crash_size,
++};
++
++static int __init export_crashkernel(void)
++{
++	struct device_node *node;
++	int ret;
++
++	if (!crash_size)
++		return 0;
++
++	/* Add /chosen/linux,crashkernel-* properties */
++	node = of_find_node_by_path("/chosen");
++	if (!node)
++		return -ENOENT;
++
++	/*
++	 * There might be existing crash kernel properties, but we can't
++	 * be sure what's in them, so remove them.
++	 */
++	of_remove_property(node, of_find_property(node,
++				"linux,crashkernel-base", NULL));
++	of_remove_property(node, of_find_property(node,
++				"linux,crashkernel-size", NULL));
++
++	ret = of_add_property(node, &crash_base_prop);
++	if (ret)
++		goto ret_err;
++
++	ret = of_add_property(node, &crash_size_prop);
++	if (ret)
++		goto ret_err;
++
++	return 0;
++
++ret_err:
++	pr_warn("Exporting crashkernel region to device tree failed\n");
++	return ret;
++}
++late_initcall(export_crashkernel);
++
++/*
++ * reserve_crashkernel() - reserves memory for crash kernel
++ *
++ * This function reserves memory area given in "crashkernel=" kernel command
++ * line parameter. The memory reserved is used by dump capture kernel when
++ * primary kernel is crashing.
++ */
++static void __init reserve_crashkernel(void)
++{
++	int ret;
++
++	ret = parse_crashkernel(boot_command_line, memblock_phys_mem_size(),
++				&crash_size, &crash_base);
++	/* no crashkernel= or invalid value specified */
++	if (ret || !crash_size)
++		return;
++
++	if (crash_base == 0) {
++		/* Current arm64 boot protocol requires 2MB alignment */
++		crash_base = memblock_find_in_range(0, ARCH_LOW_ADDRESS_LIMIT,
++				crash_size, SZ_2M);
++		if (crash_base == 0) {
++			pr_warn("Unable to allocate crashkernel (size:%llx)\n",
++				crash_size);
++			return;
++		}
++	} else {
++		/* User specifies base address explicitly. */
++		if (!memblock_is_region_memory(crash_base, crash_size) ||
++			memblock_is_region_reserved(crash_base, crash_size)) {
++			pr_warn("crashkernel has wrong address or size\n");
++			return;
++		}
++
++		if (!IS_ALIGNED(crash_base, SZ_2M)) {
++			pr_warn("crashkernel base address is not 2MB aligned\n");
++			return;
++		}
++	}
++	memblock_reserve(crash_base, crash_size);
++
++	pr_info("Reserving %lldMB of memory at %lldMB for crashkernel\n",
++		crash_size >> 20, crash_base >> 20);
++
++	crashk_res.start = crash_base;
++	crashk_res.end = crash_base + crash_size - 1;
++}
++#else
++static void __init reserve_crashkernel(void)
++{
++	;
++}
++#endif /* CONFIG_KEXEC_CORE */
++
+ /*
+  * Return the maximum physical address for ZONE_DMA (DMA_BIT_MASK(32)). It
+  * currently assumes that for memory starting above 4G, 32-bit devices will
+@@ -213,6 +320,9 @@ void __init arm64_memblock_init(void)
+ 		arm64_dma_phys_limit = max_zone_dma_phys();
+ 	else
+ 		arm64_dma_phys_limit = PHYS_MASK + 1;
++
++	reserve_crashkernel();
++
+ 	dma_contiguous_reserve(arm64_dma_phys_limit);
+ 
+ 	memblock_allow_resize();
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0117-arm64-kdump-implement-machine_crash_shutdown.patch b/tools/kdump/0117-arm64-kdump-implement-machine_crash_shutdown.patch
new file mode 100644
index 0000000..425af11
--- /dev/null
+++ b/tools/kdump/0117-arm64-kdump-implement-machine_crash_shutdown.patch
@@ -0,0 +1,309 @@
+From 179869bbdff912ce6a185e36ae7dd8e277635409 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Wed, 19 Aug 2015 16:42:23 +0900
+Subject: [PATCH 117/123] arm64: kdump: implement machine_crash_shutdown()
+
+Primary kernel calls machine_crash_shutdown() to shut down non-boot cpus
+and save registers' status in per-cpu ELF notes before starting crash
+dump kernel. See kernel_kexec().
+Even if not all secondary cpus have shut down, we do kdump anyway.
+
+As we don't have to make non-boot(crashed) cpus offline (to preserve
+correct status of cpus at crash dump) before shutting down, this patch
+also adds a variant of smp_send_stop().
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Reviewed-by: James Morse <james.morse@arm.com>
+(cherry picked from commit 99db24a46aaa2127cab3d4cb03197fc9126016f6)
+---
+ arch/arm64/include/asm/hardirq.h  |  2 +-
+ arch/arm64/include/asm/kexec.h    | 42 +++++++++++++++++++++++++-
+ arch/arm64/include/asm/smp.h      |  2 ++
+ arch/arm64/kernel/machine_kexec.c | 56 ++++++++++++++++++++++++++++++++--
+ arch/arm64/kernel/smp.c           | 63 +++++++++++++++++++++++++++++++++++++++
+ 5 files changed, 160 insertions(+), 5 deletions(-)
+
+diff --git a/arch/arm64/include/asm/hardirq.h b/arch/arm64/include/asm/hardirq.h
+index 8740297..1473fc2 100644
+--- a/arch/arm64/include/asm/hardirq.h
++++ b/arch/arm64/include/asm/hardirq.h
+@@ -20,7 +20,7 @@
+ #include <linux/threads.h>
+ #include <asm/irq.h>
+ 
+-#define NR_IPI	6
++#define NR_IPI	7
+ 
+ typedef struct {
+ 	unsigned int __softirq_pending;
+diff --git a/arch/arm64/include/asm/kexec.h b/arch/arm64/include/asm/kexec.h
+index 04744dc..b5168e8 100644
+--- a/arch/arm64/include/asm/kexec.h
++++ b/arch/arm64/include/asm/kexec.h
+@@ -40,7 +40,47 @@
+ static inline void crash_setup_regs(struct pt_regs *newregs,
+ 				    struct pt_regs *oldregs)
+ {
+-	/* Empty routine needed to avoid build errors. */
++	if (oldregs) {
++		memcpy(newregs, oldregs, sizeof(*newregs));
++	} else {
++		u64 tmp1, tmp2;
++
++		__asm__ __volatile__ (
++			"stp	 x0,   x1, [%2, #16 *  0]\n"
++			"stp	 x2,   x3, [%2, #16 *  1]\n"
++			"stp	 x4,   x5, [%2, #16 *  2]\n"
++			"stp	 x6,   x7, [%2, #16 *  3]\n"
++			"stp	 x8,   x9, [%2, #16 *  4]\n"
++			"stp	x10,  x11, [%2, #16 *  5]\n"
++			"stp	x12,  x13, [%2, #16 *  6]\n"
++			"stp	x14,  x15, [%2, #16 *  7]\n"
++			"stp	x16,  x17, [%2, #16 *  8]\n"
++			"stp	x18,  x19, [%2, #16 *  9]\n"
++			"stp	x20,  x21, [%2, #16 * 10]\n"
++			"stp	x22,  x23, [%2, #16 * 11]\n"
++			"stp	x24,  x25, [%2, #16 * 12]\n"
++			"stp	x26,  x27, [%2, #16 * 13]\n"
++			"stp	x28,  x29, [%2, #16 * 14]\n"
++			"mov	 %0,  sp\n"
++			"stp	x30,  %0,  [%2, #16 * 15]\n"
++
++			"/* faked current PSTATE */\n"
++			"mrs	 %0, CurrentEL\n"
++			"mrs	 %1, SPSEL\n"
++			"orr	 %0, %0, %1\n"
++			"mrs	 %1, DAIF\n"
++			"orr	 %0, %0, %1\n"
++			"mrs	 %1, NZCV\n"
++			"orr	 %0, %0, %1\n"
++			/* pc */
++			"adr	 %1, 1f\n"
++		"1:\n"
++			"stp	 %1, %0,   [%2, #16 * 16]\n"
++			: "+r" (tmp1), "+r" (tmp2)
++			: "r" (newregs)
++			: "memory"
++		);
++	}
+ }
+ 
+ #endif /* __ASSEMBLY__ */
+diff --git a/arch/arm64/include/asm/smp.h b/arch/arm64/include/asm/smp.h
+index 0226447..6b0f2c7 100644
+--- a/arch/arm64/include/asm/smp.h
++++ b/arch/arm64/include/asm/smp.h
+@@ -136,6 +136,8 @@ static inline void cpu_panic_kernel(void)
+  */
+ bool cpus_are_stuck_in_kernel(void);
+ 
++extern void smp_send_crash_stop(void);
++
+ #endif /* ifndef __ASSEMBLY__ */
+ 
+ #endif /* ifndef __ASM_SMP_H */
+diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
+index c40e646..24c8b82 100644
+--- a/arch/arm64/kernel/machine_kexec.c
++++ b/arch/arm64/kernel/machine_kexec.c
+@@ -9,6 +9,9 @@
+  * published by the Free Software Foundation.
+  */
+ 
++#include <linux/interrupt.h>
++#include <linux/irq.h>
++#include <linux/kernel.h>
+ #include <linux/kexec.h>
+ #include <linux/smp.h>
+ 
+@@ -22,6 +25,7 @@
+ extern const unsigned char arm64_relocate_new_kernel[];
+ extern const unsigned long arm64_relocate_new_kernel_size;
+ 
++static bool in_crash_kexec;
+ static unsigned long kimage_start;
+ 
+ void machine_kexec_cleanup(struct kimage *kimage)
+@@ -120,7 +124,8 @@ void machine_kexec(struct kimage *kimage)
+ 	/*
+ 	 * New cpus may have become stuck_in_kernel after we loaded the image.
+ 	 */
+-	BUG_ON(cpus_are_stuck_in_kernel() || (num_online_cpus() > 1));
++	BUG_ON((cpus_are_stuck_in_kernel() || (num_online_cpus() > 1)) &&
++			!WARN_ON(in_crash_kexec));
+ 
+ 	reboot_code_buffer_phys = page_to_phys(kimage->control_code_page);
+ 	reboot_code_buffer = phys_to_virt(reboot_code_buffer_phys);
+@@ -158,13 +163,58 @@ void machine_kexec(struct kimage *kimage)
+ 	 * relocation is complete.
+ 	 */
+ 
+-	cpu_soft_restart(1, reboot_code_buffer_phys, kimage->head,
++	cpu_soft_restart(!in_crash_kexec, reboot_code_buffer_phys, kimage->head,
+ 		kimage_start, 0);
+ 
+ 	BUG(); /* Should never get here. */
+ }
+ 
++static void machine_kexec_mask_interrupts(void)
++{
++	unsigned int i;
++	struct irq_desc *desc;
++
++	for_each_irq_desc(i, desc) {
++		struct irq_chip *chip;
++		int ret;
++
++		chip = irq_desc_get_chip(desc);
++		if (!chip)
++			continue;
++
++		/*
++		 * First try to remove the active state. If this
++		 * fails, try to EOI the interrupt.
++		 */
++		ret = irq_set_irqchip_state(i, IRQCHIP_STATE_ACTIVE, false);
++
++		if (ret && irqd_irq_inprogress(&desc->irq_data) &&
++		    chip->irq_eoi)
++			chip->irq_eoi(&desc->irq_data);
++
++		if (chip->irq_mask)
++			chip->irq_mask(&desc->irq_data);
++
++		if (chip->irq_disable && !irqd_irq_disabled(&desc->irq_data))
++			chip->irq_disable(&desc->irq_data);
++	}
++}
++
++/**
++ * machine_crash_shutdown - shutdown non-crashing cpus and save registers
++ */
+ void machine_crash_shutdown(struct pt_regs *regs)
+ {
+-	/* Empty routine needed to avoid build errors. */
++	local_irq_disable();
++
++	in_crash_kexec = true;
++
++	/* shutdown non-crashing cpus */
++	smp_send_crash_stop();
++
++	/* for crashing cpu */
++	crash_save_cpu(regs, smp_processor_id());
++	machine_kexec_mask_interrupts();
++
++	pr_info("Starting crashdump kernel...\n");
+ }
+diff --git a/arch/arm64/kernel/smp.c b/arch/arm64/kernel/smp.c
+index dc2eaf3..426cf43 100644
+--- a/arch/arm64/kernel/smp.c
++++ b/arch/arm64/kernel/smp.c
+@@ -37,6 +37,7 @@
+ #include <linux/completion.h>
+ #include <linux/of.h>
+ #include <linux/irq_work.h>
++#include <linux/kexec.h>
+ 
+ #include <asm/alternative.h>
+ #include <asm/atomic.h>
+@@ -70,6 +71,7 @@ enum ipi_msg_type {
+ 	IPI_RESCHEDULE,
+ 	IPI_CALL_FUNC,
+ 	IPI_CPU_STOP,
++	IPI_CPU_CRASH_STOP,
+ 	IPI_TIMER,
+ 	IPI_IRQ_WORK,
+ 	IPI_WAKEUP
+@@ -701,6 +703,7 @@ static const char *ipi_types[NR_IPI] __tracepoint_string = {
+ 	S(IPI_RESCHEDULE, "Rescheduling interrupts"),
+ 	S(IPI_CALL_FUNC, "Function call interrupts"),
+ 	S(IPI_CPU_STOP, "CPU stop interrupts"),
++	S(IPI_CPU_CRASH_STOP, "CPU stop (for crash dump) interrupts"),
+ 	S(IPI_TIMER, "Timer broadcast interrupts"),
+ 	S(IPI_IRQ_WORK, "IRQ work interrupts"),
+ 	S(IPI_WAKEUP, "CPU wake-up interrupts"),
+@@ -785,6 +788,29 @@ static void ipi_cpu_stop(unsigned int cpu)
+ 		cpu_relax();
+ }
+ 
++#ifdef CONFIG_KEXEC_CORE
++static atomic_t waiting_for_crash_ipi;
++#endif
++
++static void ipi_cpu_crash_stop(unsigned int cpu, struct pt_regs *regs)
++{
++#ifdef CONFIG_KEXEC_CORE
++	crash_save_cpu(regs, cpu);
++
++	atomic_dec(&waiting_for_crash_ipi);
++
++	local_irq_disable();
++
++#ifdef CONFIG_HOTPLUG_CPU
++	if (cpu_ops[cpu]->cpu_die)
++		cpu_ops[cpu]->cpu_die(cpu);
++#endif
++
++	/* just in case */
++	cpu_park_loop();
++#endif
++}
++
+ /*
+  * Main handler for inter-processor interrupts
+  */
+@@ -815,6 +841,15 @@ void handle_IPI(int ipinr, struct pt_regs *regs)
+ 		irq_exit();
+ 		break;
+ 
++	case IPI_CPU_CRASH_STOP:
++		if (IS_ENABLED(CONFIG_KEXEC_CORE)) {
++			irq_enter();
++			ipi_cpu_crash_stop(cpu, regs);
++
++			unreachable();
++		}
++		break;
++
+ #ifdef CONFIG_GENERIC_CLOCKEVENTS_BROADCAST
+ 	case IPI_TIMER:
+ 		irq_enter();
+@@ -883,6 +918,34 @@ void smp_send_stop(void)
+ 		pr_warning("SMP: failed to stop secondary CPUs\n");
+ }
+ 
++#ifdef CONFIG_KEXEC_CORE
++void smp_send_crash_stop(void)
++{
++	cpumask_t mask;
++	unsigned long timeout;
++
++	if (num_online_cpus() == 1)
++		return;
++
++	cpumask_copy(&mask, cpu_online_mask);
++	cpumask_clear_cpu(smp_processor_id(), &mask);
++
++	atomic_set(&waiting_for_crash_ipi, num_online_cpus() - 1);
++
++	pr_crit("SMP: stopping secondary CPUs\n");
++	smp_cross_call(&mask, IPI_CPU_CRASH_STOP);
++
++	/* Wait up to one second for other CPUs to stop */
++	timeout = USEC_PER_SEC;
++	while ((atomic_read(&waiting_for_crash_ipi) > 0) && timeout--)
++		udelay(1);
++
++	if (atomic_read(&waiting_for_crash_ipi) > 0)
++		pr_warning("SMP: failed to stop secondary CPUs %*pbl\n",
++			   cpumask_pr_args(cpu_online_mask));
++}
++#endif
++
+ /*
+  * not supported here
+  */
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0118-arm64-kdump-add-VMCOREINFO-s-for-user-space-tools.patch b/tools/kdump/0118-arm64-kdump-add-VMCOREINFO-s-for-user-space-tools.patch
new file mode 100644
index 0000000..eab8d4c
--- /dev/null
+++ b/tools/kdump/0118-arm64-kdump-add-VMCOREINFO-s-for-user-space-tools.patch
@@ -0,0 +1,54 @@
+From d68e6d88295102716ba82fa2eade4b83d05f1c53 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Mon, 23 May 2016 08:43:55 +0900
+Subject: [PATCH 118/123] arm64: kdump: add VMCOREINFO's for user-space tools
+
+In addition to common VMCOREINFO's defined in
+crash_save_vmcoreinfo_init(), we need to know, for crash utility,
+  - kimage_voffset
+  - PHYS_OFFSET
+to examine the contents of a dump file (/proc/vmcore) correctly
+due to the introduction of KASLR (CONFIG_RANDOMIZE_BASE) in v4.6.
+
+  - VA_BITS
+is also required for makedumpfile command.
+
+arch_crash_save_vmcoreinfo() appends them to the dump file.
+More VMCOREINFO's may be added later.
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Reviewed-by: James Morse <james.morse@arm.com>
+(cherry picked from commit ef446b789c4a5ed01d4f08a05a0068499f357d5b)
+---
+ arch/arm64/kernel/machine_kexec.c | 11 +++++++++++
+ 1 file changed, 11 insertions(+)
+
+diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
+index 24c8b82..0ff4ec5 100644
+--- a/arch/arm64/kernel/machine_kexec.c
++++ b/arch/arm64/kernel/machine_kexec.c
+@@ -17,6 +17,7 @@
+ 
+ #include <asm/cacheflush.h>
+ #include <asm/cpu_ops.h>
++#include <asm/memory.h>
+ #include <asm/mmu_context.h>
+ 
+ #include "cpu-reset.h"
+@@ -218,3 +219,13 @@ void machine_crash_shutdown(struct pt_regs *regs)
+ 
+ 	pr_info("Starting crashdump kernel...\n");
+ }
++
++void arch_crash_save_vmcoreinfo(void)
++{
++	VMCOREINFO_NUMBER(VA_BITS);
++	/* Please note VMCOREINFO_NUMBER() uses "%d", not "%x" */
++	vmcoreinfo_append_str("NUMBER(kimage_voffset)=0x%llx\n",
++						kimage_voffset);
++	vmcoreinfo_append_str("NUMBER(PHYS_OFFSET)=0x%llx\n",
++						PHYS_OFFSET);
++}
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0119-arm64-kdump-provide-proc-vmcore-file.patch b/tools/kdump/0119-arm64-kdump-provide-proc-vmcore-file.patch
new file mode 100644
index 0000000..faab8c1
--- /dev/null
+++ b/tools/kdump/0119-arm64-kdump-provide-proc-vmcore-file.patch
@@ -0,0 +1,224 @@
+From 03b2934d92a3c7386b8a1200bf756c75e5dba6f2 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Wed, 19 Aug 2015 16:43:42 +0900
+Subject: [PATCH 119/123] arm64: kdump: provide /proc/vmcore file
+
+Add arch-specific functions to provide a dump file, /proc/vmcore.
+
+This file is in ELF format and its ELF header needs to be prepared by
+userspace tools, like kexec-tools, in adance. The primary kernel is
+responsible to allocate the region with reserve_elfcorehdr() at boot time
+and advertize its location to crash dump kernel via a new device-tree
+property, "linux,elfcorehdr".
+
+Then crash dump kernel will access the primary kernel's memory with
+copy_oldmem_page(), which feeds the data page-by-page by ioremap'ing it
+since it does not reside in linear mapping on crash dump kernel.
+
+We also need our own elfcorehdr_read() here since the header is placed
+within crash dump kernel's usable memory.
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Reviewed-by: James Morse <james.morse@arm.com>
+(cherry picked from commit f937c8d9d51203469fd7a0138cb8507d03e78897)
+---
+ arch/arm64/Kconfig             | 11 +++++++
+ arch/arm64/kernel/Makefile     |  1 +
+ arch/arm64/kernel/crash_dump.c | 71 ++++++++++++++++++++++++++++++++++++++++++
+ arch/arm64/mm/init.c           | 54 ++++++++++++++++++++++++++++++++
+ 4 files changed, 137 insertions(+)
+ create mode 100644 arch/arm64/kernel/crash_dump.c
+
+diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
+index 2f862c7..297411e 100644
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -599,6 +599,17 @@ config KEXEC
+ 	  but it is independent of the system firmware.   And like a reboot
+ 	  you can start any kernel with it, not just Linux.
+ 
++config CRASH_DUMP
++	bool "Build kdump crash kernel"
++	help
++	  Generate crash dump after being started by kexec. This should
++	  be normally only set in special crash dump kernels which are
++	  loaded in the main kernel with kexec-tools into a specially
++	  reserved region and then later executed after a crash by
++	  kdump/kexec.
++
++	  For more details see Documentation/kdump/kdump.txt
++
+ config XEN_DOM0
+ 	def_bool y
+ 	depends on XEN
+diff --git a/arch/arm64/kernel/Makefile b/arch/arm64/kernel/Makefile
+index 2fdc6f8..6d57ceb 100644
+--- a/arch/arm64/kernel/Makefile
++++ b/arch/arm64/kernel/Makefile
+@@ -46,6 +46,7 @@ arm64-obj-$(CONFIG_PARAVIRT)		+= paravirt.o
+ arm64-obj-$(CONFIG_HIBERNATION)		+= hibernate.o hibernate-asm.o
+ arm64-obj-$(CONFIG_KEXEC)		+= machine_kexec.o relocate_kernel.o	\
+ 					   cpu-reset.o
++arm64-obj-$(CONFIG_CRASH_DUMP)		+= crash_dump.o
+ 
+ obj-y					+= $(arm64-obj-y) vdso/ probes/
+ obj-m					+= $(arm64-obj-m)
+diff --git a/arch/arm64/kernel/crash_dump.c b/arch/arm64/kernel/crash_dump.c
+new file mode 100644
+index 0000000..c3d5a21
+--- /dev/null
++++ b/arch/arm64/kernel/crash_dump.c
+@@ -0,0 +1,71 @@
++/*
++ * Routines for doing kexec-based kdump
++ *
++ * Copyright (C) 2014 Linaro Limited
++ * Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
++ *
++ * This program is free software; you can redistribute it and/or modify
++ * it under the terms of the GNU General Public License version 2 as
++ * published by the Free Software Foundation.
++ */
++
++#include <linux/crash_dump.h>
++#include <linux/errno.h>
++#include <linux/io.h>
++#include <linux/memblock.h>
++#include <linux/uaccess.h>
++#include <asm/memory.h>
++
++/**
++ * copy_oldmem_page() - copy one page from old kernel memory
++ * @pfn: page frame number to be copied
++ * @buf: buffer where the copied page is placed
++ * @csize: number of bytes to copy
++ * @offset: offset in bytes into the page
++ * @userbuf: if set, @buf is in a user address space
++ *
++ * This function copies one page from old kernel memory into buffer pointed by
++ * @buf. If @buf is in userspace, set @userbuf to %1. Returns number of bytes
++ * copied or negative error in case of failure.
++ */
++ssize_t copy_oldmem_page(unsigned long pfn, char *buf,
++			 size_t csize, unsigned long offset,
++			 int userbuf)
++{
++	void *vaddr;
++
++	if (!csize)
++		return 0;
++
++	vaddr = memremap(__pfn_to_phys(pfn), PAGE_SIZE, MEMREMAP_WB);
++	if (!vaddr)
++		return -ENOMEM;
++
++	if (userbuf) {
++		if (copy_to_user((char __user *)buf, vaddr + offset, csize)) {
++			memunmap(vaddr);
++			return -EFAULT;
++		}
++	} else {
++		memcpy(buf, vaddr + offset, csize);
++	}
++
++	memunmap(vaddr);
++
++	return csize;
++}
++
++/**
++ * elfcorehdr_read - read from ELF core header
++ * @buf: buffer where the data is placed
++ * @csize: number of bytes to read
++ * @ppos: address in the memory
++ *
++ * This function reads @count bytes from elf core header which exists
++ * on crash dump kernel's memory.
++ */
++ssize_t elfcorehdr_read(char *buf, size_t count, u64 *ppos)
++{
++	memcpy(buf, phys_to_virt((phys_addr_t)*ppos), count);
++	return count;
++}
+diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c
+index 38d3a56..1f9d414 100644
+--- a/arch/arm64/mm/init.c
++++ b/arch/arm64/mm/init.c
+@@ -36,6 +36,7 @@
+ #include <linux/efi.h>
+ #include <linux/swiotlb.h>
+ #include <linux/kexec.h>
++#include <linux/crash_dump.h>
+ 
+ #include <asm/fixmap.h>
+ #include <asm/memory.h>
+@@ -173,6 +174,57 @@ static void __init reserve_crashkernel(void)
+ }
+ #endif /* CONFIG_KEXEC_CORE */
+ 
++#ifdef CONFIG_CRASH_DUMP
++static int __init early_init_dt_scan_elfcorehdr(unsigned long node,
++		const char *uname, int depth, void *data)
++{
++	const __be32 *reg;
++	int len;
++
++	if (depth != 1 || strcmp(uname, "chosen") != 0)
++		return 0;
++
++	reg = of_get_flat_dt_prop(node, "linux,elfcorehdr", &len);
++	if (!reg || (len < (dt_root_addr_cells + dt_root_size_cells)))
++		return 1;
++
++	elfcorehdr_addr = dt_mem_next_cell(dt_root_addr_cells, &reg);
++	elfcorehdr_size = dt_mem_next_cell(dt_root_size_cells, &reg);
++
++	return 1;
++}
++
++/*
++ * reserve_elfcorehdr() - reserves memory for elf core header
++ *
++ * This function reserves elf core header given in "elfcorehdr=" kernel
++ * command line parameter. This region contains all the information about
++ * primary kernel's core image and is used by a dump capture kernel to
++ * access the system memory on primary kernel.
++ */
++static void __init reserve_elfcorehdr(void)
++{
++	of_scan_flat_dt(early_init_dt_scan_elfcorehdr, NULL);
++
++	if (!elfcorehdr_size)
++		return;
++
++	if (memblock_is_region_reserved(elfcorehdr_addr, elfcorehdr_size)) {
++		pr_warn("elfcorehdr is overlapped\n");
++		return;
++	}
++
++	memblock_reserve(elfcorehdr_addr, elfcorehdr_size);
++
++	pr_info("Reserving %lldKB of memory at 0x%llx for elfcorehdr\n",
++		elfcorehdr_size >> 10, elfcorehdr_addr);
++}
++#else
++static void __init reserve_elfcorehdr(void)
++{
++	;
++}
++#endif /* CONFIG_CRASH_DUMP */
+ /*
+  * Return the maximum physical address for ZONE_DMA (DMA_BIT_MASK(32)). It
+  * currently assumes that for memory starting above 4G, 32-bit devices will
+@@ -323,6 +375,8 @@ void __init arm64_memblock_init(void)
+ 
+ 	reserve_crashkernel();
+ 
++	reserve_elfcorehdr();
++
+ 	dma_contiguous_reserve(arm64_dma_phys_limit);
+ 
+ 	memblock_allow_resize();
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0120-arm64-kdump-enable-kdump-in-defconfig.patch b/tools/kdump/0120-arm64-kdump-enable-kdump-in-defconfig.patch
new file mode 100644
index 0000000..880bc86
--- /dev/null
+++ b/tools/kdump/0120-arm64-kdump-enable-kdump-in-defconfig.patch
@@ -0,0 +1,28 @@
+From 781bf8bfeea0874b426246860c7e440a4aa7ec8b Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Thu, 26 Mar 2015 17:01:29 +0900
+Subject: [PATCH 120/123] arm64: kdump: enable kdump in defconfig
+
+Kdump is enabled by default as kexec is.
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+(cherry picked from commit cf254413f45d4d2b7c707636f683f327dc5f1c1f)
+---
+ arch/arm64/configs/defconfig | 1 +
+ 1 file changed, 1 insertion(+)
+
+diff --git a/arch/arm64/configs/defconfig b/arch/arm64/configs/defconfig
+index c1d4dec..2215d90 100644
+--- a/arch/arm64/configs/defconfig
++++ b/arch/arm64/configs/defconfig
+@@ -61,6 +61,7 @@ CONFIG_TRANSPARENT_HUGEPAGE=y
+ CONFIG_CMA=y
+ CONFIG_CMDLINE="console=ttyAMA0"
+ CONFIG_KEXEC=y
++CONFIG_CRASH_DUMP=y
+ # CONFIG_CORE_DUMP_DEFAULT_ELF_HEADERS is not set
+ CONFIG_COMPAT=y
+ CONFIG_CPU_IDLE=y
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0121-Documentation-kdump-describe-arm64-port.patch b/tools/kdump/0121-Documentation-kdump-describe-arm64-port.patch
new file mode 100644
index 0000000..8e09634
--- /dev/null
+++ b/tools/kdump/0121-Documentation-kdump-describe-arm64-port.patch
@@ -0,0 +1,73 @@
+From b9b0847a7cdea9787a5e0ba0aa0fd6c7aed246b7 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Wed, 19 Aug 2015 11:50:20 +0900
+Subject: [PATCH 121/123] Documentation: kdump: describe arm64 port
+
+Add arch specific descriptions about kdump usage on arm64 to kdump.txt.
+
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Reviewed-by: Baoquan He <bhe@redhat.com>
+Acked-by: Dave Young <dyoung@redhat.com>
+(cherry picked from commit e04bf70ff42ffb4ec6490811591fa539f8ecee66)
+---
+ Documentation/kdump/kdump.txt | 16 +++++++++++++++-
+ 1 file changed, 15 insertions(+), 1 deletion(-)
+
+diff --git a/Documentation/kdump/kdump.txt b/Documentation/kdump/kdump.txt
+index bc4bd5a..b6fe6a8 100644
+--- a/Documentation/kdump/kdump.txt
++++ b/Documentation/kdump/kdump.txt
+@@ -18,7 +18,7 @@ memory image to a dump file on the local disk, or across the network to
+ a remote system.
+ 
+ Kdump and kexec are currently supported on the x86, x86_64, ppc64, ia64,
+-s390x and arm architectures.
++s390x, arm and arm64 architectures.
+ 
+ When the system kernel boots, it reserves a small section of memory for
+ the dump-capture kernel. This ensures that ongoing Direct Memory Access
+@@ -249,6 +249,13 @@ Dump-capture kernel config options (Arch Dependent, arm)
+ 
+     AUTO_ZRELADDR=y
+ 
++Dump-capture kernel config options (Arch Dependent, arm64)
++----------------------------------------------------------
++
++- Please note that kvm of the dump-capture kernel will not be enabled
++  on non-VHE systems even if it is configured. This is because the CPU
++  will not be reset to EL2 on panic.
++
+ Extended crashkernel syntax
+ ===========================
+ 
+@@ -312,6 +319,8 @@ Boot into System Kernel
+    any space below the alignment point may be overwritten by the dump-capture kernel,
+    which means it is possible that the vmcore is not that precise as expected.
+ 
++   On arm64, use "crashkernel=Y[@X]".  Note that the start address of
++   the kernel, X if explicitly specified, must be aligned to 2MiB (0x200000).
+ 
+ Load the Dump-capture Kernel
+ ============================
+@@ -334,6 +343,8 @@ For s390x:
+ 	- Use image or bzImage
+ For arm:
+ 	- Use zImage
++For arm64:
++	- Use vmlinux or Image
+ 
+ If you are using a uncompressed vmlinux image then use following command
+ to load dump-capture kernel.
+@@ -377,6 +388,9 @@ For s390x:
+ For arm:
+ 	"1 maxcpus=1 reset_devices"
+ 
++For arm64:
++	"1 maxcpus=1 reset_devices"
++
+ Notes on loading the dump-capture kernel:
+ 
+ * By default, the ELF headers are stored in ELF64 format to support
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0122-Documentation-dt-chosen-properties-for-arm64-kdump.patch b/tools/kdump/0122-Documentation-dt-chosen-properties-for-arm64-kdump.patch
new file mode 100644
index 0000000..3f85d7d
--- /dev/null
+++ b/tools/kdump/0122-Documentation-dt-chosen-properties-for-arm64-kdump.patch
@@ -0,0 +1,85 @@
+From 6146fb5a7efc14285213d2114404e0c86e61479f Mon Sep 17 00:00:00 2001
+From: James Morse <james.morse@arm.com>
+Date: Thu, 5 May 2016 10:49:40 +0100
+Subject: [PATCH 122/123] Documentation: dt: chosen properties for arm64 kdump
+
+Add documentation for
+	linux,crashkernel-base and crashkernel-size,
+	linux,usable-memory-range
+	linux,elfcorehdr
+used by arm64 kdump to decribe the kdump reserved area, and
+the elfcorehdr's location within it.
+
+Signed-off-by: James Morse <james.morse@arm.com>
+[takahiro.akashi@linaro.org: added "linux,crashkernel-base" and "-size" ]
+Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Cc: devicetree@vger.kernel.org
+Cc: Rob Herring <robh+dt@kernel.org>
+Cc: Mark Rutland <mark.rutland@arm.com>
+
+(cherry picked from commit 3592c762ab10ff65152572a0ebdbbf73226c7888)
+---
+ Documentation/devicetree/bindings/chosen.txt | 50 ++++++++++++++++++++++++++++
+ 1 file changed, 50 insertions(+)
+
+diff --git a/Documentation/devicetree/bindings/chosen.txt b/Documentation/devicetree/bindings/chosen.txt
+index 6ae9d82..7b11516 100644
+--- a/Documentation/devicetree/bindings/chosen.txt
++++ b/Documentation/devicetree/bindings/chosen.txt
+@@ -52,3 +52,53 @@ This property is set (currently only on PowerPC, and only needed on
+ book3e) by some versions of kexec-tools to tell the new kernel that it
+ is being booted by kexec, as the booting environment may differ (e.g.
+ a different secondary CPU release mechanism)
++
++linux,crashkernel-base
++linux,crashkernel-size
++----------------------
++
++These properties (currently used on PowerPC and arm64) indicates
++the base address and the size, respectively, of the reserved memory
++range for crash dump kernel.
++e.g.
++
++/ {
++	chosen {
++		linux,crashkernel-base = <0x9 0xf0000000>;
++		linux,crashkernel-size = <0x0 0x10000000>;
++	};
++};
++
++linux,usable-memory-range
++-------------------------
++
++This property (currently used only on arm64) holds the memory range,
++the base address and the size, which can be used as system ram on
++the *current* kernel. Note that, if this property is present, any memory
++regions under "memory" nodes in DT blob or ones marked as "conventional
++memory" in EFI memory map should be ignored.
++e.g.
++
++/ {
++	chosen {
++		linux,usable-memory-range = <0x9 0xf0000000 0x0 0x10000000>;
++	};
++};
++
++The main usage is for crash dump kernel to identify its own usable
++memory and exclude, at its boot time, any other memory areas that are
++part of the panicked kernel's memory.
++
++linux,elfcorehdr
++----------------
++
++This property (currently used only on arm64) holds the memory range,
++the address and the size, of the elf core header which mainly describes
++the panicked kernel's memory layout as PT_LOAD segments of elf format.
++e.g.
++
++/ {
++	chosen {
++		linux,elfcorehdr = <0x9 0xfffff000 0x0 0x800>;
++	};
++};
+-- 
+1.8.3.1
+
diff --git a/tools/kdump/0123-fixup-arm64-kdump-add-VMCOREINFO-s-for-user-space-to.patch b/tools/kdump/0123-fixup-arm64-kdump-add-VMCOREINFO-s-for-user-space-to.patch
new file mode 100644
index 0000000..8ce2d5c
--- /dev/null
+++ b/tools/kdump/0123-fixup-arm64-kdump-add-VMCOREINFO-s-for-user-space-to.patch
@@ -0,0 +1,27 @@
+From d39732d4ef63ba02d04f842656ee4470f35d5f51 Mon Sep 17 00:00:00 2001
+From: AKASHI Takahiro <takahiro.akashi@linaro.org>
+Date: Mon, 28 Nov 2016 13:34:42 +0900
+Subject: [PATCH 123/123] fixup! arm64: kdump: add VMCOREINFO's for user-space
+ tools
+
+If KASLR is backported, this patch won't be necessary.
+---
+ arch/arm64/kernel/machine_kexec.c | 2 --
+ 1 file changed, 2 deletions(-)
+
+diff --git a/arch/arm64/kernel/machine_kexec.c b/arch/arm64/kernel/machine_kexec.c
+index 0ff4ec5..9fc748a 100644
+--- a/arch/arm64/kernel/machine_kexec.c
++++ b/arch/arm64/kernel/machine_kexec.c
+@@ -224,8 +224,6 @@ void arch_crash_save_vmcoreinfo(void)
+ {
+ 	VMCOREINFO_NUMBER(VA_BITS);
+ 	/* Please note VMCOREINFO_NUMBER() uses "%d", not "%x" */
+-	vmcoreinfo_append_str("NUMBER(kimage_voffset)=0x%llx\n",
+-						kimage_voffset);
+ 	vmcoreinfo_append_str("NUMBER(PHYS_OFFSET)=0x%llx\n",
+ 						PHYS_OFFSET);
+ }
+-- 
+1.8.3.1
+
-- 
2.7.4

